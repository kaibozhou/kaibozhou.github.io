<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Apache Flink 客户端操作 | Kaibo's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.1"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.3.1/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-131716279-1','auto');ga('send','pageview');
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + 'adcf6eb324ad0475bb957857cf925dda';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Apache Flink 客户端操作</h1><a id="logo" href="/.">Kaibo's Blog</a><p class="description">Real-time big data processing</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Apache Flink 客户端操作</h1><div class="post-meta">Oct 15, 2019<script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><a class="disqus-comment-count" data-disqus-identifier="2019/10/15/apache-flink-intro-client/" href="/2019/10/15/apache-flink-intro-client/#disqus_thread"></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#环境说明"><span class="toc-number">1.</span> <span class="toc-text">环境说明</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#课程概要"><span class="toc-number">2.</span> <span class="toc-text">课程概要</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Flink-客户端操作"><span class="toc-number">3.</span> <span class="toc-text">Flink 客户端操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Flink命令行"><span class="toc-number">3.1.</span> <span class="toc-text">Flink命令行</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#standalone"><span class="toc-number">3.1.1.</span> <span class="toc-text">standalone</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#run"><span class="toc-number">3.1.1.1.</span> <span class="toc-text">run</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#list"><span class="toc-number">3.1.1.2.</span> <span class="toc-text">list</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#stop"><span class="toc-number">3.1.1.3.</span> <span class="toc-text">stop</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#cancel"><span class="toc-number">3.1.1.4.</span> <span class="toc-text">cancel</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#savepoint"><span class="toc-number">3.1.1.5.</span> <span class="toc-text">savepoint</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#modify"><span class="toc-number">3.1.1.6.</span> <span class="toc-text">modify</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#info"><span class="toc-number">3.1.1.7.</span> <span class="toc-text">info</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#yarn-per-job"><span class="toc-number">3.1.2.</span> <span class="toc-text">yarn per-job</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#单任务-attach-模式"><span class="toc-number">3.1.2.1.</span> <span class="toc-text">单任务 attach 模式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#单任务-detached-模式"><span class="toc-number">3.1.2.2.</span> <span class="toc-text">单任务 detached 模式</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#yarn-session"><span class="toc-number">3.1.3.</span> <span class="toc-text">yarn session</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#启动-session"><span class="toc-number">3.1.3.1.</span> <span class="toc-text">启动 session</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#提交任务"><span class="toc-number">3.1.3.2.</span> <span class="toc-text">提交任务</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#提交到指定的session"><span class="toc-number">3.1.3.3.</span> <span class="toc-text">提交到指定的session</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scala-Shell"><span class="toc-number">3.2.</span> <span class="toc-text">Scala Shell</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Deploy"><span class="toc-number">3.2.1.</span> <span class="toc-text">Deploy</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#local"><span class="toc-number">3.2.1.1.</span> <span class="toc-text">local</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#remote"><span class="toc-number">3.2.1.2.</span> <span class="toc-text">remote</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#yarn"><span class="toc-number">3.2.1.3.</span> <span class="toc-text">yarn</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Execute"><span class="toc-number">3.2.2.</span> <span class="toc-text">Execute</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#DataSet"><span class="toc-number">3.2.2.1.</span> <span class="toc-text">DataSet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DataSteam"><span class="toc-number">3.2.2.2.</span> <span class="toc-text">DataSteam</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#TableAPI"><span class="toc-number">3.2.2.3.</span> <span class="toc-text">TableAPI</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SQL-Client-Beta"><span class="toc-number">3.3.</span> <span class="toc-text">SQL Client Beta</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#基本用法"><span class="toc-number">3.3.1.</span> <span class="toc-text">基本用法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#select-查询"><span class="toc-number">3.3.1.1.</span> <span class="toc-text">select 查询</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#explain"><span class="toc-number">3.3.1.2.</span> <span class="toc-text">explain</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#结果展示"><span class="toc-number">3.3.2.</span> <span class="toc-text">结果展示</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#table-mode"><span class="toc-number">3.3.2.1.</span> <span class="toc-text">table mode</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#changlog-mode"><span class="toc-number">3.3.2.2.</span> <span class="toc-text">changlog mode</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Environment-Files"><span class="toc-number">3.3.3.</span> <span class="toc-text">Environment Files</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Restful-API"><span class="toc-number">3.4.</span> <span class="toc-text">Restful API</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Web"><span class="toc-number">3.5.</span> <span class="toc-text">Web</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#结束"><span class="toc-number">4.</span> <span class="toc-text">结束</span></a></li></ol></div></div><div class="post-content"><p>本文是我参加 Apache Flink China 社区钉钉群直播的教程（<a href="https://ververica.cn/developers/flink-training-course1/" target="_blank" rel="noopener">基础篇 1.5</a>）。我稍作调整后发在<a href="https://zhoukaibo.com">个人网站(zhoukaibo.com)</a>上。</p>
<h1 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h1><p>在前面几期的课程里面讲过了Flink开发环境的搭建和应用的部署以及运行，今天的课程主要是讲Flink的客户端操作。本次讲解以实际操作为主。这次课程是基于社区的Flink 1.7.2版本，操作系统是Mac系统，浏览器是Google Chrome浏览器。有关开发环境的准备和集群的部署，请参考前面第三期的内容。</p>
<h1 id="课程概要"><a href="#课程概要" class="headerlink" title="课程概要"></a>课程概要</h1><p>如下图所示，<a href="https://zhoukaibo.com/tags/flink/">Flink</a>提供了丰富的客户端操作来提交任务和与任务进行交互，包括 Flink命令行，Scala Shell，SQL Client，Restful API 和 Web。Flink首先提供的最重要的是命令行，其次是SQL Client用于提交SQL任务的运行，还有就是Scala Shell提交Table API的任务。同时，Flink也提供了Restful服务，用户可以通过http方式进行调用。此外，还有Web的方式可以提交任务。</p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015215818.png" alt="flink_clients.png"></p>
<p>在 flink 安装目录的 bin 目录下面可以看到有 flink, start-scala-shell.sh 和 sql-client.sh 等文件，这些都是客户端操作的入口。</p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015215849.jpg" alt="flink_1_7_2.jpg"></p>
<h1 id="Flink-客户端操作"><a href="#Flink-客户端操作" class="headerlink" title="Flink 客户端操作"></a>Flink 客户端操作</h1><h2 id="Flink命令行"><a href="#Flink命令行" class="headerlink" title="Flink命令行"></a>Flink命令行</h2><p>flink 的命令行参数很多，输入 flink -h 能看到完整的说明：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/flink -h</span><br></pre></td></tr></table></figure>
<p>如果想看某一个命令的参数，比如 run 命令，输入：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/flink run -h</span><br></pre></td></tr></table></figure>
<p>本文主要讲解常见的一些操作，更详细的文档请参考: <a href="https://ci.apache.org/projects/flink/flink-docs-stable/ops/cli.html" target="_blank" rel="noopener">Flink 命令行官方文档</a>。</p>
<h3 id="standalone"><a href="#standalone" class="headerlink" title="standalone"></a>standalone</h3><p>首先启动一个 standalone 的集群</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/start-cluster.sh</span><br><span class="line">Starting cluster.</span><br><span class="line">Starting standalonesession daemon on host zkb-MBP.local.</span><br><span class="line">Starting taskexecutor daemon on host zkb-MBP.local.</span><br></pre></td></tr></table></figure>
<p>打开 <a href="http://127.0.0.1:8081" target="_blank" rel="noopener">http://127.0.0.1:8081</a> 能看到 Web 界面。</p>
<h4 id="run"><a href="#run" class="headerlink" title="run"></a>run</h4><p>运行任务，以 <a href="https://zhoukaibo.com/tags/flink/">flink</a> 自带的例子 TopSpeedWindowing 为例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/flink run -d examples/streaming/TopSpeedWindowing.jar</span><br><span class="line">Starting execution of program</span><br><span class="line">Executing TopSpeedWindowing example with default input data set.</span><br><span class="line">Use --input to specify file input.</span><br><span class="line">Printing result to stdout. Use --output to specify output path.</span><br><span class="line">Job has been submitted with JobID 5e20cb6b0f357591171dfcca2eea09de</span><br></pre></td></tr></table></figure>
<p>运行起来后默认是1个并发:<br><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015215922.jpg" alt="flink_run_1.jpg"></p>
<p>点左侧『Task Manager』，然后点『Stdout』能看到输出日志：<br><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015215949.jpg" alt="flink_run_2.jpg"></p>
<p>或者查看本地 log 目录下的 *.out 文件：<br><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220107.jpg" alt="flink_run_3.jpg"></p>
<h4 id="list"><a href="#list" class="headerlink" title="list"></a>list</h4><p>查看任务列表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/flink list -m 127.0.0.1:8081</span><br><span class="line">Waiting for response...</span><br><span class="line">------------------ Running/Restarting Jobs -------------------</span><br><span class="line">24.03.2019 10:14:06 : 5e20cb6b0f357591171dfcca2eea09de : CarTopSpeedWindowingExample (RUNNING)</span><br><span class="line">--------------------------------------------------------------</span><br><span class="line">No scheduled jobs.</span><br></pre></td></tr></table></figure>
<h4 id="stop"><a href="#stop" class="headerlink" title="stop"></a>stop</h4><p>停止任务。通过 -m 来指定要停止的 JobManager 的主机地址和端口。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/flink stop -m 127.0.0.1:8081 d67420e52bd051fae2fddbaa79e046bb</span><br><span class="line">Stopping job d67420e52bd051fae2fddbaa79e046bb.</span><br><span class="line">------------------------------------------------------------</span><br><span class="line">The program finished with the following exception:</span><br><span class="line">  org.apache.flink.util.FlinkException: Could not stop the job   d67420e52bd051fae2fddbaa79e046bb.</span><br><span class="line">  at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:554)</span><br><span class="line">  at org.apache.flink.client.cli.CliFrontend.runClusterAction(CliFrontend.java:985)</span><br><span class="line">  at org.apache.flink.client.cli.CliFrontend.stop(CliFrontend.java:547)</span><br><span class="line">  at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:1062)</span><br><span class="line">  at org.apache.flink.client.cli.CliFrontend.lambda$main$11(CliFrontend.java:1126)</span><br><span class="line">  at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">  at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)</span><br><span class="line">  at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)</span><br><span class="line">  at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1126)</span><br><span class="line">Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.rest.util.RestClientException: [Job termination (STOP) failed: This job is not stoppable.]</span><br><span class="line">  at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)</span><br><span class="line">  at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915)</span><br><span class="line">  at org.apache.flink.client.program.rest.RestClusterClient.stop(RestClusterClient.java:392)</span><br><span class="line">  at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:552)</span><br><span class="line">... 9 more</span><br><span class="line">Caused by: org.apache.flink.runtime.rest.util.RestClientException: [Job termination (STOP) failed: This job is not stoppable.]</span><br><span class="line">  at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:380)</span><br><span class="line">  at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$3(RestClient.java:364)</span><br><span class="line">  at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:952)</span><br><span class="line">  at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926)</span><br><span class="line">  at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)</span><br><span class="line">  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">  at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure>
<p>从日志里面能看出 stop 命令执行失败了。一个 job 能够被 stop 要求所有的 source 都是可以 stoppable 的，即实现了 StoppableFunction 接口。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"> </span><br><span class="line">/**</span><br><span class="line"> * 需要能 stoppable 的函数必须实现这个接口，例如流式任务的 source。</span><br><span class="line"> * stop() 方法在任务收到 STOP 信号的时候调用。</span><br><span class="line"> * source 在接收到这个信号后，必须停止发送新的数据且优雅的停止。</span><br><span class="line"> */</span><br><span class="line">@PublicEvolving</span><br><span class="line">public interface StoppableFunction &#123;</span><br><span class="line">	/**</span><br><span class="line">	  * 停止 source。与 cancel() 不同的是，这是一个让 source 优雅停止的请求。</span><br><span class="line">     * 等待中的数据可以继续发送出去，不需要立即停止。</span><br><span class="line">	  */</span><br><span class="line">	void stop();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="cancel"><a href="#cancel" class="headerlink" title="cancel"></a>cancel</h4><p>取消任务。如果在 conf/flink-conf.yaml 里面配置了 state.savepoints.dir，会保存savepoint，否则不会保存 savepoint。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/flink cancel -m 127.0.0.1:8081 5e20cb6b0f357591171dfcca2eea09de</span><br><span class="line"> </span><br><span class="line">Cancelling job 5e20cb6b0f357591171dfcca2eea09de.</span><br><span class="line">Cancelled job 5e20cb6b0f357591171dfcca2eea09de.</span><br></pre></td></tr></table></figure>
<p>也可以在停止的时候显式指定 savepoint 目录。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/flink cancel -m 127.0.0.1:8081 -s /tmp/savepoint 29da945b99dea6547c3fbafd57ed8759</span><br><span class="line"> </span><br><span class="line">Cancelling job 29da945b99dea6547c3fbafd57ed8759 with savepoint to /tmp/savepoint.</span><br><span class="line">Cancelled job 29da945b99dea6547c3fbafd57ed8759. Savepoint stored in file:/tmp/savepoint/savepoint-29da94-88299bacafb7.</span><br><span class="line"> </span><br><span class="line">➜  flink-1.7.2 ll /tmp/savepoint/savepoint-29da94-88299bacafb7</span><br><span class="line">total 32K</span><br><span class="line">-rw-r--r-- 1 baoniu 29K Mar 24 10:33 _metadata</span><br></pre></td></tr></table></figure>
<p>取消和停止（流作业）的区别如下：</p>
<ul>
<li>cancel() 调用，立即调用作业算子的 cancel() 方法，以尽快取消它们。如果算子在接到 cancel() 调用后没有停止，Flink 将开始定期中断算子线程的执行，直到所有算子停止为止。</li>
<li>stop() 调用，是更优雅的停止正在运行流作业的方式。stop() 仅适用于 source 实现了 StoppableFunction 接口的作业。当用户请求停止作业时，作业的所有 source 都将接收 stop() 方法调用。直到所有 source 正常关闭时，作业才会正常结束。这种方式，使作业正常处理完所有作业。</li>
</ul>
<h4 id="savepoint"><a href="#savepoint" class="headerlink" title="savepoint"></a>savepoint</h4><p>触发 savepoint。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/flink savepoint -m 127.0.0.1:8081 ec53edcfaeb96b2a5dadbfbe5ff62bbb /tmp/savepoint</span><br><span class="line">Triggering savepoint for job ec53edcfaeb96b2a5dadbfbe5ff62bbb.</span><br><span class="line">Waiting for response...</span><br><span class="line">Savepoint completed. Path: file:/tmp/savepoint/savepoint-ec53ed-84b00ce500ee</span><br><span class="line">You can resume your program from this savepoint with the run command.</span><br></pre></td></tr></table></figure>
<p>说明：savepoint 和 checkpoint 的区别（<a href="https://www.ververica.com/blog/differences-between-savepoints-and-checkpoints-in-flink" target="_blank" rel="noopener">详见文档</a>）：</p>
<ul>
<li>checkpoint 是增量做的，每次的时间较短，数据量较小，只要在程序里面启用后会自动触发，用户无须感知；checkpoint 是作业 failover 的时候自动使用，不需要用户指定。</li>
<li>savepoint 是全量做的，每次的时间较长，数据量较大，需要用户主动去触发。savepoint 一般用于程序的版本更新（<a href="https://ci.apache.org/projects/flink/flink-docs-stable/ops/upgrading.html#step-1-take-a-savepoint-in-the-old-flink-version" target="_blank" rel="noopener">详见文档</a>），bug修复，A/B Test等场景，需要用户指定。</li>
</ul>
<p>通过 -s 参数从指定的 savepoint 启动：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/flink run -d -s /tmp/savepoint/savepoint-f049ff-24ec0d3e0dc7 ./examples/streaming/TopSpeedWindowing.jar</span><br><span class="line">Starting execution of program</span><br><span class="line">Executing TopSpeedWindowing example with default input data set.</span><br><span class="line">Use --input to specify file input.</span><br><span class="line">Printing result to stdout. Use --output to specify output path.</span><br></pre></td></tr></table></figure>
<p>查看 JobManager 的日志，能够看到类似这样的log：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">2019-03-28 10:30:53,957 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     </span><br><span class="line">- Starting job 790d7b98db6f6af55d04aec1d773852d from savepoint /tmp/savepoint/savepoint-f049ff-24ec0d3e0dc7 ()</span><br><span class="line">2019-03-28 10:30:53,959 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    </span><br><span class="line"> - Reset the checkpoint ID of job 790d7b98db6f6af55d04aec1d773852d to 2.</span><br><span class="line">2019-03-28 10:30:53,959 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     </span><br><span class="line">- Restoring job 790d7b98db6f6af55d04aec1d773852d from latest valid checkpoint: Checkpoint 1 @ 0 for 790d7b98db6f6af55d04aec1d773852d.</span><br></pre></td></tr></table></figure>
<h4 id="modify"><a href="#modify" class="headerlink" title="modify"></a>modify</h4><p>修改任务并行度。<br>为了方便演示，我们修改 conf/flink-conf.yaml 将 task slot 数从默认的 1 改为 4，并配置 savepoint 目录。（modify 参数后面接 -s 指定savepoint路径当前版本可能有bug，提示无法识别）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">taskmanager.numberOfTaskSlots: 4</span><br><span class="line">state.savepoints.dir: file:///tmp/savepoint</span><br></pre></td></tr></table></figure>
<p>修改参数后需要重启集群生效，然后再启动任务：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/stop-cluster.sh &amp;&amp; bin/start-cluster.sh</span><br><span class="line">Stopping taskexecutor daemon (pid: 53139) on host zkb-MBP.local.</span><br><span class="line">Stopping standalonesession daemon (pid: 52723) on host zkb-MBP.local.</span><br><span class="line">Starting cluster.</span><br><span class="line">Starting standalonesession daemon on host zkb-MBP.local.</span><br><span class="line">Starting taskexecutor daemon on host zkb-MBP.local.</span><br><span class="line"> </span><br><span class="line">➜  flink-1.7.2 bin/flink run -d examples/streaming/TopSpeedWindowing.jar</span><br><span class="line">Starting execution of program</span><br><span class="line">Executing TopSpeedWindowing example with default input data set.</span><br><span class="line">Use --input to specify file input.</span><br><span class="line">Printing result to stdout. Use --output to specify output path.</span><br><span class="line">Job has been submitted with JobID 7752ea7b0e7303c780de9d86a5ded3fa</span><br></pre></td></tr></table></figure>
<p>从页面上能看到 Task Slots 变为了 4，这时候任务的默认并发度是 1。</p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220147.jpg" alt="standalone-modify-1.jpg"></p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220219.jpg" alt="standalone-modify-2.jpg"></p>
<p>通过 modify 命令依次将并发度修改为 4 和 3，可以看到每次 modify 命令都会触发一次 savepoint。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/flink modify -p 4 7752ea7b0e7303c780de9d86a5ded3fa</span><br><span class="line">Modify job 7752ea7b0e7303c780de9d86a5ded3fa.</span><br><span class="line">Rescaled job 7752ea7b0e7303c780de9d86a5ded3fa. Its new parallelism is 4.</span><br><span class="line"> </span><br><span class="line">➜  flink-1.7.2 ll /tmp/savepoint</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x 3 baoniu 96 Jun 17 09:05 savepoint-7752ea-00c05b015836/</span><br><span class="line"> </span><br><span class="line">➜  flink-1.7.2 bin/flink modify -p 3 7752ea7b0e7303c780de9d86a5ded3fa</span><br><span class="line">Modify job 7752ea7b0e7303c780de9d86a5ded3fa.</span><br><span class="line">Rescaled job 7752ea7b0e7303c780de9d86a5ded3fa. Its new parallelism is 3.</span><br><span class="line"> </span><br><span class="line">➜  flink-1.7.2 ll /tmp/savepoint</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x 3 baoniu 96 Jun 17 09:08 savepoint-7752ea-449b131b2bd4/</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220242.jpg" alt="standalone-modify-3.jpg"></p>
<p>查看JobManager的日志，可以看到：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">2019-06-17 09:05:11,179 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Starting job 7752ea7b0e7303c780de9d86a5ded3fa from savepoint file:/tmp/savepoint/savepoint-790d7b-3581698f007e ()</span><br><span class="line">2019-06-17 09:05:11,182 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Reset the checkpoint ID of job 7752ea7b0e7303c780de9d86a5ded3fa to 3.</span><br><span class="line">2019-06-17 09:05:11,182 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Restoring job 790d7b98db6f6af55d04aec1d773852d from latest valid checkpoint: Checkpoint 2 @ 0 for 7752ea7b0e7303c780de9d86a5ded3fa.</span><br><span class="line">2019-06-17 09:05:11,184 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - No master state to restore</span><br><span class="line">2019-06-17 09:05:11,184 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Job CarTopSpeedWindowingExample (7752ea7b0e7303c780de9d86a5ded3fa) switched from state RUNNING to SUSPENDING.</span><br><span class="line">org.apache.flink.util.FlinkException: Job is being rescaled.</span><br></pre></td></tr></table></figure>
<h4 id="info"><a href="#info" class="headerlink" title="info"></a>info</h4><p>info 命令是用来查看 <a href="https://zhoukaibo.com/tags/flink/">flink</a> 任务的执行计划（StreamGraph）的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/flink info examples/streaming/TopSpeedWindowing.jar</span><br><span class="line">----------------------- Execution Plan -----------------------</span><br><span class="line">&#123;&quot;nodes&quot;:[&#123;&quot;id&quot;:1,&quot;type&quot;:&quot;Source: Custom Source&quot;,&quot;pact&quot;:&quot;Data Source&quot;,&quot;contents&quot;:&quot;Source: Custom Source&quot;,&quot;parallelism&quot;:1&#125;,&#123;&quot;id&quot;:2,&quot;type&quot;:&quot;Timestamps/Watermarks&quot;,&quot;pact&quot;:&quot;Operator&quot;,&quot;contents&quot;:&quot;Timestamps/Watermarks&quot;,&quot;parallelism&quot;:1,&quot;predecessors&quot;:[&#123;&quot;id&quot;:1,&quot;ship_strategy&quot;:&quot;FORWARD&quot;,&quot;side&quot;:&quot;second&quot;&#125;]&#125;,&#123;&quot;id&quot;:4,&quot;type&quot;:&quot;Window(GlobalWindows(), DeltaTrigger, TimeEvictor, ComparableAggregator, PassThroughWindowFunction)&quot;,&quot;pact&quot;:&quot;Operator&quot;,&quot;contents&quot;:&quot;Window(GlobalWindows(), DeltaTrigger, TimeEvictor, ComparableAggregator, PassThroughWindowFunction)&quot;,&quot;parallelism&quot;:1,&quot;predecessors&quot;:[&#123;&quot;id&quot;:2,&quot;ship_strategy&quot;:&quot;HASH&quot;,&quot;side&quot;:&quot;second&quot;&#125;]&#125;,&#123;&quot;id&quot;:5,&quot;type&quot;:&quot;Sink: Print to Std. Out&quot;,&quot;pact&quot;:&quot;Data Sink&quot;,&quot;contents&quot;:&quot;Sink: Print to Std. Out&quot;,&quot;parallelism&quot;:1,&quot;predecessors&quot;:[&#123;&quot;id&quot;:4,&quot;ship_strategy&quot;:&quot;FORWARD&quot;,&quot;side&quot;:&quot;second&quot;&#125;]&#125;]&#125;</span><br><span class="line">--------------------------------------------------------------</span><br></pre></td></tr></table></figure>
<p>拷贝输出的 json 内容，粘贴到这个网站：<a href="http://flink.apache.org/visualizer/" target="_blank" rel="noopener">http://flink.apache.org/visualizer/</a></p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220400.jpg" alt=""></p>
<p>可以和实际运行的物理执行计划对比：</p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220425.jpg" alt=""></p>
<h3 id="yarn-per-job"><a href="#yarn-per-job" class="headerlink" title="yarn per-job"></a>yarn per-job</h3><h4 id="单任务-attach-模式"><a href="#单任务-attach-模式" class="headerlink" title="单任务 attach 模式"></a>单任务 attach 模式</h4><p>默认是attach模式，即客户端会一直等待直到程序结束才会退出。</p>
<ul>
<li>通过 -m yarn-cluster 指定 yarn 模式</li>
<li>Yarn上显示名字为 Flink session cluster，这个 batch 的 wordcount 任务运行完会 FINISHED。</li>
<li>客户端能看到结果输出</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[admin@z17.sqa.zth /home/admin/flink/flink-1.7.2]</span><br><span class="line">$echo $HADOOP_CONF_DIR</span><br><span class="line">/etc/hadoop/conf/</span><br><span class="line"> </span><br><span class="line">[admin@z17.sqa.zth /home/admin/flink/flink-1.7.2]</span><br><span class="line">$./bin/flink run -m yarn-cluster ./examples/batch/WordCount.jar</span><br><span class="line"> </span><br><span class="line">2019-06-17 09:15:24,511 INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at z05c05217.sqa.zth.tbsite.net/11.163.188.29:8050</span><br><span class="line">2019-06-17 09:15:24,690 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar</span><br><span class="line">2019-06-17 09:15:24,690 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar</span><br><span class="line">2019-06-17 09:15:24,907 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Cluster specification: ClusterSpecification&#123;masterMemoryMB=1024, taskManagerMemoryMB=1024, numberTaskManagers=1, slotsPerTaskManager=4&#125;</span><br><span class="line">2019-06-17 09:15:25,430 WARN  org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory       - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.</span><br><span class="line">2019-06-17 09:15:25,438 WARN  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - The configuration directory (&apos;/Users/baoniu/Documents/work/tool/flink/flink-1.7.2/conf&apos;) contains both LOG4J and Logback configuration files. Please delete or rename one of them.</span><br><span class="line">2019-06-17 09:15:36,239 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Submitting application master application_1532332183347_0724</span><br><span class="line">2019-06-17 09:15:36,276 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl         - Submitted application application_1532332183347_0724</span><br><span class="line">2019-06-17 09:15:36,276 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Waiting for the cluster to be allocated</span><br><span class="line">2019-06-17 09:15:36,281 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Deploying cluster, current state ACCEPTED</span><br><span class="line">2019-06-17 09:15:40,426 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - YARN application has been deployed successfully.</span><br><span class="line">Starting execution of program</span><br><span class="line">Executing WordCount example with default input data set.</span><br><span class="line">Use --input to specify file input.</span><br><span class="line">Printing result to stdout. Use --output to specify output path.</span><br><span class="line">(a,5)</span><br><span class="line">(action,1)</span><br><span class="line">(after,1)</span><br><span class="line">(against,1)</span><br><span class="line">(all,2)</span><br><span class="line">... ...</span><br><span class="line">(would,2)</span><br><span class="line">(wrong,1)</span><br><span class="line">(you,1)</span><br><span class="line">Program execution finished</span><br><span class="line">Job with JobID 8bfe7568cb5c3254af30cbbd9cd5971e has finished.</span><br><span class="line">Job Runtime: 9371 ms</span><br><span class="line">Accumulator Results:</span><br><span class="line">- 2bed2c5506e9237fb85625416a1bc508 (java.util.ArrayList) [170 elements]</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220443.jpg" alt=""></p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220510.jpg" alt=""></p>
<p>如果我们以 attach 模式运行 streaming 的任务，客户端会一直等待不退出，可以运行以下的例子试验下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/flink run -m yarn-cluster ./examples/streaming/TopSpeedWindowing.jar</span><br></pre></td></tr></table></figure>
<h4 id="单任务-detached-模式"><a href="#单任务-detached-模式" class="headerlink" title="单任务 detached 模式"></a>单任务 detached 模式</h4><ul>
<li>由于是 detached 模式，客户端提交完任务就退出了</li>
<li>Yarn 上显示为 Flink per-job cluster</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$./bin/flink run -yd -m yarn-cluster ./examples/streaming/TopSpeedWindowing.jar</span><br><span class="line"> </span><br><span class="line">2019-06-18 09:21:59,247 INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at z05c05217.sqa.zth.tbsite.net/11.163.188.29:8050</span><br><span class="line">2019-06-18 09:21:59,428 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar</span><br><span class="line">2019-06-18 09:21:59,428 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar</span><br><span class="line">2019-06-18 09:21:59,940 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Cluster specification: ClusterSpecification&#123;masterMemoryMB=1024, taskManagerMemoryMB=1024, numberTaskManagers=1, slotsPerTaskManager=4&#125;</span><br><span class="line">2019-06-18 09:22:00,427 WARN  org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory       - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.</span><br><span class="line">2019-06-18 09:22:00,436 WARN  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - The configuration directory (&apos;/Users/baoniu/Documents/work/tool/flink/flink-1.7.2/conf&apos;) contains both LOG4J and Logback configuration files. Please delete or rename one of them.</span><br><span class="line">^@2019-06-18 09:22:12,113 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Submitting application master application_1532332183347_0729</span><br><span class="line">2019-06-18 09:22:12,151 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl         - Submitted application application_1532332183347_0729</span><br><span class="line">2019-06-18 09:22:12,151 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Waiting for the cluster to be allocated</span><br><span class="line">2019-06-18 09:22:12,155 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Deploying cluster, current state ACCEPTED</span><br><span class="line">2019-06-18 09:22:16,275 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - YARN application has been deployed successfully.</span><br><span class="line">2019-06-18 09:22:16,275 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - The Flink YARN client has been started in detached mode. In order to stop Flink on YARN, use the following command or a YARN web interface to stop it:</span><br><span class="line">yarn application -kill application_1532332183347_0729</span><br><span class="line">Please also note that the temporary files of the YARN session in the home directory will not be removed.</span><br><span class="line">Job has been submitted with JobID e61b9945c33c300906ad50a9a11f36df</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220545.jpg" alt=""></p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220559.jpg" alt=""></p>
<h3 id="yarn-session"><a href="#yarn-session" class="headerlink" title="yarn session"></a>yarn session</h3><h4 id="启动-session"><a href="#启动-session" class="headerlink" title="启动 session"></a>启动 session</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/yarn-session.sh -tm 2048 -s 3</span><br></pre></td></tr></table></figure>
<p>表示启动一个 yarn session 集群，每个TM的内存是2G，每个TM有3个slot。(注意：-n 参数不生效)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 ./bin/yarn-session.sh -tm 2048 -s 3</span><br><span class="line">2019-06-17 09:21:50,177 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.address, localhost</span><br><span class="line">2019-06-17 09:21:50,179 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.port, 6123</span><br><span class="line">2019-06-17 09:21:50,179 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.heap.size, 1024m</span><br><span class="line">2019-06-17 09:21:50,179 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.heap.size, 1024m</span><br><span class="line">2019-06-17 09:21:50,179 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.numberOfTaskSlots, 4</span><br><span class="line">2019-06-17 09:21:50,179 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: state.savepoints.dir, file:///tmp/savepoint</span><br><span class="line">2019-06-17 09:21:50,180 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: parallelism.default, 1</span><br><span class="line">2019-06-17 09:21:50,180 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: rest.port, 8081</span><br><span class="line">2019-06-17 09:21:50,644 WARN  org.apache.hadoop.util.NativeCodeLoader                       - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">2019-06-17 09:21:50,746 INFO  org.apache.flink.runtime.security.modules.HadoopModule        - Hadoop user set to baoniu (auth:SIMPLE)</span><br><span class="line">2019-06-17 09:21:50,848 INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at z05c05217.sqa.zth.tbsite.net/11.163.188.29:8050</span><br><span class="line">2019-06-17 09:21:51,148 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Cluster specification: ClusterSpecification&#123;masterMemoryMB=1024, taskManagerMemoryMB=2048, numberTaskManagers=1, slotsPerTaskManager=3&#125;</span><br><span class="line">2019-06-17 09:21:51,588 WARN  org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory       - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.</span><br><span class="line">2019-06-17 09:21:51,596 WARN  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - The configuration directory (&apos;/Users/baoniu/Documents/work/tool/flink/flink-1.7.2/conf&apos;) contains both LOG4J and Logback configuration files. Please delete or rename one of them.</span><br><span class="line">^@2019-06-17 09:22:03,304 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Submitting application master application_1532332183347_0726</span><br><span class="line">2019-06-17 09:22:03,336 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl         - Submitted application application_1532332183347_0726</span><br><span class="line">2019-06-17 09:22:03,336 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Waiting for the cluster to be allocated</span><br><span class="line">2019-06-17 09:22:03,340 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Deploying cluster, current state ACCEPTED</span><br><span class="line">2019-06-17 09:22:07,722 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - YARN application has been deployed successfully.</span><br><span class="line">2019-06-17 09:22:08,050 INFO  org.apache.flink.runtime.rest.RestClient                      - Rest client endpoint started.</span><br><span class="line">Flink JobManager is now running on z07.sqa.net:37109 with leader id 00000000-0000-0000-0000-000000000000.</span><br><span class="line">JobManager Web Interface: http://z07.sqa.net:37109</span><br></pre></td></tr></table></figure>
<p>客户端默认是attach模式，不会退出</p>
<ul>
<li>可以 ctrl + c 退出，然后再通过 ./bin/yarn-session.sh -id application_1532332183347_0726 连上来。</li>
<li>或者启动的时候用 -d 则为detached模式<br>Yarn上显示为 Flink session cluster</li>
</ul>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220618.jpg" alt=""></p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220648.jpg" alt=""></p>
<ul>
<li>在本机的临时目录（有些机器是 /tmp 目录）下会生成一个文件</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 cat /var/folders/2b/r6d49pcs23z43b8fqsyz885c0000gn/T/.yarn-properties-baoniu</span><br><span class="line">#Generated YARN properties file</span><br><span class="line">#Mon Jun 17 09:22:08 CST 2019</span><br><span class="line">parallelism=3</span><br><span class="line">dynamicPropertiesString=</span><br><span class="line">applicationID=application_1532332183347_0726</span><br></pre></td></tr></table></figure>
<h4 id="提交任务"><a href="#提交任务" class="headerlink" title="提交任务"></a>提交任务</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/flink run ./examples/batch/WordCount.jar</span><br></pre></td></tr></table></figure>
<p>将会根据 /tmp/.yarn-properties-admin 文件内容提交到了刚启动的 session。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 ./bin/flink run ./examples/batch/WordCount.jar</span><br><span class="line">2019-06-17 09:26:42,767 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - Found Yarn properties file under /var/folders/2b/r6d49pcs23z43b8fqsyz885c0000gn/T/.yarn-properties-baoniu.</span><br><span class="line">2019-06-17 09:26:42,767 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - Found Yarn properties file under /var/folders/2b/r6d49pcs23z43b8fqsyz885c0000gn/T/.yarn-properties-baoniu.</span><br><span class="line">2019-06-17 09:26:43,058 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - YARN properties set default parallelism to 3</span><br><span class="line">2019-06-17 09:26:43,058 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - YARN properties set default parallelism to 3</span><br><span class="line">YARN properties set default parallelism to 3</span><br><span class="line">2019-06-17 09:26:43,097 INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at z05c05217.sqa.zth.tbsite.net/11.163.188.29:8050</span><br><span class="line">2019-06-17 09:26:43,229 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar</span><br><span class="line">2019-06-17 09:26:43,229 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar</span><br><span class="line">2019-06-17 09:26:43,327 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Found application JobManager host name &apos;z05c07216.sqa.zth.tbsite.net&apos; and port &apos;37109&apos; from supplied application id &apos;application_1532332183347_0726&apos;</span><br><span class="line">Starting execution of program</span><br><span class="line">Executing WordCount example with default input data set.</span><br><span class="line">Use --input to specify file input.</span><br><span class="line">Printing result to stdout. Use --output to specify output path.</span><br><span class="line">^@(a,5)</span><br><span class="line">(action,1)</span><br><span class="line">(after,1)</span><br><span class="line">(against,1)</span><br><span class="line">(all,2)</span><br><span class="line">(and,12)</span><br><span class="line">... ...</span><br><span class="line">(wrong,1)</span><br><span class="line">(you,1)</span><br><span class="line">Program execution finished</span><br><span class="line">Job with JobID ad9b0f1feed6d0bf6ba4e0f18b1e65ef has finished.</span><br><span class="line">Job Runtime: 9152 ms</span><br><span class="line">Accumulator Results:</span><br><span class="line">- fd07c75d503d0d9a99e4f27dd153114c (java.util.ArrayList) [170 elements]</span><br></pre></td></tr></table></figure>
<p>运行结束后TM的资源会释放。</p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220705.jpg" alt=""></p>
<h4 id="提交到指定的session"><a href="#提交到指定的session" class="headerlink" title="提交到指定的session"></a>提交到指定的session</h4><p>通过 -yid 参数来提交到指定的session。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$./bin/flink run -d -p 30 -m yarn-cluster -yid application_1532332183347_0708 ./examples/streaming/TopSpeedWindowing.jar</span><br><span class="line"> </span><br><span class="line">2019-03-24 12:36:33,668 INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at z05c05217.sqa.zth.tbsite.net/11.163.188.29:8050</span><br><span class="line">2019-03-24 12:36:33,773 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar</span><br><span class="line">2019-03-24 12:36:33,773 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar</span><br><span class="line">2019-03-24 12:36:33,837 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Found application JobManager host name &apos;z05c05218.sqa.zth.tbsite.net&apos; and port &apos;60783&apos; from supplied application id &apos;application_1532332183347_0708&apos;</span><br><span class="line">Starting execution of program</span><br><span class="line">Executing TopSpeedWindowing example with default input data set.</span><br><span class="line">Use --input to specify file input.</span><br><span class="line">Printing result to stdout. Use --output to specify output path.</span><br><span class="line">Job has been submitted with JobID 58d5049ebbf28d515159f2f88563f5fd</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220735.jpg" alt=""></p>
<p>注：<a href="https://github.com/apache/flink/commits/blink" target="_blank" rel="noopener">blink版本</a> 的 session 与 <a href="https://zhoukaibo.com/tags/flink/">flink</a> 的 session 的区别：</p>
<ul>
<li>flink 的 session -n 参数不生效，而且不会提前启动TM</li>
<li>blink 的 session 可以通过 -n 指定启动多少个TM，而且TM会提前起来</li>
</ul>
<h2 id="Scala-Shell"><a href="#Scala-Shell" class="headerlink" title="Scala Shell"></a>Scala Shell</h2><p>官方文档：<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/scala_shell.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/scala_shell.html</a></p>
<h3 id="Deploy"><a href="#Deploy" class="headerlink" title="Deploy"></a>Deploy</h3><h4 id="local"><a href="#local" class="headerlink" title="local"></a>local</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$bin/start-scala-shell.sh local</span><br><span class="line">Starting Flink Shell:</span><br><span class="line">Starting local Flink cluster (host: localhost, port: 8081).</span><br><span class="line">Connecting to Flink cluster (host: localhost, port: 8081).</span><br><span class="line">... ...</span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure>
<p>任务运行说明：</p>
<ul>
<li>Batch 任务内置了 benv 变量，通过 print() 将结果输出到控制台</li>
<li>Streaming 任务内置了 senv 变量，通过 senv.execute(“job name”) 来提交任务，且Datastream的输出只有在 local 模式下打印到控制台。</li>
</ul>
<h4 id="remote"><a href="#remote" class="headerlink" title="remote"></a>remote</h4><p>先启动一个 yarn session cluster</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$./bin/yarn-session.sh  -tm 2048 -s 3</span><br><span class="line"></span><br><span class="line">2019-03-25 09:52:16,341 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.address, localhost</span><br><span class="line">2019-03-25 09:52:16,342 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.port, 6123</span><br><span class="line">2019-03-25 09:52:16,342 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.heap.size, 1024m</span><br><span class="line">2019-03-25 09:52:16,343 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.heap.size, 1024m</span><br><span class="line">2019-03-25 09:52:16,343 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.numberOfTaskSlots, 4</span><br><span class="line">2019-03-25 09:52:16,343 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: parallelism.default, 1</span><br><span class="line">2019-03-25 09:52:16,343 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: state.savepoints.dir, file:///tmp/savepoint</span><br><span class="line">2019-03-25 09:52:16,343 INFO  org.apache.flink.configuration.GlobalConfiguration            </span><br><span class="line">… ...</span><br><span class="line">Flink JobManager is now running on z054.sqa.net:28665 with leader id 00000000-0000-0000-0000-000000000000.</span><br><span class="line">JobManager Web Interface: http://z054.sqa.net:28665</span><br></pre></td></tr></table></figure>
<p>启动 scala shell，连到 jm</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$bin/start-scala-shell.sh remote z054.sqa.net 28665</span><br><span class="line">Starting Flink Shell:</span><br><span class="line">Connecting to Flink cluster (host: z054.sqa.net, port: 28665).</span><br><span class="line">... ...</span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure>
<h4 id="yarn"><a href="#yarn" class="headerlink" title="yarn"></a>yarn</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">$./bin/start-scala-shell.sh yarn -n 2 -jm 1024 -s 2 -tm 1024 -nm flink-yarn</span><br><span class="line"></span><br><span class="line">Starting Flink Shell:</span><br><span class="line">2019-03-25 09:47:44,695 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.address, localhost</span><br><span class="line">2019-03-25 09:47:44,697 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.port, 6123</span><br><span class="line">2019-03-25 09:47:44,697 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.heap.size, 1024m</span><br><span class="line">2019-03-25 09:47:44,697 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.heap.size, 1024m</span><br><span class="line">2019-03-25 09:47:44,697 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.numberOfTaskSlots, 4</span><br><span class="line">2019-03-25 09:47:44,698 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: parallelism.default, 1</span><br><span class="line">2019-03-25 09:47:44,698 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: state.savepoints.dir, file:///tmp/savepoint</span><br><span class="line">2019-03-25 09:47:44,698 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: rest.port, 8081</span><br><span class="line">2019-03-25 09:47:44,717 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - Found Yarn properties file under /tmp/.yarn-properties-admin.</span><br><span class="line">2019-03-25 09:47:45,041 INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at z05c05217.sqa.zth.tbsite.net/11.163.188.29:8050</span><br><span class="line">2019-03-25 09:47:45,098 WARN  org.apache.hadoop.util.NativeCodeLoader                       - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">2019-03-25 09:47:45,266 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar</span><br><span class="line">2019-03-25 09:47:45,275 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - The argument yn is deprecated in will be ignored.</span><br><span class="line">2019-03-25 09:47:45,357 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Cluster specification: ClusterSpecification&#123;masterMemoryMB=1024, taskManagerMemoryMB=1024, numberTaskManagers=2, slotsPerTaskManager=2&#125;</span><br><span class="line">2019-03-25 09:47:45,711 WARN  org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory       - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.</span><br><span class="line">2019-03-25 09:47:45,718 WARN  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - The configuration directory (&apos;/home/admin/flink/flink-1.7.2/conf&apos;) contains both LOG4J and Logback configuration files. Please delete or rename one of them.</span><br><span class="line">2019-03-25 09:47:46,514 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Submitting application master application_1532332183347_0710</span><br><span class="line">2019-03-25 09:47:46,534 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl         - Submitted application application_1532332183347_0710</span><br><span class="line">2019-03-25 09:47:46,534 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Waiting for the cluster to be allocated</span><br><span class="line">2019-03-25 09:47:46,535 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Deploying cluster, current state ACCEPTED</span><br><span class="line">2019-03-25 09:47:51,051 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - YARN application has been deployed successfully.</span><br><span class="line">2019-03-25 09:47:51,222 INFO  org.apache.flink.runtime.rest.RestClient                      - Rest client endpoint started.</span><br><span class="line"></span><br><span class="line">Connecting to Flink cluster (host: 10.10.10.10, port: 56942).</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220753.jpg" alt=""></p>
<p>按 CTRL + C 退出 shell 后，这个 flink cluster 还会继续运行，不会退出。</p>
<h3 id="Execute"><a href="#Execute" class="headerlink" title="Execute"></a>Execute</h3><h4 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/stop-cluster.sh</span><br><span class="line">No taskexecutor daemon to stop on host zkb-MBP.local.</span><br><span class="line">No standalonesession daemon to stop on host zkb-MBP.local.</span><br><span class="line">➜  flink-1.7.2 bin/start-scala-shell.sh local</span><br><span class="line">Starting Flink Shell:</span><br><span class="line">Starting local Flink cluster (host: localhost, port: 8081).</span><br><span class="line">Connecting to Flink cluster (host: localhost, port: 8081).</span><br><span class="line"></span><br><span class="line">scala&gt; val text = benv.fromElements(&quot;To be, or not to be,--that is the question:--&quot;)</span><br><span class="line">text: org.apache.flink.api.scala.DataSet[String] = org.apache.flink.api.scala.DataSet@5b407336</span><br><span class="line"></span><br><span class="line">scala&gt; val counts = text.flatMap &#123; _.toLowerCase.split(&quot;\\W+&quot;) &#125;.map &#123; (_, 1) &#125;.groupBy(0).sum(1)</span><br><span class="line">counts: org.apache.flink.api.scala.AggregateDataSet[(String, Int)] = org.apache.flink.api.scala.AggregateDataSet@6ee34fe4</span><br><span class="line"></span><br><span class="line">scala&gt; counts.print()</span><br><span class="line">(be,2)</span><br><span class="line">(is,1)</span><br><span class="line">(not,1)</span><br><span class="line">(or,1)</span><br><span class="line">(question,1)</span><br><span class="line">(that,1)</span><br><span class="line">(the,1)</span><br><span class="line">(to,2)</span><br></pre></td></tr></table></figure>
<p>对 DataSet 任务来说，print() 会触发任务的执行。</p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220816.jpg" alt=""></p>
<p>也可以将结果输出到文件（先删除 /tmp/ou1，不然会报错同名文件已经存在），继续执行以下命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; counts.writeAsText(&quot;/tmp/out1&quot;)</span><br><span class="line">res1: org.apache.flink.api.java.operators.DataSink[(String, Int)] = DataSink &apos;&lt;unnamed&gt;&apos; (TextOutputFormat (/tmp/out1) - UTF-8)</span><br><span class="line"></span><br><span class="line">scala&gt; benv.execute(&quot;batch test&quot;)</span><br><span class="line">res2: org.apache.flink.api.common.JobExecutionResult = org.apache.flink.api.common.JobExecutionResult@737652a9</span><br></pre></td></tr></table></figure>
<p>查看 /tmp/out1 文件就能看到输出结果。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 cat /tmp/out1</span><br><span class="line">(be,2)</span><br><span class="line">(is,1)</span><br><span class="line">(not,1)</span><br><span class="line">(or,1)</span><br><span class="line">(question,1)</span><br><span class="line">(that,1)</span><br><span class="line">(the,1)</span><br><span class="line">(to,2)</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220957.jpg" alt=""></p>
<h4 id="DataSteam"><a href="#DataSteam" class="headerlink" title="DataSteam"></a>DataSteam</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val textStreaming = senv.fromElements(&quot;To be, or not to be,--that is the question:--&quot;)</span><br><span class="line">textStreaming: org.apache.flink.streaming.api.scala.DataStream[String] = org.apache.flink.streaming.api.scala.DataStream@4970b93d</span><br><span class="line"></span><br><span class="line">scala&gt; val countsStreaming = textStreaming.flatMap &#123; _.toLowerCase.split(&quot;\\W+&quot;) &#125;.map &#123; (_, 1) &#125;.keyBy(0).sum(1)</span><br><span class="line">countsStreaming: org.apache.flink.streaming.api.scala.DataStream[(String, Int)] = org.apache.flink.streaming.api.scala.DataStream@6a478680</span><br><span class="line"></span><br><span class="line">scala&gt; countsStreaming.print()</span><br><span class="line">res3: org.apache.flink.streaming.api.datastream.DataStreamSink[(String, Int)] = org.apache.flink.streaming.api.datastream.DataStreamSink@42bfc11f</span><br><span class="line"></span><br><span class="line">scala&gt; senv.execute(&quot;Streaming Wordcount&quot;)</span><br><span class="line">(to,1)</span><br><span class="line">(be,1)</span><br><span class="line">(or,1)</span><br><span class="line">(not,1)</span><br><span class="line">(to,2)</span><br><span class="line">(be,2)</span><br><span class="line">(that,1)</span><br><span class="line">(is,1)</span><br><span class="line">(the,1)</span><br><span class="line">(question,1)</span><br><span class="line">res4: org.apache.flink.api.common.JobExecutionResult = org.apache.flink.api.common.JobExecutionResult@1878815a</span><br></pre></td></tr></table></figure>
<p>对 DataStream 任务，print() 并不会触发任务的执行，需要显式调用 execute(“job name”) 才会执行任务。</p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220839.jpg" alt=""></p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221018.jpg" alt=""></p>
<h4 id="TableAPI"><a href="#TableAPI" class="headerlink" title="TableAPI"></a>TableAPI</h4><p>在 Blink 开源版本里面，支持了 TableAPI 方式提交任务（可以用 btenv.sqlQuery 提交sql查询），社区版本 1.8 会支持 TableAPI: <a href="https://issues.apache.org/jira/browse/FLINK-9555" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/FLINK-9555</a></p>
<h2 id="SQL-Client-Beta"><a href="#SQL-Client-Beta" class="headerlink" title="SQL Client Beta"></a>SQL Client Beta</h2><p>SQL Client 目前还只是测试版，处于开发阶段，只能用于SQL的原型验证，不推荐在生产环境使用。</p>
<h3 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/start-cluster.sh</span><br><span class="line">Starting cluster.</span><br><span class="line">Starting standalonesession daemon on host zkb-MBP.local.</span><br><span class="line">Starting taskexecutor daemon on host zkb-MBP.local.</span><br><span class="line"></span><br><span class="line">➜  flink-1.7.2 ./bin/sql-client.sh embedded</span><br><span class="line">No default environment specified.</span><br><span class="line">Searching for &apos;/Users/baoniu/Documents/work/tool/flink/flink-1.7.2/conf/sql-client-defaults.yaml&apos;...found.</span><br><span class="line">Reading default environment from: file:/Users/baoniu/Documents/work/tool/flink/flink-1.7.2/conf/sql-client-defaults.yaml</span><br><span class="line">No session environment specified.</span><br><span class="line">Validating current environment...done.</span><br><span class="line">… …</span><br><span class="line"></span><br><span class="line">Flink SQL&gt; help;</span><br><span class="line">The following commands are available:</span><br><span class="line"></span><br><span class="line">QUIT		Quits the SQL CLI client.</span><br><span class="line">CLEAR		Clears the current terminal.</span><br><span class="line">HELP		Prints the available commands.</span><br><span class="line">SHOW TABLES		Shows all registered tables.</span><br><span class="line">SHOW FUNCTIONS		Shows all registered user-defined functions.</span><br><span class="line">DESCRIBE		Describes the schema of a table with the given name.</span><br><span class="line">EXPLAIN		Describes the execution plan of a query or table with the given name.</span><br><span class="line">SELECT		Executes a SQL SELECT query on the Flink cluster.</span><br><span class="line">INSERT INTO		Inserts the results of a SQL SELECT query into a declared table sink.</span><br><span class="line">CREATE VIEW		Creates a virtual table from a SQL query. Syntax: &apos;CREATE VIEW &lt;name&gt; AS &lt;query&gt;;&apos;</span><br><span class="line">DROP VIEW		Deletes a previously created virtual table. Syntax: &apos;DROP VIEW &lt;name&gt;;&apos;</span><br><span class="line">SOURCE		Reads a SQL SELECT query from a file and executes it on the Flink cluster.</span><br><span class="line">SET		Sets a session configuration property. Syntax: &apos;SET &lt;key&gt;=&lt;value&gt;;&apos;. Use &apos;SET;&apos; for listing all properties.</span><br><span class="line">RESET		Resets all session configuration properties.</span><br><span class="line"></span><br><span class="line">Hint: Make sure that a statement ends with &apos;;&apos; for finalizing (multi-line) statements.</span><br></pre></td></tr></table></figure>
<h4 id="select-查询"><a href="#select-查询" class="headerlink" title="select 查询"></a>select 查询</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Flink SQL&gt; SELECT &apos;Hello World&apos;;</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221031.jpg" alt=""></p>
<p>按 ”Q” 退出这个界面<br>打开 <a href="http://127.0.0.1:8081" target="_blank" rel="noopener">http://127.0.0.1:8081</a> 能看到这条 select 语句产生的查询任务已经结束了。这个查询采用的是读取固定数据集的 Custom Source，输出用的是 Stream Collect Sink，且只输出一条结果。</p>
<p>注意：如果本机的临时目录存在类似 .yarn-properties-baoniu 的文件，任务会提交到 yarn 上。</p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221045.jpg" alt=""></p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221059.jpg" alt=""></p>
<h4 id="explain"><a href="#explain" class="headerlink" title="explain"></a>explain</h4><p>explain 命令可以查看 SQL 的执行计划。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Flink SQL&gt; explain SELECT name, COUNT(*) AS cnt FROM (VALUES (&apos;Bob&apos;), (&apos;Alice&apos;), (&apos;Greg&apos;), (&apos;Bob&apos;)) AS NameTable(name) GROUP BY name;</span><br><span class="line"></span><br><span class="line">== Abstract Syntax Tree ==        // 抽象语法树</span><br><span class="line">LogicalAggregate(group=[&#123;0&#125;], cnt=[COUNT()])</span><br><span class="line">  LogicalValues(tuples=[[&#123; _UTF-16LE&apos;Bob  &apos; &#125;, &#123; _UTF-16LE&apos;Alice&apos; &#125;, &#123; _UTF-16LE&apos;Greg &apos; &#125;, &#123; _UTF-16LE&apos;Bob  &apos; &#125;]])</span><br><span class="line"></span><br><span class="line">== Optimized Logical Plan ==     // 优化后的逻辑执行计划</span><br><span class="line">DataStreamGroupAggregate(groupBy=[name], select=[name, COUNT(*) AS cnt])</span><br><span class="line">  DataStreamValues(tuples=[[&#123; _UTF-16LE&apos;Bob  &apos; &#125;, &#123; _UTF-16LE&apos;Alice&apos; &#125;, &#123; _UTF-16LE&apos;Greg &apos; &#125;, &#123; _UTF-16LE&apos;Bob  &apos; &#125;]])</span><br><span class="line"></span><br><span class="line">== Physical Execution Plan ==   // 物理执行计划</span><br><span class="line">Stage 3 : Data Source</span><br><span class="line">	content : collect elements with CollectionInputFormat</span><br><span class="line"></span><br><span class="line">	Stage 5 : Operator</span><br><span class="line">		content : groupBy: (name), select: (name, COUNT(*) AS cnt)</span><br><span class="line">		ship_strategy : HASH</span><br></pre></td></tr></table></figure>
<h3 id="结果展示"><a href="#结果展示" class="headerlink" title="结果展示"></a>结果展示</h3><p>SQL Client 支持两种模式来维护并展示查询结果：</p>
<ul>
<li><p>table mode: 在内存中物化查询结果，并以分页 table 形式展示。用户可以通过以下命令启用 table mode：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SET execution.result-mode=table</span><br></pre></td></tr></table></figure>
</li>
<li><p>changlog mode: 不会物化查询结果，而是直接对 continuous query 产生的添加和撤回（retractions）结果进行展示。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SET execution.result-mode=changelog</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>接下来通过实际的例子进行演示。</p>
<h4 id="table-mode"><a href="#table-mode" class="headerlink" title="table mode"></a>table mode</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Flink SQL&gt; SET execution.result-mode=table;</span><br><span class="line">[INFO] Session property has been set.</span><br><span class="line"></span><br><span class="line">Flink SQL&gt; SELECT name, COUNT(*) AS cnt FROM (VALUES (&apos;Bob&apos;), (&apos;Alice&apos;), (&apos;Greg&apos;), (&apos;Bob&apos;)) AS NameTable(name) GROUP BY name;</span><br></pre></td></tr></table></figure>
<p>运行结果如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221114.jpg" alt=""></p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221131.jpg" alt=""></p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221148.jpg" alt=""></p>
<h4 id="changlog-mode"><a href="#changlog-mode" class="headerlink" title="changlog mode"></a>changlog mode</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Flink SQL&gt; SET execution.result-mode=changelog;</span><br><span class="line">[INFO] Session property has been set.</span><br><span class="line"></span><br><span class="line">Flink SQL&gt; SELECT name, COUNT(*) AS cnt FROM (VALUES (&apos;Bob&apos;), (&apos;Alice&apos;), (&apos;Greg&apos;), (&apos;Bob&apos;)) AS NameTable(name) GROUP BY name;</span><br></pre></td></tr></table></figure>
<p>运行结果如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221204.jpg" alt=""></p>
<p>其中 ‘-’ 代表的就是撤回消息。<br><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221216.jpg" alt=""></p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221231.jpg" alt=""></p>
<h3 id="Environment-Files"><a href="#Environment-Files" class="headerlink" title="Environment Files"></a>Environment Files</h3><p>目前的 SQL Client 还不支持 DDL 语句，只能通过 yaml 文件的方式来定义 SQL 查询需要的表，udf 和运行参数等信息。</p>
<p>首先，准备 env.yaml 和 input.csv 两个文件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 cat /tmp/env.yaml</span><br><span class="line">tables:</span><br><span class="line">  - name: MyTableSource</span><br><span class="line">    type: source-table</span><br><span class="line">    update-mode: append</span><br><span class="line">    connector:</span><br><span class="line">      type: filesystem</span><br><span class="line">      path: &quot;/tmp/input.csv&quot;</span><br><span class="line">    format:</span><br><span class="line">      type: csv</span><br><span class="line">      fields:</span><br><span class="line">        - name: MyField1</span><br><span class="line">          type: INT</span><br><span class="line">        - name: MyField2</span><br><span class="line">          type: VARCHAR</span><br><span class="line">      line-delimiter: &quot;\n&quot;</span><br><span class="line">      comment-prefix: &quot;#&quot;</span><br><span class="line">    schema:</span><br><span class="line">      - name: MyField1</span><br><span class="line">        type: INT</span><br><span class="line">      - name: MyField2</span><br><span class="line">        type: VARCHAR</span><br><span class="line">  - name: MyCustomView</span><br><span class="line">    type: view</span><br><span class="line">    query: &quot;SELECT MyField2 FROM MyTableSource&quot;</span><br><span class="line">  - name: MyTableSink</span><br><span class="line">    type: sink-table</span><br><span class="line">    update-mode: append</span><br><span class="line">    connector:</span><br><span class="line">      type: filesystem</span><br><span class="line">      path: &quot;/tmp/output.csv&quot;</span><br><span class="line">    format:</span><br><span class="line">      type: csv</span><br><span class="line">      fields:</span><br><span class="line">        - name: MyField1</span><br><span class="line">          type: INT</span><br><span class="line">        - name: MyField2</span><br><span class="line">          type: VARCHAR</span><br><span class="line">    schema:</span><br><span class="line">      - name: MyField1</span><br><span class="line">        type: INT</span><br><span class="line">      - name: MyField2</span><br><span class="line">        type: VARCHAR</span><br><span class="line">        </span><br><span class="line"># Execution properties allow for changing the behavior of a table program.</span><br><span class="line"></span><br><span class="line">execution:</span><br><span class="line">  type: streaming                   # required: execution mode either &apos;batch&apos; or &apos;streaming&apos;</span><br><span class="line">  result-mode: table                # required: either &apos;table&apos; or &apos;changelog&apos;</span><br><span class="line">  max-table-result-rows: 1000000    # optional: maximum number of maintained rows in</span><br><span class="line">                                    #   &apos;table&apos; mode (1000000 by default, smaller 1 means unlimited)</span><br><span class="line">  time-characteristic: event-time   # optional: &apos;processing-time&apos; or &apos;event-time&apos; (default)</span><br><span class="line">  parallelism: 1                    # optional: Flink&apos;s parallelism (1 by default)</span><br><span class="line">  periodic-watermarks-interval: 200 # optional: interval for periodic watermarks (200 ms by default)</span><br><span class="line">  max-parallelism: 16               # optional: Flink&apos;s maximum parallelism (128 by default)</span><br><span class="line">  min-idle-state-retention: 0       # optional: table program&apos;s minimum idle state time</span><br><span class="line">  max-idle-state-retention: 0       # optional: table program&apos;s maximum idle state time</span><br><span class="line">  restart-strategy:                 # optional: restart strategy</span><br><span class="line">    type: fallback                  #   &quot;fallback&quot; to global restart strategy by default</span><br><span class="line"></span><br><span class="line"># Deployment properties allow for describing the cluster to which table programs are submitted to.</span><br><span class="line"></span><br><span class="line">deployment:</span><br><span class="line">  response-timeout: 5000</span><br><span class="line"></span><br><span class="line">➜  flink-1.7.2 cat /tmp/input.csv</span><br><span class="line">1,hello</span><br><span class="line">2,world</span><br><span class="line">3,hello world</span><br><span class="line">1,ok</span><br><span class="line">3,bye bye</span><br><span class="line">4,yes</span><br></pre></td></tr></table></figure>
<p>启动 SQL Client：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 ./bin/sql-client.sh embedded -e /tmp/env.yaml</span><br><span class="line">No default environment specified.</span><br><span class="line">Searching for &apos;/Users/baoniu/Documents/work/tool/flink/flink-1.7.2/conf/sql-client-defaults.yaml&apos;...found.</span><br><span class="line">Reading default environment from: file:/Users/baoniu/Documents/work/tool/flink/flink-1.7.2/conf/sql-client-defaults.yaml</span><br><span class="line">Reading session environment from: file:/tmp/env.yaml</span><br><span class="line">Validating current environment...done.</span><br><span class="line"></span><br><span class="line">Flink SQL&gt; show tables;</span><br><span class="line">MyCustomView</span><br><span class="line">MyTableSink</span><br><span class="line">MyTableSource</span><br><span class="line"></span><br><span class="line">Flink SQL&gt; describe MyTableSource;</span><br><span class="line">root</span><br><span class="line"> |-- MyField1: Integer</span><br><span class="line"> |-- MyField2: String</span><br><span class="line"></span><br><span class="line">Flink SQL&gt; describe MyCustomView;</span><br><span class="line">root</span><br><span class="line"> |-- MyField2: String</span><br><span class="line"></span><br><span class="line">Flink SQL&gt; create view MyView1 as select MyField1 from MyTableSource;</span><br><span class="line">[INFO] View has been created.</span><br><span class="line"></span><br><span class="line">Flink SQL&gt; show tables;</span><br><span class="line">MyCustomView</span><br><span class="line">MyTableSource</span><br><span class="line">MyView1</span><br><span class="line"></span><br><span class="line">Flink SQL&gt; describe MyView1;</span><br><span class="line">root</span><br><span class="line"> |-- MyField1: Integer</span><br><span class="line"></span><br><span class="line">Flink SQL&gt; select * from MyTableSource;</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221246.jpg" alt=""></p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221256.jpg" alt=""></p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221306.jpg" alt=""></p>
<p>使用 insert into 写入结果表：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Flink SQL&gt; insert into MyTableSink select * from MyTableSource;</span><br><span class="line">[INFO] Submitting SQL update statement to the cluster...</span><br><span class="line">[INFO] Table update statement has been successfully submitted to the cluster:</span><br><span class="line">Cluster ID: StandaloneClusterId</span><br><span class="line">Job ID: 3fac2be1fd891e3e07595c684bb7b7a0</span><br><span class="line">Web interface: http://localhost:8081</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221321.jpg" alt=""></p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221339.jpg" alt=""></p>
<p>查询生成的结果数据文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 cat /tmp/output.csv</span><br><span class="line">1,hello</span><br><span class="line">2,world</span><br><span class="line">3,hello world</span><br><span class="line">1,ok</span><br><span class="line">3,bye bye</span><br><span class="line">4,yes</span><br></pre></td></tr></table></figure>
<p>也可以在 environment 文件里面定义udf，在 SQL Client 里面通过 『SHOW FUNCTIONS』查询和使用，这里就不再说明了。</p>
<p>SQL Client 功能社区还在开发中，详见 <a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-24+-+SQL+Client" target="_blank" rel="noopener">FLIP-24</a>。</p>
<h2 id="Restful-API"><a href="#Restful-API" class="headerlink" title="Restful API"></a>Restful API</h2><p>接下来我们演示如何通过 Rest API 来提交jar包和执行任务。</p>
<p>更详细的操作请参考 flink 的 Restful API 文档：<a href="https://ci.apache.org/projects/flink/flink-docs-stable/monitoring/rest_api.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-stable/monitoring/rest_api.html</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 curl http://127.0.0.1:8081/overview</span><br><span class="line">&#123;&quot;taskmanagers&quot;:1,&quot;slots-total&quot;:4,&quot;slots-available&quot;:0,&quot;jobs-running&quot;:3,&quot;jobs-finished&quot;:0,&quot;jobs-cancelled&quot;:0,&quot;jobs-failed&quot;:0,&quot;flink-version&quot;:&quot;1.7.2&quot;,&quot;flink-commit&quot;:&quot;ceba8af&quot;&#125;%</span><br><span class="line"></span><br><span class="line">➜  flink-1.7.2 curl -X POST -H &quot;Expect:&quot; -F &quot;jarfile=@/Users/baoniu/Documents/work/tool/flink/flink-1.7.2/examples/streaming/TopSpeedWindowing.jar&quot; http://127.0.0.1:8081/jars/upload</span><br><span class="line">&#123;&quot;filename&quot;:&quot;/var/folders/2b/r6d49pcs23z43b8fqsyz885c0000gn/T/flink-web-124c4895-cf08-4eec-8e15-8263d347efc2/flink-web-upload/6077eca7-6db0-4570-a4d0-4c3e05a5dc59_TopSpeedWindowing.jar&quot;,&quot;status&quot;:&quot;success&quot;&#125;%       </span><br><span class="line">                                                                                                                                                                                                   ➜  flink-1.7.2 curl http://127.0.0.1:8081/jars</span><br><span class="line">&#123;&quot;address&quot;:&quot;http://localhost:8081&quot;,&quot;files&quot;:[&#123;&quot;id&quot;:&quot;6077eca7-6db0-4570-a4d0-4c3e05a5dc59_TopSpeedWindowing.jar&quot;,&quot;name&quot;:&quot;TopSpeedWindowing.jar&quot;,&quot;uploaded&quot;:1553743438000,&quot;entry&quot;:[&#123;&quot;name&quot;:&quot;org.apache.flink.streaming.examples.windowing.TopSpeedWindowing&quot;,&quot;description&quot;:null&#125;]&#125;]&#125;%</span><br><span class="line">                                                                                                                                         </span><br><span class="line">➜  flink-1.7.2 curl http://127.0.0.1:8081/jars/6077eca7-6db0-4570-a4d0-4c3e05a5dc59_TopSpeedWindowing.jar/plan</span><br><span class="line">&#123;&quot;plan&quot;:&#123;&quot;jid&quot;:&quot;41029eb3feb9132619e454ec9b2a89fb&quot;,&quot;name&quot;:&quot;CarTopSpeedWindowingExample&quot;,&quot;nodes&quot;:[&#123;&quot;id&quot;:&quot;90bea66de1c231edf33913ecd54406c1&quot;,&quot;parallelism&quot;:1,&quot;operator&quot;:&quot;&quot;,&quot;operator_strategy&quot;:&quot;&quot;,&quot;description&quot;:&quot;Window(GlobalWindows(), DeltaTrigger, TimeEvictor, ComparableAggregator, PassThroughWindowFunction) -&amp;gt; Sink: Print to Std. Out&quot;,&quot;inputs&quot;:[&#123;&quot;num&quot;:0,&quot;id&quot;:&quot;cbc357ccb763df2852fee8c4fc7d55f2&quot;,&quot;ship_strategy&quot;:&quot;HASH&quot;,&quot;exchange&quot;:&quot;pipelined_bounded&quot;&#125;],&quot;optimizer_properties&quot;:&#123;&#125;&#125;,&#123;&quot;id&quot;:&quot;cbc357ccb763df2852fee8c4fc7d55f2&quot;,&quot;parallelism&quot;:1,&quot;operator&quot;:&quot;&quot;,&quot;operator_strategy&quot;:&quot;&quot;,&quot;description&quot;:&quot;Source: Custom Source -&amp;gt; Timestamps/Watermarks&quot;,&quot;optimizer_properties&quot;:&#123;&#125;&#125;]&#125;&#125;%                                                                                                                                                    ➜  flink-1.7.2 curl -X POST http://127.0.0.1:8081/jars/6077eca7-6db0-4570-a4d0-4c3e05a5dc59_TopSpeedWindowing.jar/run</span><br><span class="line">&#123;&quot;jobid&quot;:&quot;04d80a24b076523d3dc5fbaa0ad5e1ad&quot;&#125;%</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221355.jpg" alt=""></p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221406.jpg" alt=""></p>
<p>Restful API 还提供了很多监控和 Metrics 相关的功能，对于任务提交的操作也支持的比较全面。</p>
<h2 id="Web"><a href="#Web" class="headerlink" title="Web"></a>Web</h2><p>在 <a href="https://zhoukaibo.com/tags/flink/">Flink</a> Dashboard 页面左侧可以看到有个『Submit new Job』的地方，用户可以上传 jar 包和显示执行计划和提交任务。Web 提交功能主要用于新手入门和演示用。</p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221421.jpg" alt=""></p>
<h1 id="结束"><a href="#结束" class="headerlink" title="结束"></a>结束</h1><p>本期的课程到这里就结束了，我们主要讲解了 Flink 的 5 种任务提交的方式。熟练掌握各种任务提交方式，有利于提高我们日常的开发和运维效率。</p>
</div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>kaibo</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2019/10/15/apache-flink-intro-client/">https://zhoukaibo.com/2019/10/15/apache-flink-intro-client/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>转载请保留作者及链接</li></ul></div><br><div class="tags"><a href="/tags/flink/">flink</a></div><div class="post-nav"><a class="pre" href="/2019/10/15/flink-yarn-k8s-principle-practice/">Flink on Yarn / Kubernetes 原理剖析及实践</a><a class="next" href="/2019/03/16/java-spi/">Java SPI 机制分析及其优缺点</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'https://zhoukaibo.com/2019/10/15/apache-flink-intro-client/';
    this.page.identifier = '2019/10/15/apache-flink-intro-client/';
    this.page.title = 'Apache Flink 客户端操作';
  };</script><script type="text/javascript" id="disqus-lazy-load-script">$.ajax({
url: 'https://disqus.com/next/config.json',
timeout: 2500,
type: 'GET',
success: function(){
  var d = document;
  var s = d.createElement('script');
  s.src = '//flying5.disqus.com/embed.js';
  s.setAttribute('data-timestamp', + new Date());
  (d.head || d.body).appendChild(s);
  $('.disqus_click_btn').css('display', 'none');
},
error: function() {
  $('.disqus_click_btn').css('display', 'block');
}
});</script><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//flying5.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://flying5.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });
</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="https://zhoukaibo.com"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/java/" style="font-size: 15px;">java</a> <a href="/tags/spi/" style="font-size: 15px;">spi</a> <a href="/tags/flink/" style="font-size: 15px;">flink</a> <a href="/tags/kafka/" style="font-size: 15px;">kafka</a> <a href="/tags/exactly-once/" style="font-size: 15px;">exactly-once</a> <a href="/tags/at-least-once/" style="font-size: 15px;">at-least-once</a> <a href="/tags/kubernetes/" style="font-size: 15px;">kubernetes</a> <a href="/tags/yarn/" style="font-size: 15px;">yarn</a> <a href="/tags/k8s/" style="font-size: 15px;">k8s</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/02/18/Flink-1-10-Native-Kubernetes/">Flink 1.10 Native Kubernetes 原理与实践</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/15/flink-yarn-k8s-principle-practice/">Flink on Yarn / Kubernetes 原理剖析及实践</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/15/apache-flink-intro-client/">Apache Flink 客户端操作</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/16/java-spi/">Java SPI 机制分析及其优缺点</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/02/14/exactly-once-exactly-same/">谈谈流计算中的『Exactly Once』特性</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/10/flink-kafka-exactly-once/">Apache Flink结合Kafka构建端到端的Exactly-Once处理</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/10/hello-world/">Hello World</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-comment-o"> 最近评论</i></div><script type="text/javascript" src="//flying5.disqus.com/recent_comments_widget.js?num_items=5&amp;hide_avatars=1&amp;avatar_size=32&amp;excerpt_length=20&amp;hide_mods=1"></script></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2020 <a href="/." rel="nofollow">Kaibo's Blog</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.1" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.1" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.1"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.1"></script></div></body></html>