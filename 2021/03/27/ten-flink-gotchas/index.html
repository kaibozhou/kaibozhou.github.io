<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>使用 Flink 前需要知道的10个『陷阱』 | Kaibo's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.1"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.3.1/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/rss+xml" href="/rss2.xml"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-131716279-1','auto');ga('send','pageview');
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = 'https://hm.baidu.com/hm.js?' + 'adcf6eb324ad0475bb957857cf925dda';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">使用 Flink 前需要知道的10个『陷阱』</h1><a id="logo" href="/.">Kaibo's Blog</a><p class="description">Real-time big data processing</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">使用 Flink 前需要知道的10个『陷阱』</h1><div class="post-meta">Mar 27, 2021<script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><a class="disqus-comment-count" data-disqus-identifier="2021/03/27/ten-flink-gotchas/" href="/2021/03/27/ten-flink-gotchas/#disqus_thread"></a><div class="post-content"><p>Contentsquare 公司的 Robin 总结了他们将 Spark 任务迁移到 Flink 遇到的 10 个『陷阱』。对于第一次将 Flink 用于生产环境的用户来说，这些经验非常有参考意义。</p>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>采用新的框架总是会带来很多惊喜。当你花了几天时间去排查为什么服务运行异常，结果发现只是因为某个功能的用法不对或者缺少一些简单的配置。</p>
<p>在 Contentsquare，我们需要不断升级数据处理任务，以满足越来越多的数据上的苛刻需求。这也是为什么我们决定将用于会话处理的小时级 Spark 任务迁移到 Flink 流服务。这样我们就可以利用 Flink 更为健壮的处理能力，提供更实时的数据给用户，并能提供历史数据。不过这并不轻松，我们的团队在上面工作了有一年时间。同时，我们也遇到了一些令人惊讶的问题，本文将尝试帮助你避免这些陷阱。</p>
<h2 id="1-并行度设置导致的负载倾斜"><a href="#1-并行度设置导致的负载倾斜" class="headerlink" title="1 - 并行度设置导致的负载倾斜"></a>1 - 并行度设置导致的负载倾斜</h2><p>我们从一个简单的问题开始：在 Flink UI 中调查某个作业的子任务时，关于每个子任务处理的数据量，你可能会遇到如下这种奇怪的情况。</p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20210327125449.png" alt="每个子任务的工作负载并不均衡"></p>
<p>这表明每个子任务的算子没有收到相同数量的 Key Groups，它代表所有可能的 key 的一部分。如果一个算子收到了 1 个 Key Group，而另外一个算子收到了 2 个，则第二个子任务很可能需要完成两倍的工作。查看 Flink 的代码，我们可以找到以下函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">public static int computeOperatorIndexForKeyGroup(</span><br><span class="line">        int maxParallelism, int parallelism, int keyGroupId) &#123;</span><br><span class="line">    return keyGroupId * parallelism / maxParallelism;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其目的是将所有 Key Groups 分发给实际的算子。Key Groups 的总数由 maxParallelism 参数决定，而算子的数量和 parallelism 相同。这里最大的问题是 maxParallelism 的默认值，它默认等于 operatorParallelism + (operatorParallelism / 2) 。假如我们设置 parallelism 为10，那么 maxParallelism 为 15 （实际最大并发度值的下限是 128 ，上限是 32768，这里只是为了方便举例）。这样，根据上面的函数，我们可以计算出哪些算子会分配给哪些 Key Group。</p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20210327125628.png" alt="在默认配置下，部分算子分配了两个Key Group，部分算子只分配了1个"></p>
<p>解决这个问题非常容易：设置并发度的时候，还要为 maxParallelism 设置一个值，且该值为 parallelism 的倍数。这将让负载更加均衡，同时方便以后扩展。</p>
<h2 id="2-注意-mapWithState-amp-TTL-的重要性"><a href="#2-注意-mapWithState-amp-TTL-的重要性" class="headerlink" title="2 - 注意 mapWithState &amp; TTL 的重要性"></a>2 - 注意 mapWithState &amp; TTL 的重要性</h2><p>在处理包含无限多键的数据时，要考虑到 keyed 状态保留策略（通过 TTL 定时器来在给定的时间之后清理未使用的数据）是很重要的。术语『无限』在这里有点误导，因为如果你要处理的 key 以 128 位编码，则 key 的最大数量将会有个限制（等于2的128次方）。但这是一个巨大的数字！你可能无法在状态中存储那么多值，所以最好考虑你的键空间是无界的，同时新键会随着时间不断出现。</p>
<p>如果你的 keyed 状态包含在某个Flink的默认窗口中，则将是安全的：即使未使用TTL，在处理窗口的元素时也会注册一个清除计时器，该计时器将调用 clearAllState 函数，并删除与该窗口关联的状态及其元数据。</p>
<p>如果要使用 Keyed State Descriptor 来管理状态，可以很方便地添加TTL配置，以确保在状态中的键数量不会无限制地增加。</p>
<p>但是，你可能会想使用更简便的 mapWithState 方法，该方法可让你访问 valueState 并隐藏操作的复杂性。虽然这对于测试和少量键的数据来说是很好的选择，但如果在生产环境中遇到无限多键值时，会引发问题。由于状态是对你隐藏的，因此你无法设置TTL，并且默认情况下未配置任何TTL。这就是为什么值得考虑做一些额外工作的原因，如声明诸如RichMapFunction之类的东西，这将使你能更好的控制状态的生命周期。</p>
<h2 id="3-从检查点还原和重新分区"><a href="#3-从检查点还原和重新分区" class="headerlink" title="3 - 从检查点还原和重新分区"></a>3 - 从检查点还原和重新分区</h2><p>在使用大状态时，有必要使用增量检查点（incremental checkpointing）。在我们的案例中，任务的完整状态约为8TB，我们将检查点配置为每15分钟做一次。由于检查点是增量式的，因此我们只能设法每15分钟将大约100GB的数据发送到对象存储，这是一种更快的方式并且网络占用较少。这对于容错效果很好，但是在更新任务时我们也需要检索状态。常用的方法是为正在运行的作业创建一个保存点（savepoint），以可移植的格式包含整个状态。</p>
<p>但是，在我们的情况下，保存点可能需要几个小时才能完成，这使得每次发布版本都是一个漫长而麻烦的过程。相反，我们决定使用保留检查点（Retained Checkpoints）。设置此参数后，我们可以通过从上一个作业的检查点恢复状态来加快发布速度，而不必触发冗长的保存点！</p>
<p>此外，尽管保存点比检查点具有更高的可移植性，但您仍然可以使用保留的检查点来更改作业的分区（它可能不适用于所有类型的作业，所以最好对其进行测试）。这与从保存点重新分区完全一样，但是不需要经历Flink在TaskManager之间重新分配数据的漫长过程。当我们尝试这样做时，大约花了8个小时才完成，这是不可持续的。幸运的是，由于我们使用的是RocksDB状态后端，因此我们可以在这步中增加更多线程以提高其速度。这是通过将以下两个参数从1增加到8来完成的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">state.backend.rocksdb.checkpoint.transfer.thread.num: 8</span><br><span class="line">state.backend.rocksdb.thread.num: 8</span><br></pre></td></tr></table></figure>
<p>使用保留的检查点，并增加分配给RocksDB传输的线程数，能将发布和重新分区时间减少10倍！</p>
<h2 id="4-提前增加日志记录"><a href="#4-提前增加日志记录" class="headerlink" title="4 - 提前增加日志记录"></a>4 - 提前增加日志记录</h2><p>这一点可能看起来很明显，但也很容易忘记。开发作业时，请记住它将运行很长时间，并且可能会处理意外的数据。发生这种情况时，你将需要尽可能多的信息来调查发生了什么，而不必通过再次回溯相同的数据来重现问题。</p>
<p>我们的任务是将事件汇总在一起，并根据特定规则进行合并。这些规则中的某些规则在大多数情况下性能还可以，但是当有数据倾斜时却要花费很长时间。当我们发现任务卡住了3个小时，却不知道它在做什么。似乎只有一个TaskManager的CPU可以正常工作，因此我们怀疑是特定数据导致我们的算法性能下降。</p>
<p>最终处理完数据后，一切恢复正常，但是我们不知道从哪开始检查！这就是为什么我们为这些情况添加了一些预防性日志的原因：在处理窗口时，我们会测量花费的时间。只要计算窗口所需的时间超过1分钟，我们就会记录下所有可能的数据。这对于准确了解导致性能下降的倾斜是非常有帮助的，并且当再次发生这种情况时，我们能够定位到合并过程处理慢的部分原因。假如收到的是重复的数据，则可能确实需要几个小时。当然，重要的是不要过多地记录信息，因为这会降低性能。因此，请尝试找到仅在异常情况下才显示信息的阈值。</p>
<h2 id="5-如何找出卡住的作业实际上在做什么"><a href="#5-如何找出卡住的作业实际上在做什么" class="headerlink" title="5 - 如何找出卡住的作业实际上在做什么"></a>5 - 如何找出卡住的作业实际上在做什么</h2><p>对上述问题的调查也使我们意识到，我们需要找到一种简单的方法，来定位作业疑似卡住时当前正在运行的代码段。幸运的是，有一个简单的方法可以做到这一点！首先，您将需要配置TaskManagers的JMX以接受远程监视。在Kubernetes部署中，我们可以通过三个步骤连接到JMX：</p>
<p>首先，将此属性添加到我们的flink-conf.yaml中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.java.opts: &quot;-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.port=1099 -Dcom.sun.management.jmxremote.rmi.port=1099 -Djava.rmi.server.hostname=127.0.0.1&quot;​</span><br></pre></td></tr></table></figure>
<p>然后，将本地端口1099转发到TaskManager的pod中的端口<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl port-forward flink-taskmanager-4 1099</span><br></pre></td></tr></table></figure></p>
<p>最后，打开 jconsole<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ jconsole 127.0.0.1:1099​</span><br></pre></td></tr></table></figure></p>
<p>这使您可以轻松地在JVM上查看目标TaskManager的信息。对于卡住的作业，我们以正在运行的唯一一个TaskManager为目标，分析了正在运行的线程：</p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20210327125702.png" alt="JConsole向我们展示了每个线程当前正在做什么"></p>
<p>深入研究，我们可以看到所有线程都在等待，除了其中一个（在上面的屏幕截图中已突出显示）。这使得我们能够快速发现作业是卡在哪个方法调用里面的，并轻松修复！</p>
<h2 id="6-将数据从一种状态迁移到另一种状态的风险"><a href="#6-将数据从一种状态迁移到另一种状态的风险" class="headerlink" title="6 - 将数据从一种状态迁移到另一种状态的风险"></a>6 - 将数据从一种状态迁移到另一种状态的风险</h2><p>根据你的实际情况，可能需要保留两个具有不同语义的不同状态描述符。例如，我们通过WindowContent状态为进行中的会话累积事件，接着将处理后的会话移动到称为 HistoricalSessions 的 ValueState 中。第二个状态为了防止后面需要用到会保留几天，直到TTL过期丢弃它为止。</p>
<p>我们做的第一个测试运行良好：我们可以发送额外的数据到已处理的会话，这将为相同的键创建一个新窗口。在窗口的处理过程中，我们会从 HistoricalSessions 状态中获取数据，以将新数据与旧会话合并，并且结果会话是历史会话的增强版本，这也正是我们所期望的。</p>
<p>在执行此操作时，我们遇到过几次内存问题。经过几次测试后，我们了解到OOM仅在将旧数据发送到Flink时才发生（即，发送数据的时间戳早于其当前水印）。这使得我们发现了当前处理方式中的一个大问题：当接收到旧数据时，Flink将其与旧窗口合并，而旧窗口的数据仍在WindowContent状态内（这可以通过设置AllowedLateness实现）。然后结果窗口会与HistoricalSessions内容合并，该内容还包含旧的数据。最终我们得到的是重复的事件，在同一会话中收到一些事件后，每个事件都将有数千条重复，从而导致了OOM。</p>
<p>这个问题的解法非常简单：我们希望WindowContent在将其内容移至第二个状态之前自动清除。我们使用了Flink的PurgingTrigger来达到这个目的，当窗口触发时，该消息会发送一条清除状态内容的消息。具体代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// Purging the window&apos;s content allows us to receive late events without merging them twice with the old session</span><br><span class="line">val sessionWindows = keyedStream</span><br><span class="line">  .window(EventTimeSessionWindows.withGap(Time.minutes(30)))</span><br><span class="line">  .allowedLateness(Time.days(7))</span><br><span class="line">  .trigger(PurgingTrigger.of(EventTimeTrigger.create()))</span><br></pre></td></tr></table></figure>
<h2 id="7-Reduce-VS-Process"><a href="#7-Reduce-VS-Process" class="headerlink" title="7 - Reduce VS Process"></a>7 - Reduce VS Process</h2><p>如上所述，我们对Flink的使用依赖于累积给定键的数据，并将所有这些数据合并在一起。这可以通过两种方式完成：</p>
<p>将数据存储在 ListState 容器中，等待会话结束，并在会话结束时将所有数据合并在一起<br>使用 ReducingState 在每个新事件到达时，将其与之前的事件合并<br>使用第一种还是第二种状态取决于你在WindowedStream上运行的功能：使用ProcessWindowFunction的 process 调用将使用ListState，而使用ReduceFunction的reduce调用则将使用ReducingState。</p>
<p>ReducingState的优点非常明显：不存储窗口处理之前的所有数据，而是在单个记录中不断地对其进行聚合。这通常会导致状态更小，取决于在reduce操作期间会丢弃多少数据。对我们来说，它在存储方面几乎没有改善，因为与我们为历史会话存储的7天数据相比，该状态的大小可以忽略不计。相反，我们注意到通过使用ListState可以提高性能！</p>
<p>原因是：每次新事件到来时，连续的reduce操作都需要对数据进行反序列化和序列化。这可以在 RocksDBReducingState 的add函数中看到，该函数会调用getInternal，从而导致数据反序列化。<br>但是，当使用RocksDB更新ListState中的值，我们可以看到没有序列化发生。这要归功于RocksDB的合并操作能让Flink可以将数据进行追加而无需反序列化。</p>
<p>最后，我们选择了ListState方法，因为性能提升有助于减少延迟，而存储的影响却很小。</p>
<h2 id="8-不要相信输入数据！"><a href="#8-不要相信输入数据！" class="headerlink" title="8 - 不要相信输入数据！"></a>8 - 不要相信输入数据！</h2><p>永远不要假设你的输入会像你期望的那样。可能会出现各种未知的情况，比如你的任务接收到了倾斜的数据、重复的数据、意外的峰值、无效的记录……总是往最坏的方面想，保护你的作业免受这些影响。</p>
<p>让我们快速定义几个关键术语，供后面使用:</p>
<p>“网页浏览（PV）事件”是我们接收到的主要信息。当访问者在客户端加载URL以及userId、sessionNumber和pageNumber等信息时，就会触发它<br>“会话”代表用户在不离开网站的情况下进行的所有互动的总和。它们是由Flink通过汇总PV事件和其他信息计算得出的<br>为了保护我们的任务，我们已尽可能的增加前置过滤。我们必须遵守的规则是，尽可能早地在流中过滤掉无效数据，以避免在中后期造成不必要的昂贵操作。例如，我们有一个规则，对于给定的会话，最多只能发送300个PV事件。每个PV事件都用一个递增的页码标记，以指示其在会话中的位置。当我们在一个会话中接收到超过300个PV事件时，我们可以通过以下方法来过滤它们:</p>
<p>计算一个给定窗口过期时的PV事件的数量<br>丢弃页码超过300的事件<br>第一个方案似乎更可靠，因为它不依赖于页码的值，但是我们要在状态中累积300多个PV事件，然后才能排除它们。最终我们选择了第二个方案，该方案在错误数据进入Flink时就进行了排除。</p>
<p>除了这些无状态过滤器之外，我们还需要根据与每个键相关的指标排除数据。例如，每个会话的最大大小（以字节为单位）设置为4MB。选择此数字是出于业务原因，也是为了帮助解决Flink中RocksDB状态的一个限制。事实上，如果Flink使用的RocksDB API的值超过2 ^ 31字节，那么它就会失败。因此，如果你像上面解释的那样使用一个ListState，则需要确保你永远不要累积太多的数据。</p>
<p>当你只有关于新消费的事件的信息时，就不可能知道会话的当前大小，这意味着我们不能使用与处理页码相同的技巧。我们所做的只是将RocksDB中的每个键(即每个会话)的元数据存储在一个单独的ValueState中。此元数据在keyBy算子之后，但在开窗之前使用和更新。这意味着我们可以保护RocksDB避免在其ListState中积累太多数据，因为基于此元数据，我们知道何时停止接受给定键的值！</p>
<h2 id="9-事件时间的危险性"><a href="#9-事件时间的危险性" class="headerlink" title="9 - 事件时间的危险性"></a>9 - 事件时间的危险性</h2><p>事件时间处理在大多数情况下都很出色，但你必须牢记：如果你处理晚到数据的方法很费时，可能会有一些糟糕的后果。这个问题并不是直接与Flink有关，当某个外部组件往Kafka topic在写数据，而同时Flink正在消费这个topic的数据，如果这个外部组件出现问题，就会发生数据晚到的现象。具体来说，当这个组件消费某些分区的速度比其他组件慢时。</p>
<p>这个组件（称为Asimov）是一个简单的Akka流程序，该程序读取Kafka topic，解析JSON数据，将其转换为protobuf，然后将其推送到另一个Kafka topic，这样Flink就可以处理这个protobuf。Asimov的输入在每个分区中应该是有序的，但是由于分区不是与输出topic一对一映射，因此当Flink最终处理消息时，可能会出现一些乱序。这样也没啥问题，因为Flink能通过延迟水印来支持乱序。</p>
<p>问题是，当Asimov读取一个分区的速度比其他分区慢时：这意味着Flink的水印将随着最快的Asimov输入分区（而不是Flink的输入，因为所有分区都正常前进）前进，而慢的分区将发出具有更旧时间戳的记录。这最终会导致Flink将这些记录视为迟来的记录! 这可能没问题，但是在我们的作业中，我们使用特定的逻辑来处理晚到的记录，需要从RocksDB获取数据并生成额外的消息来执行下游的更新。这意味着，每当Asimov因为某种原因在几个分区上落后时，Flink就需要做更多的工作。</p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20210327125747.png" alt="在有128个分区的topic中，只有8个分区累积延迟，从而导致Flink中的数据晚到"></p>
<p>我们发现了两种解决此问题的方法：</p>
<p>我们可以按照与它的输出topic相同的方式（通过userId）对Asimov的输入topic进行分区。这意味着，当Asimov滞后几个分区，Flink输入中的相应分区也滞后，从而导致水印前进得更慢：</p>
<p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20210327125813.png" alt=""></p>
<p>我们决定不这样做，因为如果我们在 Asimov 之前就有晚到的数据，这个问题仍然会存在，这迫使我们得以相同的方式来给每个topic划分分区。但这在很多情况下是不能做的。</p>
<p>另一个解决方案依赖于攒批处理晚到的事件：如果我们可以推迟对晚到事件的处理，我们可以确保每个会话最多产生一个更新，而不是每个事件产生一个更新。我们可以通过使用自定义触发器，以避免出现晚到事件到达时就触发窗口，从而实现第二种解决方案。正如你在默认的EventTimeTrigger实现中所看到的，晚到事件在特定情况下不会注册计时器。在我们的方案中，无论如何我们都会注册一个计时器，并且不会立即触发窗口。因为我们的业务需求允许以这种方式进行批量更新，所以我们可以确保当上游出现延迟时，我们不会生成数百个昂贵的更新。</p>
<h2 id="10-避免将所有内容存储在Flink中"><a href="#10-避免将所有内容存储在Flink中" class="headerlink" title="10 - 避免将所有内容存储在Flink中"></a>10 - 避免将所有内容存储在Flink中</h2><p>让我们以一些普遍的观点来结束我们的讨论：如果你的数据很大，并且不需要经常访问，那么最好将其存储在Flink之外。在设计作业时，你希望所有需要的数据都直接在Flink节点上（在RocksDB或内存中）可用。当然，这使得使用这种数据的方式更快，但当数据很大时，它会给你的作业增加很多成本。这是因为Flink的状态没有被复制，所以丢失一个节点需要从检查点完全恢复。如果你经常需要向检查点存储写入数百GB数据，则检查点机制本身也很昂贵。</p>
<p>如果对状态的访问是性能需求中的关键部分，那么将它存储在Flink中绝对是值得的。但是，如果你可以忍受额外的延迟，那么将它存储在具有复制功能和支持对给定记录快速访问的外部数据库中，这将为你节省很多麻烦。对于我们的用例，我们选择将WindowContent状态保留在RocksDB中，但我们将HistoricalSessions数据移入了Aerospike中。由于状态较小，这使得我们的Flink作业更快，更容易维护。我们甚至还受益于这样一个事实：存储在Flink中的剩余数据足够小，可以都放入内存，这让我们无需使用RocksDB和本地SSD。</p>
<h2 id="结尾"><a href="#结尾" class="headerlink" title="结尾"></a>结尾</h2><p>总而言之，使用Flink是一次很棒的经历：尽管有时我们无法理解框架的行为，但最终它总是有道理的。 我强烈推荐订阅Flink用户邮件列表，从这个非常有用和友好的社区获得额外的提示!</p>
<p>原文链接：<a href="https://engineering.contentsquare.com/2021/ten-flink-gotchas/" target="_blank" rel="noopener">https://engineering.contentsquare.com/2021/ten-flink-gotchas/</a></p>
</div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>kaibo</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2021/03/27/ten-flink-gotchas/">https://zhoukaibo.com/2021/03/27/ten-flink-gotchas/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>转载请保留作者及链接</li></ul></div><br><div class="tags"><a href="/tags/flink-rocksdb-streaming-windown/">flink,rocksdb,streaming,windown</a></div><div class="post-nav"><a class="next" href="/2020/02/18/Flink-1-10-Native-Kubernetes/">Flink 1.10 Native Kubernetes 原理与实践</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'https://zhoukaibo.com/2021/03/27/ten-flink-gotchas/';
    this.page.identifier = '2021/03/27/ten-flink-gotchas/';
    this.page.title = '使用 Flink 前需要知道的10个『陷阱』';
  };</script><script type="text/javascript" id="disqus-lazy-load-script">$.ajax({
url: 'https://disqus.com/next/config.json',
timeout: 2500,
type: 'GET',
success: function(){
  var d = document;
  var s = d.createElement('script');
  s.src = '//flying5.disqus.com/embed.js';
  s.setAttribute('data-timestamp', + new Date());
  (d.head || d.body).appendChild(s);
  $('.disqus_click_btn').css('display', 'none');
},
error: function() {
  $('.disqus_click_btn').css('display', 'block');
}
});</script><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//flying5.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://flying5.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });
</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"><input type="hidden" name="sitesearch" value="https://zhoukaibo.com"></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/java/" style="font-size: 15px;">java</a> <a href="/tags/spi/" style="font-size: 15px;">spi</a> <a href="/tags/flink/" style="font-size: 15px;">flink</a> <a href="/tags/kubernetes/" style="font-size: 15px;">kubernetes</a> <a href="/tags/exactly-once/" style="font-size: 15px;">exactly-once</a> <a href="/tags/at-least-once/" style="font-size: 15px;">at-least-once</a> <a href="/tags/kafka/" style="font-size: 15px;">kafka</a> <a href="/tags/yarn/" style="font-size: 15px;">yarn</a> <a href="/tags/k8s/" style="font-size: 15px;">k8s</a> <a href="/tags/flink-rocksdb-streaming-windown/" style="font-size: 15px;">flink,rocksdb,streaming,windown</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2021/03/27/ten-flink-gotchas/">使用 Flink 前需要知道的10个『陷阱』</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/18/Flink-1-10-Native-Kubernetes/">Flink 1.10 Native Kubernetes 原理与实践</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/15/flink-yarn-k8s-principle-practice/">Flink on Yarn / Kubernetes 原理剖析及实践</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/15/apache-flink-intro-client/">Apache Flink 客户端操作</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/16/java-spi/">Java SPI 机制分析及其优缺点</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/02/14/exactly-once-exactly-same/">谈谈流计算中的『Exactly Once』特性</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/10/flink-kafka-exactly-once/">Apache Flink结合Kafka构建端到端的Exactly-Once处理</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/10/hello-world/">Hello World</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-comment-o"> 最近评论</i></div><script type="text/javascript" src="//flying5.disqus.com/recent_comments_widget.js?num_items=5&amp;hide_avatars=1&amp;avatar_size=32&amp;excerpt_length=20&amp;hide_mods=1"></script></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/." rel="nofollow">Kaibo's Blog</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.1" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.1" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.1"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.1"></script></div></body></html>