<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Kaibo&#39;s Blog</title>
    <link>https://zhoukaibo.com/</link>
    
    <atom:link href="https://zhoukaibo.com/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description></description>
    <pubDate>Sat, 27 Mar 2021 05:10:23 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>使用 Flink 前需要知道的10个『陷阱』</title>
      <link>https://zhoukaibo.com/2021/03/27/ten-flink-gotchas/</link>
      <guid>https://zhoukaibo.com/2021/03/27/ten-flink-gotchas/</guid>
      <pubDate>Sat, 27 Mar 2021 04:39:03 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;Contentsquare 公司的 Robin 总结了他们将 Spark 任务迁移到 Flink 遇到的 10 个『陷阱』。对于第一次将 Flink 用于生产环境的用户来说，这些经验非常有参考意义。&lt;/p&gt;
&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;</description>
        
      
      
      
      <content:encoded><![CDATA[<p>Contentsquare 公司的 Robin 总结了他们将 Spark 任务迁移到 Flink 遇到的 10 个『陷阱』。对于第一次将 Flink 用于生产环境的用户来说，这些经验非常有参考意义。</p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>采用新的框架总是会带来很多惊喜。当你花了几天时间去排查为什么服务运行异常，结果发现只是因为某个功能的用法不对或者缺少一些简单的配置。</p><p>在 Contentsquare，我们需要不断升级数据处理任务，以满足越来越多的数据上的苛刻需求。这也是为什么我们决定将用于会话处理的小时级 Spark 任务迁移到 Flink 流服务。这样我们就可以利用 Flink 更为健壮的处理能力，提供更实时的数据给用户，并能提供历史数据。不过这并不轻松，我们的团队在上面工作了有一年时间。同时，我们也遇到了一些令人惊讶的问题，本文将尝试帮助你避免这些陷阱。</p><h2 id="1-并行度设置导致的负载倾斜"><a href="#1-并行度设置导致的负载倾斜" class="headerlink" title="1 - 并行度设置导致的负载倾斜"></a>1 - 并行度设置导致的负载倾斜</h2><p>我们从一个简单的问题开始：在 Flink UI 中调查某个作业的子任务时，关于每个子任务处理的数据量，你可能会遇到如下这种奇怪的情况。</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20210327125449.png" alt="每个子任务的工作负载并不均衡"></p><p>这表明每个子任务的算子没有收到相同数量的 Key Groups，它代表所有可能的 key 的一部分。如果一个算子收到了 1 个 Key Group，而另外一个算子收到了 2 个，则第二个子任务很可能需要完成两倍的工作。查看 Flink 的代码，我们可以找到以下函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">public static int computeOperatorIndexForKeyGroup(</span><br><span class="line">        int maxParallelism, int parallelism, int keyGroupId) &#123;</span><br><span class="line">    return keyGroupId * parallelism / maxParallelism;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其目的是将所有 Key Groups 分发给实际的算子。Key Groups 的总数由 maxParallelism 参数决定，而算子的数量和 parallelism 相同。这里最大的问题是 maxParallelism 的默认值，它默认等于 operatorParallelism + (operatorParallelism / 2) 。假如我们设置 parallelism 为10，那么 maxParallelism 为 15 （实际最大并发度值的下限是 128 ，上限是 32768，这里只是为了方便举例）。这样，根据上面的函数，我们可以计算出哪些算子会分配给哪些 Key Group。</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20210327125628.png" alt="在默认配置下，部分算子分配了两个Key Group，部分算子只分配了1个"></p><p>解决这个问题非常容易：设置并发度的时候，还要为 maxParallelism 设置一个值，且该值为 parallelism 的倍数。这将让负载更加均衡，同时方便以后扩展。</p><h2 id="2-注意-mapWithState-amp-TTL-的重要性"><a href="#2-注意-mapWithState-amp-TTL-的重要性" class="headerlink" title="2 - 注意 mapWithState &amp; TTL 的重要性"></a>2 - 注意 mapWithState &amp; TTL 的重要性</h2><p>在处理包含无限多键的数据时，要考虑到 keyed 状态保留策略（通过 TTL 定时器来在给定的时间之后清理未使用的数据）是很重要的。术语『无限』在这里有点误导，因为如果你要处理的 key 以 128 位编码，则 key 的最大数量将会有个限制（等于2的128次方）。但这是一个巨大的数字！你可能无法在状态中存储那么多值，所以最好考虑你的键空间是无界的，同时新键会随着时间不断出现。</p><p>如果你的 keyed 状态包含在某个Flink的默认窗口中，则将是安全的：即使未使用TTL，在处理窗口的元素时也会注册一个清除计时器，该计时器将调用 clearAllState 函数，并删除与该窗口关联的状态及其元数据。</p><p>如果要使用 Keyed State Descriptor 来管理状态，可以很方便地添加TTL配置，以确保在状态中的键数量不会无限制地增加。</p><p>但是，你可能会想使用更简便的 mapWithState 方法，该方法可让你访问 valueState 并隐藏操作的复杂性。虽然这对于测试和少量键的数据来说是很好的选择，但如果在生产环境中遇到无限多键值时，会引发问题。由于状态是对你隐藏的，因此你无法设置TTL，并且默认情况下未配置任何TTL。这就是为什么值得考虑做一些额外工作的原因，如声明诸如RichMapFunction之类的东西，这将使你能更好的控制状态的生命周期。</p><h2 id="3-从检查点还原和重新分区"><a href="#3-从检查点还原和重新分区" class="headerlink" title="3 - 从检查点还原和重新分区"></a>3 - 从检查点还原和重新分区</h2><p>在使用大状态时，有必要使用增量检查点（incremental checkpointing）。在我们的案例中，任务的完整状态约为8TB，我们将检查点配置为每15分钟做一次。由于检查点是增量式的，因此我们只能设法每15分钟将大约100GB的数据发送到对象存储，这是一种更快的方式并且网络占用较少。这对于容错效果很好，但是在更新任务时我们也需要检索状态。常用的方法是为正在运行的作业创建一个保存点（savepoint），以可移植的格式包含整个状态。</p><p>但是，在我们的情况下，保存点可能需要几个小时才能完成，这使得每次发布版本都是一个漫长而麻烦的过程。相反，我们决定使用保留检查点（Retained Checkpoints）。设置此参数后，我们可以通过从上一个作业的检查点恢复状态来加快发布速度，而不必触发冗长的保存点！</p><p>此外，尽管保存点比检查点具有更高的可移植性，但您仍然可以使用保留的检查点来更改作业的分区（它可能不适用于所有类型的作业，所以最好对其进行测试）。这与从保存点重新分区完全一样，但是不需要经历Flink在TaskManager之间重新分配数据的漫长过程。当我们尝试这样做时，大约花了8个小时才完成，这是不可持续的。幸运的是，由于我们使用的是RocksDB状态后端，因此我们可以在这步中增加更多线程以提高其速度。这是通过将以下两个参数从1增加到8来完成的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">state.backend.rocksdb.checkpoint.transfer.thread.num: 8</span><br><span class="line">state.backend.rocksdb.thread.num: 8</span><br></pre></td></tr></table></figure><p>使用保留的检查点，并增加分配给RocksDB传输的线程数，能将发布和重新分区时间减少10倍！</p><h2 id="4-提前增加日志记录"><a href="#4-提前增加日志记录" class="headerlink" title="4 - 提前增加日志记录"></a>4 - 提前增加日志记录</h2><p>这一点可能看起来很明显，但也很容易忘记。开发作业时，请记住它将运行很长时间，并且可能会处理意外的数据。发生这种情况时，你将需要尽可能多的信息来调查发生了什么，而不必通过再次回溯相同的数据来重现问题。</p><p>我们的任务是将事件汇总在一起，并根据特定规则进行合并。这些规则中的某些规则在大多数情况下性能还可以，但是当有数据倾斜时却要花费很长时间。当我们发现任务卡住了3个小时，却不知道它在做什么。似乎只有一个TaskManager的CPU可以正常工作，因此我们怀疑是特定数据导致我们的算法性能下降。</p><p>最终处理完数据后，一切恢复正常，但是我们不知道从哪开始检查！这就是为什么我们为这些情况添加了一些预防性日志的原因：在处理窗口时，我们会测量花费的时间。只要计算窗口所需的时间超过1分钟，我们就会记录下所有可能的数据。这对于准确了解导致性能下降的倾斜是非常有帮助的，并且当再次发生这种情况时，我们能够定位到合并过程处理慢的部分原因。假如收到的是重复的数据，则可能确实需要几个小时。当然，重要的是不要过多地记录信息，因为这会降低性能。因此，请尝试找到仅在异常情况下才显示信息的阈值。</p><h2 id="5-如何找出卡住的作业实际上在做什么"><a href="#5-如何找出卡住的作业实际上在做什么" class="headerlink" title="5 - 如何找出卡住的作业实际上在做什么"></a>5 - 如何找出卡住的作业实际上在做什么</h2><p>对上述问题的调查也使我们意识到，我们需要找到一种简单的方法，来定位作业疑似卡住时当前正在运行的代码段。幸运的是，有一个简单的方法可以做到这一点！首先，您将需要配置TaskManagers的JMX以接受远程监视。在Kubernetes部署中，我们可以通过三个步骤连接到JMX：</p><p>首先，将此属性添加到我们的flink-conf.yaml中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.java.opts: &quot;-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.port=1099 -Dcom.sun.management.jmxremote.rmi.port=1099 -Djava.rmi.server.hostname=127.0.0.1&quot;​</span><br></pre></td></tr></table></figure><p>然后，将本地端口1099转发到TaskManager的pod中的端口<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl port-forward flink-taskmanager-4 1099</span><br></pre></td></tr></table></figure></p><p>最后，打开 jconsole<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ jconsole 127.0.0.1:1099​</span><br></pre></td></tr></table></figure></p><p>这使您可以轻松地在JVM上查看目标TaskManager的信息。对于卡住的作业，我们以正在运行的唯一一个TaskManager为目标，分析了正在运行的线程：</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20210327125702.png" alt="JConsole向我们展示了每个线程当前正在做什么"></p><p>深入研究，我们可以看到所有线程都在等待，除了其中一个（在上面的屏幕截图中已突出显示）。这使得我们能够快速发现作业是卡在哪个方法调用里面的，并轻松修复！</p><h2 id="6-将数据从一种状态迁移到另一种状态的风险"><a href="#6-将数据从一种状态迁移到另一种状态的风险" class="headerlink" title="6 - 将数据从一种状态迁移到另一种状态的风险"></a>6 - 将数据从一种状态迁移到另一种状态的风险</h2><p>根据你的实际情况，可能需要保留两个具有不同语义的不同状态描述符。例如，我们通过WindowContent状态为进行中的会话累积事件，接着将处理后的会话移动到称为 HistoricalSessions 的 ValueState 中。第二个状态为了防止后面需要用到会保留几天，直到TTL过期丢弃它为止。</p><p>我们做的第一个测试运行良好：我们可以发送额外的数据到已处理的会话，这将为相同的键创建一个新窗口。在窗口的处理过程中，我们会从 HistoricalSessions 状态中获取数据，以将新数据与旧会话合并，并且结果会话是历史会话的增强版本，这也正是我们所期望的。</p><p>在执行此操作时，我们遇到过几次内存问题。经过几次测试后，我们了解到OOM仅在将旧数据发送到Flink时才发生（即，发送数据的时间戳早于其当前水印）。这使得我们发现了当前处理方式中的一个大问题：当接收到旧数据时，Flink将其与旧窗口合并，而旧窗口的数据仍在WindowContent状态内（这可以通过设置AllowedLateness实现）。然后结果窗口会与HistoricalSessions内容合并，该内容还包含旧的数据。最终我们得到的是重复的事件，在同一会话中收到一些事件后，每个事件都将有数千条重复，从而导致了OOM。</p><p>这个问题的解法非常简单：我们希望WindowContent在将其内容移至第二个状态之前自动清除。我们使用了Flink的PurgingTrigger来达到这个目的，当窗口触发时，该消息会发送一条清除状态内容的消息。具体代码如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// Purging the window&apos;s content allows us to receive late events without merging them twice with the old session</span><br><span class="line">val sessionWindows = keyedStream</span><br><span class="line">  .window(EventTimeSessionWindows.withGap(Time.minutes(30)))</span><br><span class="line">  .allowedLateness(Time.days(7))</span><br><span class="line">  .trigger(PurgingTrigger.of(EventTimeTrigger.create()))</span><br></pre></td></tr></table></figure><h2 id="7-Reduce-VS-Process"><a href="#7-Reduce-VS-Process" class="headerlink" title="7 - Reduce VS Process"></a>7 - Reduce VS Process</h2><p>如上所述，我们对Flink的使用依赖于累积给定键的数据，并将所有这些数据合并在一起。这可以通过两种方式完成：</p><p>将数据存储在 ListState 容器中，等待会话结束，并在会话结束时将所有数据合并在一起<br>使用 ReducingState 在每个新事件到达时，将其与之前的事件合并<br>使用第一种还是第二种状态取决于你在WindowedStream上运行的功能：使用ProcessWindowFunction的 process 调用将使用ListState，而使用ReduceFunction的reduce调用则将使用ReducingState。</p><p>ReducingState的优点非常明显：不存储窗口处理之前的所有数据，而是在单个记录中不断地对其进行聚合。这通常会导致状态更小，取决于在reduce操作期间会丢弃多少数据。对我们来说，它在存储方面几乎没有改善，因为与我们为历史会话存储的7天数据相比，该状态的大小可以忽略不计。相反，我们注意到通过使用ListState可以提高性能！</p><p>原因是：每次新事件到来时，连续的reduce操作都需要对数据进行反序列化和序列化。这可以在 RocksDBReducingState 的add函数中看到，该函数会调用getInternal，从而导致数据反序列化。<br>但是，当使用RocksDB更新ListState中的值，我们可以看到没有序列化发生。这要归功于RocksDB的合并操作能让Flink可以将数据进行追加而无需反序列化。</p><p>最后，我们选择了ListState方法，因为性能提升有助于减少延迟，而存储的影响却很小。</p><h2 id="8-不要相信输入数据！"><a href="#8-不要相信输入数据！" class="headerlink" title="8 - 不要相信输入数据！"></a>8 - 不要相信输入数据！</h2><p>永远不要假设你的输入会像你期望的那样。可能会出现各种未知的情况，比如你的任务接收到了倾斜的数据、重复的数据、意外的峰值、无效的记录……总是往最坏的方面想，保护你的作业免受这些影响。</p><p>让我们快速定义几个关键术语，供后面使用:</p><p>“网页浏览（PV）事件”是我们接收到的主要信息。当访问者在客户端加载URL以及userId、sessionNumber和pageNumber等信息时，就会触发它<br>“会话”代表用户在不离开网站的情况下进行的所有互动的总和。它们是由Flink通过汇总PV事件和其他信息计算得出的<br>为了保护我们的任务，我们已尽可能的增加前置过滤。我们必须遵守的规则是，尽可能早地在流中过滤掉无效数据，以避免在中后期造成不必要的昂贵操作。例如，我们有一个规则，对于给定的会话，最多只能发送300个PV事件。每个PV事件都用一个递增的页码标记，以指示其在会话中的位置。当我们在一个会话中接收到超过300个PV事件时，我们可以通过以下方法来过滤它们:</p><p>计算一个给定窗口过期时的PV事件的数量<br>丢弃页码超过300的事件<br>第一个方案似乎更可靠，因为它不依赖于页码的值，但是我们要在状态中累积300多个PV事件，然后才能排除它们。最终我们选择了第二个方案，该方案在错误数据进入Flink时就进行了排除。</p><p>除了这些无状态过滤器之外，我们还需要根据与每个键相关的指标排除数据。例如，每个会话的最大大小（以字节为单位）设置为4MB。选择此数字是出于业务原因，也是为了帮助解决Flink中RocksDB状态的一个限制。事实上，如果Flink使用的RocksDB API的值超过2 ^ 31字节，那么它就会失败。因此，如果你像上面解释的那样使用一个ListState，则需要确保你永远不要累积太多的数据。</p><p>当你只有关于新消费的事件的信息时，就不可能知道会话的当前大小，这意味着我们不能使用与处理页码相同的技巧。我们所做的只是将RocksDB中的每个键(即每个会话)的元数据存储在一个单独的ValueState中。此元数据在keyBy算子之后，但在开窗之前使用和更新。这意味着我们可以保护RocksDB避免在其ListState中积累太多数据，因为基于此元数据，我们知道何时停止接受给定键的值！</p><h2 id="9-事件时间的危险性"><a href="#9-事件时间的危险性" class="headerlink" title="9 - 事件时间的危险性"></a>9 - 事件时间的危险性</h2><p>事件时间处理在大多数情况下都很出色，但你必须牢记：如果你处理晚到数据的方法很费时，可能会有一些糟糕的后果。这个问题并不是直接与Flink有关，当某个外部组件往Kafka topic在写数据，而同时Flink正在消费这个topic的数据，如果这个外部组件出现问题，就会发生数据晚到的现象。具体来说，当这个组件消费某些分区的速度比其他组件慢时。</p><p>这个组件（称为Asimov）是一个简单的Akka流程序，该程序读取Kafka topic，解析JSON数据，将其转换为protobuf，然后将其推送到另一个Kafka topic，这样Flink就可以处理这个protobuf。Asimov的输入在每个分区中应该是有序的，但是由于分区不是与输出topic一对一映射，因此当Flink最终处理消息时，可能会出现一些乱序。这样也没啥问题，因为Flink能通过延迟水印来支持乱序。</p><p>问题是，当Asimov读取一个分区的速度比其他分区慢时：这意味着Flink的水印将随着最快的Asimov输入分区（而不是Flink的输入，因为所有分区都正常前进）前进，而慢的分区将发出具有更旧时间戳的记录。这最终会导致Flink将这些记录视为迟来的记录! 这可能没问题，但是在我们的作业中，我们使用特定的逻辑来处理晚到的记录，需要从RocksDB获取数据并生成额外的消息来执行下游的更新。这意味着，每当Asimov因为某种原因在几个分区上落后时，Flink就需要做更多的工作。</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20210327125747.png" alt="在有128个分区的topic中，只有8个分区累积延迟，从而导致Flink中的数据晚到"></p><p>我们发现了两种解决此问题的方法：</p><p>我们可以按照与它的输出topic相同的方式（通过userId）对Asimov的输入topic进行分区。这意味着，当Asimov滞后几个分区，Flink输入中的相应分区也滞后，从而导致水印前进得更慢：</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20210327125813.png" alt=""></p><p>我们决定不这样做，因为如果我们在 Asimov 之前就有晚到的数据，这个问题仍然会存在，这迫使我们得以相同的方式来给每个topic划分分区。但这在很多情况下是不能做的。</p><p>另一个解决方案依赖于攒批处理晚到的事件：如果我们可以推迟对晚到事件的处理，我们可以确保每个会话最多产生一个更新，而不是每个事件产生一个更新。我们可以通过使用自定义触发器，以避免出现晚到事件到达时就触发窗口，从而实现第二种解决方案。正如你在默认的EventTimeTrigger实现中所看到的，晚到事件在特定情况下不会注册计时器。在我们的方案中，无论如何我们都会注册一个计时器，并且不会立即触发窗口。因为我们的业务需求允许以这种方式进行批量更新，所以我们可以确保当上游出现延迟时，我们不会生成数百个昂贵的更新。</p><h2 id="10-避免将所有内容存储在Flink中"><a href="#10-避免将所有内容存储在Flink中" class="headerlink" title="10 - 避免将所有内容存储在Flink中"></a>10 - 避免将所有内容存储在Flink中</h2><p>让我们以一些普遍的观点来结束我们的讨论：如果你的数据很大，并且不需要经常访问，那么最好将其存储在Flink之外。在设计作业时，你希望所有需要的数据都直接在Flink节点上（在RocksDB或内存中）可用。当然，这使得使用这种数据的方式更快，但当数据很大时，它会给你的作业增加很多成本。这是因为Flink的状态没有被复制，所以丢失一个节点需要从检查点完全恢复。如果你经常需要向检查点存储写入数百GB数据，则检查点机制本身也很昂贵。</p><p>如果对状态的访问是性能需求中的关键部分，那么将它存储在Flink中绝对是值得的。但是，如果你可以忍受额外的延迟，那么将它存储在具有复制功能和支持对给定记录快速访问的外部数据库中，这将为你节省很多麻烦。对于我们的用例，我们选择将WindowContent状态保留在RocksDB中，但我们将HistoricalSessions数据移入了Aerospike中。由于状态较小，这使得我们的Flink作业更快，更容易维护。我们甚至还受益于这样一个事实：存储在Flink中的剩余数据足够小，可以都放入内存，这让我们无需使用RocksDB和本地SSD。</p><h2 id="结尾"><a href="#结尾" class="headerlink" title="结尾"></a>结尾</h2><p>总而言之，使用Flink是一次很棒的经历：尽管有时我们无法理解框架的行为，但最终它总是有道理的。 我强烈推荐订阅Flink用户邮件列表，从这个非常有用和友好的社区获得额外的提示!</p><p>原文链接：<a href="https://engineering.contentsquare.com/2021/ten-flink-gotchas/" target="_blank" rel="noopener">https://engineering.contentsquare.com/2021/ten-flink-gotchas/</a></p>]]></content:encoded>
      
      
      
      <category domain="https://zhoukaibo.com/tags/flink-rocksdb-streaming-windown/">flink,rocksdb,streaming,windown</category>
      
      
      <comments>https://zhoukaibo.com/2021/03/27/ten-flink-gotchas/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Flink 1.10 Native Kubernetes 原理与实践</title>
      <link>https://zhoukaibo.com/2020/02/18/Flink-1-10-Native-Kubernetes/</link>
      <guid>https://zhoukaibo.com/2020/02/18/Flink-1-10-Native-Kubernetes/</guid>
      <pubDate>Tue, 18 Feb 2020 15:01:46 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;千呼万唤始出来，在 Kubernetes 如火如荼的今天，Flink 社区终于在 1.10 版本提供了对 Kubernetes 的原生支持，也就是 &lt;a href=&quot;https://flink.apache.org/news/2020/02/11/release-1.10.</description>
        
      
      
      
      <content:encoded><![CDATA[<p>千呼万唤始出来，在 Kubernetes 如火如荼的今天，Flink 社区终于在 1.10 版本提供了对 Kubernetes 的原生支持，也就是 <a href="https://flink.apache.org/news/2020/02/11/release-1.10.0.html#native-kubernetes-integration-beta" target="_blank" rel="noopener">Native Kubernetes Integration</a>。不过还只是Beta版本，预计会在 1.11 版本里面提供完整的支持。</p><p>我们知道，在 Flink 1.9 以及之前的版本里面，如果要在 Kubernetes 上运行 Flink 任务是需要事先指定好需要的 TaskManager(TM) 的个数以及CPU和内存的。这样的问题是：大多数情况下，你在任务启动前根本无法精确的预估这个任务需要多少个TM。如果指定的TM多了，会导致资源浪费；如果指定的TM个数少了，会导致任务调度不起来。本质原因是在 Kubernetes 上运行的 Flink 任务并没有直接向 Kubernetes 集群去申请资源。</p><p>Flink 在 1.10 版本完成了<code>Active Kubernetes Integration</code>的第一阶段，支持了 session clusters。后续的第二阶段会提供更完整的支持，如支持 per-job 任务提交，以及基于原生 Kubernetes API 的高可用，支持更多的 Kubernetes 参数如 toleration, label 和 node selector 等。<code>Active Kubernetes Integration</code>中的<code>Active</code>意味着 <a href="https://zhoukaibo.com/tags/flink/">Flink</a> 的 ResourceManager (KubernetesResourceManager) 可以直接和 Kubernetes 通信，按需申请新的 Pod，类似于 Flink 中对 Yarn 和 Mesos 的集成所做的那样。在多租户环境中，用户可以利用 <a href="https://zhoukaibo.com/tags/kubernetes/">Kubernetes</a> 里面的 namespace 做资源隔离启动不同的 Flink 集群。当然，Kubernetes 中的用户帐号和赋权是需要提前准备好的。 </p><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20200218232746.png" alt="Flink 1.10 native k8s"></p><p>工作原理如下（段首的序号对应图中箭头所示的数字）：</p><ol><li><p>Flink 客户端首先连接 Kubernetes API Server，提交 Flink 集群的资源描述文件，包括 configmap，job manager service，job manager deployment 和 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/" target="_blank" rel="noopener">Owner Reference</a>。</p></li><li><p>Kubernetes Master 就会根据这些资源描述文件去创建对应的 Kubernetes 实体。以我们最关心的 job manager deployment 为例，Kubernetes 集群中的某个节点收到请求后，Kubelet 进程会从中央仓库下载 Flink 镜像，准备和挂载 volume，然后执行启动命令。在 flink master 的 pod 启动后，Dispacher 和 KubernetesResourceManager 也都启动了。</p><p>前面两步完成后，整个 Flink session cluster 就启动好了，可以接受提交任务请求了。</p></li><li><p>用户可以通过 flink 命令行即 flink client 往这个 session cluster 提交任务。此时 job graph 会在 flink client 端生成，然后和用户 jar 包一起通过 RestClinet 上传。</p></li><li><p>一旦 job 提交成功，JobSubmitHandler 收到请求就会提交 job 给 Dispatcher。接着就会生成一个 job master。</p></li><li><p>JobMaster 向 KubernetesResourceManager 请求 slots。</p></li><li><p>KubernetesResourceManager 从 Kubernetes 集群分配 TaskManager。每个TaskManager都是具有唯一标识的 Pod。KubernetesResourceManager 会为 TaskManager 生成一份新的配置文件，里面有 Flink Master 的 service name 作为地址。这样在 Flink Master failover之后，TaskManager 仍然可以重新连上。</p></li><li><p>Kubernetes 集群分配一个新的 Pod 后，在上面启动 TaskManager。</p></li><li><p>TaskManager 启动后注册到 SlotManager。</p></li><li><p>SlotManager 向 TaskManager 请求 slots。</p></li><li><p>TaskManager 提供 slots 给 JobMaster。然后任务就会被分配到这个 slots 上运行。</p></li></ol><h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><p>Flink 的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/deployment/native_kubernetes.html" target="_blank" rel="noopener">文档</a>上对如何使用已经写的比较详细了，不过刚开始总会踩到一些坑。如果对 Kubernetes 不熟，可能会花点时间。</p><p>(1) 首先得有个 Kubernetes 集群，会有个 <code>~/.kube/config</code> 文件。尝试执行 kubectl get nodes 看下集群是否正常。</p><p>如果没有这个 <code>~/.kube/config</code> 文件，会报错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">2020-02-17 22:27:17,253 WARN  io.fabric8.kubernetes.client.Config                           - Error reading service account token from: [/var/run/secrets/kubernetes.io/serviceaccount/token]. Ignoring.</span><br><span class="line">2020-02-17 22:27:17,437 ERROR org.apache.flink.kubernetes.cli.KubernetesSessionCli          - Error while running the Flink session.</span><br><span class="line">io.fabric8.kubernetes.client.KubernetesClientException: Operation: [get]  for kind: [Service]  with name: [flink-cluster-81832d75-662e-40fd-8564-cd5a902b243c]  in namespace: [default]  failed.</span><br><span class="line">at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:64)</span><br><span class="line">at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:72)</span><br><span class="line">at io.fabric8.kubernetes.client.dsl.base.BaseOperation.getMandatory(BaseOperation.java:231)</span><br><span class="line">at io.fabric8.kubernetes.client.dsl.base.BaseOperation.get(BaseOperation.java:164)</span><br><span class="line">at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.getService(Fabric8FlinkKubeClient.java:334)</span><br><span class="line">at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.getInternalService(Fabric8FlinkKubeClient.java:246)</span><br><span class="line">at org.apache.flink.kubernetes.cli.KubernetesSessionCli.run(KubernetesSessionCli.java:104)</span><br><span class="line">at org.apache.flink.kubernetes.cli.KubernetesSessionCli.lambda$main$0(KubernetesSessionCli.java:185)</span><br><span class="line">at org.apache.flink.runtime.security.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)</span><br><span class="line">at org.apache.flink.kubernetes.cli.KubernetesSessionCli.main(KubernetesSessionCli.java:185)</span><br><span class="line">Caused by: java.net.UnknownHostException: kubernetes.default.svc: nodename nor servname provided, or not known</span><br></pre></td></tr></table></figure><p>(2) 提前创建好用户和赋权(<a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/" target="_blank" rel="noopener">RBAC</a>)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl create serviceaccount flink</span><br><span class="line">kubectl create clusterrolebinding flink-role-binding-flink --clusterrole=edit --serviceaccount=default:flink</span><br></pre></td></tr></table></figure><p>如果没有创建用户，使用默认的用户去提交，会报错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Caused by: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: GET at: https://10.10.0.1/api/v1/namespaces/default/pods?labelSelector=app%3Dkaibo-test%2Ccomponent%3Dtaskmanager%2Ctype%3Dflink-native-kubernetes. </span><br><span class="line"></span><br><span class="line">Message: Forbidden!Configured service account doesn&apos;t have access. </span><br><span class="line">Service account may have been revoked. pods is forbidden: </span><br><span class="line">User &quot;system:serviceaccount:default:default&quot; cannot list resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;default&quot;.</span><br></pre></td></tr></table></figure><p>(3) 这一步是可选的。默认情况下, JobManager 和 TaskManager 只会将 log 写到各自 pod 的 /opt/flink/log 。如果想通过 kubectl logs <podname> 看到日志，需要将 log 输出到控制台。要做如下修改 FLINK_HOME/conf 目录下的 log4j.properties 文件。</podname></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootLogger=INFO, file, console</span><br><span class="line"></span><br><span class="line"># Log all infos to the console</span><br><span class="line">log4j.appender.console=org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.console.layout=org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.console.layout.ConversionPattern=%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; %-5p %-60c %x - %m%n</span><br></pre></td></tr></table></figure><p>然后启动 session cluster 的命令行需要带上参数:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-Dkubernetes.container-start-command-template=&quot;%java% %classpath% %jvmmem% %jvmopts% %logging% %class% %args%&quot;</span><br></pre></td></tr></table></figure><p>(4) 终于可以开始启动 session cluster了。如下命令是启动一个每个 TaskManager  是4G内存，2个CPU，4个slot 的 session cluster。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kubernetes-session.sh -Dkubernetes.container-start-command-template=&quot;%java% %classpath% %jvmmem% %jvmopts% %logging% %class% %args%&quot; -Dkubernetes.cluster-id=kaibo-test -Dtaskmanager.memory.process.size=4096m -Dkubernetes.taskmanager.cpu=2 -Dtaskmanager.numberOfTaskSlots=4</span><br></pre></td></tr></table></figure><p>更多的参数详见文档：<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/config.html#kubernetes" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/config.html#kubernetes</a></p><p>使用 <code>kubectl logs kaibo-test-6f7dffcbcf-c2p7g -f</code> 就能看到日志了。</p><p>如果出现大量的如下这种日志（目前遇到是云厂商的LoadBalance liveness探测导致）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">2020-02-17 14:58:56,323 WARN  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint    - Unhandled exception</span><br><span class="line">java.io.IOException: Connection reset by peer</span><br><span class="line">at sun.nio.ch.FileDispatcherImpl.read0(Native Method)</span><br><span class="line">at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)</span><br><span class="line">at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)</span><br><span class="line">at sun.nio.ch.IOUtil.read(IOUtil.java:192)</span><br><span class="line">at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:377)</span><br><span class="line">at org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:247)</span><br></pre></td></tr></table></figure><p>可以暂时在 log4j.properties 里面配置上：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">log4j.logger.org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint=ERROR, file</span><br></pre></td></tr></table></figure></p><p>这个日志太多会导致 WebUI 上打开 jobmanger log 是空白，因为文件太大了前端无法显示。</p><p>如果前面第(1)和第(2)步没有做，会出现各种异常，通过 kubectl logs 就能很方便的看到日志了。</p><p>Session cluster 启动后可以通过 kubectl get pods,svc 来看是否正常。</p><p>通过端口转发来查看 Web UI：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl port-forward service/kaibo-test 8081</span><br></pre></td></tr></table></figure><p>打开 <a href="http://127.0.0.1:8001" target="_blank" rel="noopener">http://127.0.0.1:8001</a> 就能看到 Flink 的 WebUI 了。</p><p>(5) 提交任务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/flink run -d -e kubernetes-session -Dkubernetes.cluster-id=kaibo-test examples/streaming/TopSpeedWindowing.jar</span><br></pre></td></tr></table></figure><p>我们从 Flink WebUI 页面上可以看到，刚开始启动时，UI上显示 Total/Available Task Slots 为0, Task Managers 也是0。随着任务的提交，资源会动态增加。任务停止后，资源会释放掉。</p><p>在提交任务后，通过 kubectl get pods 能够看到 Flink 为 TaskManager 分配了新的 Pod。</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20200219003114.png" alt="pods"></p><p>(6) 停止 session cluster</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &apos;stop&apos; | ./bin/kubernetes-session.sh -Dkubernetes.cluster-id=kaibo-test -Dexecution.attached=true</span><br></pre></td></tr></table></figure><p>也可以手工删除资源：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete service/&lt;ClusterID&gt;</span><br></pre></td></tr></table></figure></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>可以看到，Flink 1.10 版本对和 Kubernetes 的集成做了很好的尝试。期待社区后续的 1.11 版本能对 per-job 提供支持，以及和 Kubernetes 的深度集成，例如基于原生 Kubernetes API 的高可用。最新进展请关注 <a href="https://issues.apache.org/jira/browse/FLINK-14460" target="_blank" rel="noopener">FLINK-14460</a>。</p>]]></content:encoded>
      
      
      
      <category domain="https://zhoukaibo.com/tags/flink/">flink</category>
      
      <category domain="https://zhoukaibo.com/tags/kubernetes/">kubernetes</category>
      
      
      <comments>https://zhoukaibo.com/2020/02/18/Flink-1-10-Native-Kubernetes/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Flink on Yarn / Kubernetes 原理剖析及实践</title>
      <link>https://zhoukaibo.com/2019/10/15/flink-yarn-k8s-principle-practice/</link>
      <guid>https://zhoukaibo.com/2019/10/15/flink-yarn-k8s-principle-practice/</guid>
      <pubDate>Tue, 15 Oct 2019 14:33:23 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;本文是我参加 Apache Flink China 社区钉钉群直播的教程（&lt;a href=&quot;https://ververica.cn/developers/flink-training-course2/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;进阶篇</description>
        
      
      
      
      <content:encoded><![CDATA[<p>本文是我参加 Apache Flink China 社区钉钉群直播的教程（<a href="https://ververica.cn/developers/flink-training-course2/" target="_blank" rel="noopener">进阶篇 2.4</a>）。我稍作调整后发在<a href="https://zhoukaibo.com">个人网站(zhoukaibo.com)</a>上。</p><p>文中首先对 <a href="https://zhoukaibo.com/tags/flink/">Flink</a> 架构进行了概述；其次介绍了Yarn在Flink应用中的原理及实践；再次介绍了Kubernetes在Flink应用中的原理及实践；最后对Flink on Yarn/Kubernetes中存在的一些问题进行了解析。</p><h1 id="Flink-架构概览"><a href="#Flink-架构概览" class="headerlink" title="Flink 架构概览"></a>Flink 架构概览</h1><h2 id="Flink架构概览–Job"><a href="#Flink架构概览–Job" class="headerlink" title="Flink架构概览–Job"></a>Flink架构概览–Job</h2><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015223853.png" alt=""></p><p>用户通过 DataStream API、DataSet API、SQL 和 Table API 编写 Flink 任务，它会生成一个JobGraph。JobGraph 是由 source、map()、keyBy()/window()/apply() 和 Sink 等算子组成的。当 JobGraph 提交给 Flink 集群后，能够以 Local、Standalone、<a href="https://zhoukaibo.com/tags/yarn/">Yarn</a> 和 <a href="https://zhoukaibo.com/tags/k8s/">Kubernetes</a> 四种模式运行。</p><h2 id="Flink架构概览–JobManager"><a href="#Flink架构概览–JobManager" class="headerlink" title="Flink架构概览–JobManager"></a>Flink架构概览–JobManager</h2><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015223909.png" alt=""></p><p>JobManager的功能主要有：</p><ul><li>将 JobGraph 转换成 Execution Graph，最终将 Execution Graph 拿来运行</li><li>Scheduler 组件负责 Task 的调度</li><li>Checkpoint Coordinator 组件负责协调整个任务的 Checkpoint，包括 Checkpoint 的开始和完成</li><li>通过 Actor System 与 TaskManager 进行通信</li><li>其它的一些功能，例如 Recovery Metadata，用于进行故障恢复时，可以从 Metadata里面读取数据。 </li></ul><h2 id="Flink架构概览–TaskManager"><a href="#Flink架构概览–TaskManager" class="headerlink" title="Flink架构概览–TaskManager"></a>Flink架构概览–TaskManager</h2><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015223928.png" alt=""></p><p>TaskManager 是负责具体任务的执行过程，在 JobManager 申请到资源之后开始启动。TaskManager 里面的主要组件有：</p><ul><li>Memory &amp; I/O Manager，即内存I/O的管理</li><li>Network Manager，用来对网络方面进行管理</li><li>Actor system，用来负责网络的通信</li></ul><p>TaskManager 被分成很多个 TaskSlot，每个任务都要运行在一个 TaskSlot 里面，TaskSlot 是调度资源里的最小单位。</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015223944.jpg" alt=""></p><p>在介绍 <a href="https://zhoukaibo.com/tags/yarn/">Yarn</a> 之前先简单的介绍一下 Flink Standalone 模式，这样有助于更好地了解 Yarn 和 <a href="https://zhoukaibo.com/tags/k8s/">Kubernetes</a> 架构。</p><ul><li>在 Standalone 模式下，Master 和 TaskManager 可以运行在同一台机器上，也可以运行在不同的机器上。</li><li>在 Master 进程中，Standalone ResourceManager 的作用是对资源进行管理。当用户通过 Flink Cluster Client 将 JobGraph 提交给 Master 时，JobGraph 先经过 Dispatcher。</li><li>当 Dispatcher 收到客户端的请求之后，生成一个 JobManager。接着 JobManager 进程向 Standalone ResourceManager 申请资源，最终再启动 TaskManager。</li><li>TaskManager 启动之后，会有一个注册的过程，注册之后 JobManager 再将具体的 Task 任务分发给这个 TaskManager 去执行。</li></ul><p>以上就是一个 Standalone 任务的运行过程。 </p><h2 id="Flink-运行时相关组件"><a href="#Flink-运行时相关组件" class="headerlink" title="Flink 运行时相关组件"></a>Flink 运行时相关组件</h2><p>接下来总结一下 <a href="https://zhoukaibo.com/tags/flink/">Flink</a> 的基本架构和它在运行时的一些组件，具体如下： </p><ul><li>Client：用户通过 SQL 或者 API 的方式进行任务的提交，提交后会生成一个 JobGraph。</li><li>JobManager：JobManager 接受到用户的请求之后，会对任务进行调度，并且申请资源启动 TaskManager。</li><li>TaskManager：它负责一个具体 Task 的执行。TaskManager 向 JobManager 进行注册，当 TaskManager 接收到 JobManager 分配的任务之后，开始执行具体的任务。</li></ul><h1 id="Flink-on-Yarn-原理及实践"><a href="#Flink-on-Yarn-原理及实践" class="headerlink" title="Flink on Yarn 原理及实践"></a>Flink on Yarn 原理及实践</h1><h2 id="Yarn-架构原理–总览"><a href="#Yarn-架构原理–总览" class="headerlink" title="Yarn 架构原理–总览"></a>Yarn 架构原理–总览</h2><p>Yarn 模式在国内使用比较广泛，基本上大多数公司在生产环境中都使用过 Yarn 模式。首先介绍一下 Yarn 的架构原理，因为只有足够了解 Yarn 的架构原理，才能更好的知道 Flink 是如何在 Yarn 上运行的。</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015224001.png" alt=""></p><p>Yarn 的架构原理如上图所示，最重要的角色是 ResourceManager，主要用来负责整个资源的管理，Client 端是负责向 ResourceManager 提交任务。</p><p>用户在 Client 端提交任务后会先给到 Resource Manager。Resource Manager 会启动 Container，接着进一步启动 Application Master，即对 Master 节点的启动。当 Master 节点启动之后，会向 Resource Manager 再重新申请资源，当 Resource Manager 将资源分配给 Application Master 之后，Application Master 再将具体的 Task 调度起来去执行。</p><h2 id="Yarn架构原理–组件"><a href="#Yarn架构原理–组件" class="headerlink" title="Yarn架构原理–组件"></a>Yarn架构原理–组件</h2><p>Yarn 集群中的组件包括：</p><ul><li>ResourceManager (RM)：ResourceManager (RM)负责处理客户端请求、启动/监控 ApplicationMaster、监控 NodeManager、资源的分配与调度，包含 Scheduler 和 Applications Manager。</li><li>ApplicationMaster (AM)：ApplicationMaster (AM)运行在 Slave 上，负责数据切分、申请资源和分配、任务监控和容错。</li><li>NodeManager (NM)：NodeManager (NM)运行在 Slave 上，用于单节点资源管理、AM/RM通信以及汇报状态。</li><li>Container：Container 负责对资源进行抽象，包括内存、CPU、磁盘，网络等资源。</li></ul><h2 id="Yarn-架构原理–交互"><a href="#Yarn-架构原理–交互" class="headerlink" title="Yarn 架构原理–交互"></a>Yarn 架构原理–交互</h2><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015224017.png" alt=""></p><p>以在 <a href="https://zhoukaibo.com/tags/yarn/">Yarn</a> 上运行 MapReduce 任务为例来讲解下 Yarn 架构的交互原理：</p><ul><li>首先，用户编写 MapReduce 代码后，通过 Client 端进行任务提交</li><li>ResourceManager 在接收到客户端的请求后，会分配一个 Container 用来启动 ApplicationMaster，并通知 NodeManager 在这个 Container 下启动 ApplicationMaster。</li><li>ApplicationMaster 启动后，向 ResourceManager 发起注册请求。接着 ApplicationMaster 向 ResourceManager 申请资源。根据获取到的资源，和相关的 NodeManager 通信，要求其启动程序。</li><li>一个或者多个 NodeManager 启动 Map/Reduce Task。</li><li>NodeManager 不断汇报 Map/Reduce Task 状态和进展给 ApplicationMaster。</li><li>当所有 Map/Reduce Task 都完成时，ApplicationMaster 向 ResourceManager 汇报任务完成，并注销自己。</li></ul><h2 id="Flink-on-Yarn–Per-Job"><a href="#Flink-on-Yarn–Per-Job" class="headerlink" title="Flink on Yarn–Per Job"></a>Flink on Yarn–Per Job</h2><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191021095226.jpg" alt=""></p><p>Flink on Yarn中的 Per Job 模式是指每次提交一个任务，然后任务运行完成之后资源就会被释放。在了解了 Yarn 的原理之后，Per Job 的流程也就比较容易理解了，具体如下：</p><ul><li>首先 Client 提交 Yarn App，比如 JobGraph 或者 JARs</li><li>接下来 Yarn 的 ResourceManager 会申请第一个 Container。这个Container 通过 Application Master 启动进程，Application Master 里面运行的是 <a href="https://zhoukaibo.com/tags/flink/">Flink</a> 程序，即 Flink-Yarn ResourceManager 和 JobManager。</li><li>最后 Flink-Yarn ResourceManager 向 Yarn ResourceManager 申请资源。当分配到资源后，启动 TaskManager。TaskManager 启动后向 Flink-Yarn ResourceManager 进行注册，注册成功后 JobManager 就会分配具体的任务给 TaskManager 开始执行。</li></ul><h2 id="Flink-on-Yarn–Session"><a href="#Flink-on-Yarn–Session" class="headerlink" title="Flink on Yarn–Session"></a>Flink on Yarn–Session</h2><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191021095310.jpg" alt=""></p><p>在 Per Job 模式中，执行完任务后整个资源就会释放，包括 JobManager、TaskManager 都全部退出。而  Session 模式则不一样，它的 Dispatcher 和 ResourceManager 是可以复用的。Session 模式下，当 Dispatcher 在收到请求之后，会启动 JobManager(A)，让 JobManager(A) 来完成启动 TaskManager，接着会启动 JobManager(B) 和对应的 TaskManager 的运行。当 A、B 任务运行完成后，资源并不会释放。Session 模式也称为多线程模式，其特点是资源会一直存在不会释放，多个 JobManager 共享一个 Dispatcher，而且还共享 Flink-YARN ResourceManager。</p><p>Session 模式和 Per Job 模式的应用场景不一样。Per Job 模式比较适合那种对启动时间不敏感，运行时间较长的任务。Seesion 模式适合短时间运行的任务，一般是批处理任务。若用 Per Job 模式去运行短时间的任务，那就需要频繁的申请资源，运行结束后，还需要资源释放，下次还需再重新申请资源才能运行。显然，这种任务会频繁启停的情况不适用于 Per Job 模式，更适合用 Session 模式。</p><h2 id="Yarn模式特点"><a href="#Yarn模式特点" class="headerlink" title="Yarn模式特点"></a>Yarn模式特点</h2><p>Yarn 模式的优点有：</p><ul><li>资源的统一管理和调度。<a href="https://zhoukaibo.com/tags/yarn/">Yarn</a> 集群中所有节点的资源（内存、CPU、磁盘、网络等）被抽象为 Container。计算框架需要资源进行运算任务时需要向 Resource Manager 申请 Container，YARN 按照特定的策略对资源进行调度和进行 Container 的分配。Yarn 模式能通过多种任务调度策略来利用提高集群资源利用率。例如 FIFO Scheduler、Capacity Scheduler、Fair Scheduler，并能设置任务优先级。</li><li>资源隔离：Yarn 使用了轻量级资源隔离机制 Cgroups 进行资源隔离以避免相互干扰，一旦 Container 使用的资源量超过事先定义的上限值，就将其杀死。</li><li>自动 failover 处理。例如 Yarn NodeManager 监控、Yarn ApplicationManager 异常恢复。</li></ul><p>Yarn 模式虽然有不少优点，但是也有诸多缺点，例如运维部署成本较高，灵活性不够。</p><h2 id="Flink-on-Yarn-实践"><a href="#Flink-on-Yarn-实践" class="headerlink" title="Flink on Yarn 实践"></a>Flink on Yarn 实践</h2><p>关于 Flink on Yarn 的实践在 <a href="https://ververica.cn/developers/flink-training-course1/" target="_blank" rel="noopener">https://ververica.cn/developers/flink-training-course1/</a> 上面有很多课程，例如：《Flink 安装部署、环境配置及运行应用程序》 和 《客户端操作》都是基于 Yarn 进行讲解的，这里就不再赘述。</p><h1 id="Flink-on-Kubernetes-原理剖析"><a href="#Flink-on-Kubernetes-原理剖析" class="headerlink" title="Flink on Kubernetes 原理剖析"></a>Flink on Kubernetes 原理剖析</h1><p>Kubernetes 是 Google 开源的容器集群管理系统，其提供应用部署、维护、扩展机制等功能，利用 Kubernetes 能方便地管理跨机器运行容器化的应用。Kubernetes 和 Yarn 相比，相当于下一代的资源管理系统，但是它的能力远远不止这些。</p><h2 id="Kubernetes–基本概念"><a href="#Kubernetes–基本概念" class="headerlink" title="Kubernetes–基本概念"></a>Kubernetes–基本概念</h2><p>Kubernetes（k8s）中的 Master 节点，负责管理整个集群，含有一个集群的资源数据访问入口，还包含一个 Etcd 高可用键值存储服务。Master 中运行着 API Server，Controller Manager 及 Scheduler 服务。</p><p>Node 为集群的一个操作单元，是 Pod 运行的宿主机。Node 节点里包含一个 agent 进程，能够维护和管理该Node 上的所有容器的创建、启停等。Node 还含有一个服务端 kube-proxy，用于服务发现、反向代理和负载均衡。Node 底层含有 docker engine，docker引擎主要负责本机容器的创建和管理工作。</p><p>Pod 运行于 Node 节点上，是若干相关容器的组合。在 k8s 里面 Pod 是创建、调度和管理的最小单位。</p><h2 id="Kubernetes–架构图"><a href="#Kubernetes–架构图" class="headerlink" title="Kubernetes–架构图"></a>Kubernetes–架构图</h2><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015224120.png" alt=""></p><p>Kubernetes 的架构如图所示，从这个图里面能看出 <a href="https://zhoukaibo.com/tags/k8s/">Kubernetes</a> 的整个运行过程。</p><ul><li>API Server 相当于用户的一个请求入口，用户可以提交命令给 Etcd，这时会将这些请求存储到 Etcd 里面去。</li><li>Etcd 是一个键值存储，负责将任务分配给具体的机器，在每个节点上的 Kubelet 会找到对应的 container 在本机上运行。</li><li>用户可以提交一个 Replication Controller 资源描述，Replication Controller 会监视集群中的容器并保持数量；用户也可以提交 service 描述文件，并由 kube proxy 负责具体工作的流量转发。</li></ul><h2 id="Kubernetes–核心概念"><a href="#Kubernetes–核心概念" class="headerlink" title="Kubernetes–核心概念"></a>Kubernetes–核心概念</h2><p>Kubernetes 中比较重要的概念有：</p><ul><li>Replication Controller (RC) 用来管理 Pod 的副本。RC 确保任何时候 Kubernetes 集群中有指定数量的 pod 副本(replicas) 在运行， 如果少于指定数量的 pod 副本，RC 会启动新的 Container，反之会杀死多余的以保证数量不变。</li><li>Service 提供了一个统一的服务访问入口以及服务代理和发现机制</li><li>Persistent Volume(PV) 和 Persistent Volume Claim(PVC) 用于数据的持久化存储。</li><li>ConfigMap 是指存储用户程序的配置文件，其后端存储是基于 Etcd。</li></ul><h2 id="Flink-on-Kubernetes–架构"><a href="#Flink-on-Kubernetes–架构" class="headerlink" title="Flink on Kubernetes–架构"></a>Flink on Kubernetes–架构</h2><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015224138.jpg" alt=""></p><p>Flink on Kubernetes 的架构如图所示，Flink 任务在 <a href="https://zhoukaibo.com/tags/k8s/">Kubernetes</a> 上运行的步骤有：</p><ul><li>首先往 Kubernetes 集群提交了资源描述文件后，会启动 Master 和 Worker 的 container。</li><li>Master Container 中会启动Flink Master Process，包含 Flink-Container ResourceManager、JobManager 和 Program Runner。</li><li>Worker Container 会启动 TaskManager，并向负责资源管理的 ResourceManager 进行注册，注册完成之后，由 JobManager 将具体的任务分给 Container，再由 Container 去执行。</li><li>需要说明的是，在 Flink 里的 Master 和 Worker 都是一个镜像，只是脚本的命令不一样，通过参数来选择启动 master 还是启动 Worker。</li></ul><h2 id="Flink-on-Kubernetes–JobManager"><a href="#Flink-on-Kubernetes–JobManager" class="headerlink" title="Flink on Kubernetes–JobManager"></a>Flink on Kubernetes–JobManager</h2><p>JobManager 的执行过程分为两步:</p><ul><li>首先，JobManager 通过 Deployment 进行描述，保证1个副本的 Container 运行 JobManager，可以定义一个标签，例如 flink-jobmanager。</li><li>其次，还需要定义一个 JobManager Service，通过 service name 和 port 暴露 JobManager 服务，通过标签选择对应的 pods。</li></ul><h2 id="Flink-on-Kubernetes–TaskManager"><a href="#Flink-on-Kubernetes–TaskManager" class="headerlink" title="Flink on Kubernetes–TaskManager"></a>Flink on Kubernetes–TaskManager</h2><p>TaskManager 也是通过 Deployment 来进行描述，保证n个副本的 Container 运行 TaskManager，同时也需要定义一个标签，例如 flink-taskmanager。</p><p>对于 JobManager 和 TaskManager 运行过程中需要的一些配置文件，如：flink-conf.yaml、hdfs-site.xml、core-site.xml，可以通过将它们定义为 ConfigMap 来实现配置的传递和读取。</p><h2 id="Flink-on-Kubernetes–交互"><a href="#Flink-on-Kubernetes–交互" class="headerlink" title="Flink on Kubernetes–交互"></a>Flink on Kubernetes–交互</h2><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015224154.jpg" alt=""></p><p>整个交互的流程比较简单，用户往 Kubernetes 集群提交定义好的资源描述文件即可，例如 deployment、configmap、service 等描述。后续的事情就交给 Kubernetes 集群自动完成。Kubernetes 集群会按照定义好的描述来启动 pod，运行用户程序。各个组件的具体工作如下：</p><ul><li>Service: 通过标签(label selector)找到 job manager 的 pod 暴露服务。</li><li>Deployment：保证 n 个副本的 container 运行 JM/TM，应用升级策略。</li><li>ConfigMap：在每个 pod 上通过挂载 /etc/flink 目录，包含 flink-conf.yaml 内容。</li></ul><h2 id="Flink-on-Kubernetes–实践"><a href="#Flink-on-Kubernetes–实践" class="headerlink" title="Flink on Kubernetes–实践"></a>Flink on Kubernetes–实践</h2><p>接下来就讲一下 Flink on Kubernetes 的实践篇，即<a href="https://zhoukaibo.com/tags/k8s/">K8s</a>上是怎么运行任务的。 </p><h3 id="Session-Cluster"><a href="#Session-Cluster" class="headerlink" title="Session Cluster"></a>Session Cluster</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">•Session Cluster</span><br><span class="line">•启动</span><br><span class="line">•kubectl create -f jobmanager-service.yaml </span><br><span class="line">•kubectl create -f jobmanager-deployment.yaml </span><br><span class="line">•kubectl create -f taskmanager-deployment.yaml</span><br><span class="line">•Submit job</span><br><span class="line">•kubectl port-forward service/flink-jobmanager 8081:8081</span><br><span class="line">•bin/flink run -d -m localhost:8081 ./examples/streaming/TopSpeedWindowing.jar</span><br><span class="line">•停止</span><br><span class="line">•kubectl delete -f jobmanager-deployment.yaml </span><br><span class="line">•kubectl delete -f taskmanager-deployment.yaml </span><br><span class="line">•kubectl delete -f  jobmanager-service.yaml</span><br></pre></td></tr></table></figure><p>首先启动 Session Cluster，执行上述三条启动命令就可以将 Flink 的 JobManager-service、jobmanager-deployment、taskmanager-deployment 启动起来。启动完成之后用户可以通过接口进行访问，然后通过端口进行提交任务。若想销毁集群，直接用 kubectl delete 即可，整个资源就可以销毁。</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015224215.jpg" alt=""></p><p><a href="https://zhoukaibo.com/tags/flink/">Flink</a> 官方提供的例子如图所示，图中左侧为 jobmanager-deployment.yaml 配置，右侧为 taskmanager-deployment.yaml 配置。</p><p>在 jobmanager-deployment.yaml 配置中，代码的第一行为 apiVersion，apiVersion 是API的一个版本号，版本号用的是 extensions/vlbetal 版本。资源类型为 Deployment，元数据 metadata 的名为 flink-jobmanager，spec 中含有副本数为1的 replicas，labels 标签用于 pod 的选取。containers 的镜像名为 jobmanager，containers 包含从公共 docker 仓库下载的 image，当然也可以使用公司内部的私有仓库。args 启动参数用于决定启动的是 jobmanager 还是 taskmanager；ports 是服务端口，常见的服务端口为8081端口；env 是定义的环境变量，会传递给具体的启动脚本。</p><p>右图为 taskmanager-deployment.yaml 配置，taskmanager-deployment.yaml 配置与 jobmanager-deployment.yaml 相似，但 taskmanager-deployment.yaml 的副本数是2个。</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015224230.png" alt=""></p><p>接下来是 jobmanager-service.yaml 的配置，jobmanager-service.yaml 的资源类型为 Service，在 Service 中的配置相对少一些，spec 中配置需要暴露的服务端口的 port，在 selector 中，通过标签选取  jobmanager 的 pod。</p><h3 id="Job-Cluster"><a href="#Job-Cluster" class="headerlink" title="Job Cluster"></a>Job Cluster</h3><p>除了 Session 模式，还有一种 Per Job 模式。在 Per Job 模式下，需要将用户代码都打到镜像里面，这样如果业务逻辑的变动涉及到 Jar 包的修改，都需要重新生成镜像，整个过程比较繁琐，因此在生产环境中使用的比较少。</p><p>以使用公用 docker 仓库为例，Job Cluster 的运行步骤如下：</p><ul><li>build 镜像：在 flink/flink-container/docker 目录下执行 build.sh 脚本，指定从哪个版本开始去构建镜像，成功后会输出 “Successfully tagged topspeed:latest” 的提示。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh build.sh --from-release --flink-version 1.7.0 --hadoop-version 2.8 --scala-version 2.11 --job-jar ~/flink/flink-1.7.1/examples/streaming/TopSpeedWindowing.jar --image-name topspeed</span><br></pre></td></tr></table></figure><ul><li>上传镜像：在 hub.docker.com 上需要注册账号和创建仓库进行上传镜像。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker tag topspeed zkb555/topspeedwindowing </span><br><span class="line">docker push zkb555/topspeedwindowing</span><br></pre></td></tr></table></figure><ul><li>启动任务：在镜像上传之后，可以启动任务。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f job-cluster-service.yaml </span><br><span class="line">FLINK_IMAGE_NAME=zkb555/topspeedwindowing:latest FLINK_JOB=org.apache.flink.streaming.examples.windowing.TopSpeedWindowing FLINK_JOB_PARALLELISM=3 envsubst &lt; job-cluster-job.yaml.template | kubectl create -f – </span><br><span class="line">FLINK_IMAGE_NAME=zkb555/topspeedwindowing:latest FLINK_JOB_PARALLELISM=4 envsubst &lt; task-manager-deployment.yaml.template | kubectl create -f -</span><br></pre></td></tr></table></figure><h1 id="Flink-on-Yarn-Kubernetes-问题解答"><a href="#Flink-on-Yarn-Kubernetes-问题解答" class="headerlink" title="Flink on Yarn/Kubernetes 问题解答"></a>Flink on Yarn/Kubernetes 问题解答</h1><p><strong>问题一： Flink 在 k8s 上可以通过 Operator 方式提交任务吗？</strong></p><p>目前 Flink 官方还没有提供 Operator 的方式，Lyft 公司开源了自己的 Operator 实现：<a href="https://github.com/lyft/flinkk8soperator" target="_blank" rel="noopener">flinkk8soperator</a>。GoogleCloudPlatform 也开源了一个实现：<a href="https://github.com/GoogleCloudPlatform/flink-on-k8s-operator" target="_blank" rel="noopener">flink-on-k8s-operator</a>。</p><p><strong>问题二： 在 k8s 集群上如果不使用 Zookeeper 有没有其他高可用（HA）的方案？</strong></p><p>Etcd 是一个类似于 Zookeeper 的高可用键值服务，目前 Flink 社区正在考虑基于 Etcd 实现高可用的方案（<a href="https://issues.apache.org/jira/browse/FLINK-11105" target="_blank" rel="noopener">FLINK-11105</a>）以及直接依赖 k8s API的方案（<a href="https://issues.apache.org/jira/browse/FLINK-12884" target="_blank" rel="noopener">FLINK-12884</a>）。</p><p><strong>问题三： Flink on k8s 在任务启动时需要指定 TaskManager 的个数，有和 Yarn 一样的动态资源申请方式吗？</strong></p><p>Flink on k8s 目前的实现在任务启动前就需要确定好 TaskManager 的个数，这样容易造成 TM 指定太少，任务无法启动，或者指定的太多，造成资源浪费。社区正在考虑实现和 Yarn 一样的任务启动时动态资源申请的方式。这是一种和 k8s 结合的更为 Nativey 的方式，称为 <code>Active</code> 模式。Active 意味着 ResourceManager 可以直接向 k8s 集群申请资源。具体设计方案和进展请关注 <a href="https://issues.apache.org/jira/browse/FLINK-9953" target="_blank" rel="noopener">FLINK-9953</a>。</p>]]></content:encoded>
      
      
      
      <category domain="https://zhoukaibo.com/tags/flink/">flink</category>
      
      <category domain="https://zhoukaibo.com/tags/kubernetes/">kubernetes</category>
      
      <category domain="https://zhoukaibo.com/tags/yarn/">yarn</category>
      
      <category domain="https://zhoukaibo.com/tags/k8s/">k8s</category>
      
      
      <comments>https://zhoukaibo.com/2019/10/15/flink-yarn-k8s-principle-practice/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Apache Flink 客户端操作</title>
      <link>https://zhoukaibo.com/2019/10/15/apache-flink-intro-client/</link>
      <guid>https://zhoukaibo.com/2019/10/15/apache-flink-intro-client/</guid>
      <pubDate>Tue, 15 Oct 2019 12:09:25 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;本文是我参加 Apache Flink China 社区钉钉群直播的教程（&lt;a href=&quot;https://ververica.cn/developers/flink-training-course1/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;基础篇</description>
        
      
      
      
      <content:encoded><![CDATA[<p>本文是我参加 Apache Flink China 社区钉钉群直播的教程（<a href="https://ververica.cn/developers/flink-training-course1/" target="_blank" rel="noopener">基础篇 1.5</a>）。我稍作调整后发在<a href="https://zhoukaibo.com">个人网站(zhoukaibo.com)</a>上。</p><h1 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h1><p>在前面几期的课程里面讲过了Flink开发环境的搭建和应用的部署以及运行，今天的课程主要是讲Flink的客户端操作。本次讲解以实际操作为主。这次课程是基于社区的Flink 1.7.2版本，操作系统是Mac系统，浏览器是Google Chrome浏览器。有关开发环境的准备和集群的部署，请参考前面第三期的内容。</p><h1 id="课程概要"><a href="#课程概要" class="headerlink" title="课程概要"></a>课程概要</h1><p>如下图所示，<a href="https://zhoukaibo.com/tags/flink/">Flink</a>提供了丰富的客户端操作来提交任务和与任务进行交互，包括 Flink命令行，Scala Shell，SQL Client，Restful API 和 Web。Flink首先提供的最重要的是命令行，其次是SQL Client用于提交SQL任务的运行，还有就是Scala Shell提交Table API的任务。同时，Flink也提供了Restful服务，用户可以通过http方式进行调用。此外，还有Web的方式可以提交任务。</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015215818.png" alt="flink_clients.png"></p><p>在 flink 安装目录的 bin 目录下面可以看到有 flink, start-scala-shell.sh 和 sql-client.sh 等文件，这些都是客户端操作的入口。</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015215849.jpg" alt="flink_1_7_2.jpg"></p><h1 id="Flink-客户端操作"><a href="#Flink-客户端操作" class="headerlink" title="Flink 客户端操作"></a>Flink 客户端操作</h1><h2 id="Flink命令行"><a href="#Flink命令行" class="headerlink" title="Flink命令行"></a>Flink命令行</h2><p>flink 的命令行参数很多，输入 flink -h 能看到完整的说明：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/flink -h</span><br></pre></td></tr></table></figure><p>如果想看某一个命令的参数，比如 run 命令，输入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/flink run -h</span><br></pre></td></tr></table></figure><p>本文主要讲解常见的一些操作，更详细的文档请参考: <a href="https://ci.apache.org/projects/flink/flink-docs-stable/ops/cli.html" target="_blank" rel="noopener">Flink 命令行官方文档</a>。</p><h3 id="standalone"><a href="#standalone" class="headerlink" title="standalone"></a>standalone</h3><p>首先启动一个 standalone 的集群</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/start-cluster.sh</span><br><span class="line">Starting cluster.</span><br><span class="line">Starting standalonesession daemon on host zkb-MBP.local.</span><br><span class="line">Starting taskexecutor daemon on host zkb-MBP.local.</span><br></pre></td></tr></table></figure><p>打开 <a href="http://127.0.0.1:8081" target="_blank" rel="noopener">http://127.0.0.1:8081</a> 能看到 Web 界面。</p><h4 id="run"><a href="#run" class="headerlink" title="run"></a>run</h4><p>运行任务，以 <a href="https://zhoukaibo.com/tags/flink/">flink</a> 自带的例子 TopSpeedWindowing 为例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/flink run -d examples/streaming/TopSpeedWindowing.jar</span><br><span class="line">Starting execution of program</span><br><span class="line">Executing TopSpeedWindowing example with default input data set.</span><br><span class="line">Use --input to specify file input.</span><br><span class="line">Printing result to stdout. Use --output to specify output path.</span><br><span class="line">Job has been submitted with JobID 5e20cb6b0f357591171dfcca2eea09de</span><br></pre></td></tr></table></figure><p>运行起来后默认是1个并发:<br><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015215922.jpg" alt="flink_run_1.jpg"></p><p>点左侧『Task Manager』，然后点『Stdout』能看到输出日志：<br><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015215949.jpg" alt="flink_run_2.jpg"></p><p>或者查看本地 log 目录下的 *.out 文件：<br><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220107.jpg" alt="flink_run_3.jpg"></p><h4 id="list"><a href="#list" class="headerlink" title="list"></a>list</h4><p>查看任务列表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/flink list -m 127.0.0.1:8081</span><br><span class="line">Waiting for response...</span><br><span class="line">------------------ Running/Restarting Jobs -------------------</span><br><span class="line">24.03.2019 10:14:06 : 5e20cb6b0f357591171dfcca2eea09de : CarTopSpeedWindowingExample (RUNNING)</span><br><span class="line">--------------------------------------------------------------</span><br><span class="line">No scheduled jobs.</span><br></pre></td></tr></table></figure><h4 id="stop"><a href="#stop" class="headerlink" title="stop"></a>stop</h4><p>停止任务。通过 -m 来指定要停止的 JobManager 的主机地址和端口。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/flink stop -m 127.0.0.1:8081 d67420e52bd051fae2fddbaa79e046bb</span><br><span class="line">Stopping job d67420e52bd051fae2fddbaa79e046bb.</span><br><span class="line">------------------------------------------------------------</span><br><span class="line">The program finished with the following exception:</span><br><span class="line">  org.apache.flink.util.FlinkException: Could not stop the job   d67420e52bd051fae2fddbaa79e046bb.</span><br><span class="line">  at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:554)</span><br><span class="line">  at org.apache.flink.client.cli.CliFrontend.runClusterAction(CliFrontend.java:985)</span><br><span class="line">  at org.apache.flink.client.cli.CliFrontend.stop(CliFrontend.java:547)</span><br><span class="line">  at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:1062)</span><br><span class="line">  at org.apache.flink.client.cli.CliFrontend.lambda$main$11(CliFrontend.java:1126)</span><br><span class="line">  at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">  at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)</span><br><span class="line">  at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)</span><br><span class="line">  at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1126)</span><br><span class="line">Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.rest.util.RestClientException: [Job termination (STOP) failed: This job is not stoppable.]</span><br><span class="line">  at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)</span><br><span class="line">  at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1915)</span><br><span class="line">  at org.apache.flink.client.program.rest.RestClusterClient.stop(RestClusterClient.java:392)</span><br><span class="line">  at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:552)</span><br><span class="line">... 9 more</span><br><span class="line">Caused by: org.apache.flink.runtime.rest.util.RestClientException: [Job termination (STOP) failed: This job is not stoppable.]</span><br><span class="line">  at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:380)</span><br><span class="line">  at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$3(RestClient.java:364)</span><br><span class="line">  at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:952)</span><br><span class="line">  at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926)</span><br><span class="line">  at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)</span><br><span class="line">  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">  at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure><p>从日志里面能看出 stop 命令执行失败了。一个 job 能够被 stop 要求所有的 source 都是可以 stoppable 的，即实现了 StoppableFunction 接口。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"> </span><br><span class="line">/**</span><br><span class="line"> * 需要能 stoppable 的函数必须实现这个接口，例如流式任务的 source。</span><br><span class="line"> * stop() 方法在任务收到 STOP 信号的时候调用。</span><br><span class="line"> * source 在接收到这个信号后，必须停止发送新的数据且优雅的停止。</span><br><span class="line"> */</span><br><span class="line">@PublicEvolving</span><br><span class="line">public interface StoppableFunction &#123;</span><br><span class="line">/**</span><br><span class="line">  * 停止 source。与 cancel() 不同的是，这是一个让 source 优雅停止的请求。</span><br><span class="line">     * 等待中的数据可以继续发送出去，不需要立即停止。</span><br><span class="line">  */</span><br><span class="line">void stop();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="cancel"><a href="#cancel" class="headerlink" title="cancel"></a>cancel</h4><p>取消任务。如果在 conf/flink-conf.yaml 里面配置了 state.savepoints.dir，会保存savepoint，否则不会保存 savepoint。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/flink cancel -m 127.0.0.1:8081 5e20cb6b0f357591171dfcca2eea09de</span><br><span class="line"> </span><br><span class="line">Cancelling job 5e20cb6b0f357591171dfcca2eea09de.</span><br><span class="line">Cancelled job 5e20cb6b0f357591171dfcca2eea09de.</span><br></pre></td></tr></table></figure><p>也可以在停止的时候显式指定 savepoint 目录。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/flink cancel -m 127.0.0.1:8081 -s /tmp/savepoint 29da945b99dea6547c3fbafd57ed8759</span><br><span class="line"> </span><br><span class="line">Cancelling job 29da945b99dea6547c3fbafd57ed8759 with savepoint to /tmp/savepoint.</span><br><span class="line">Cancelled job 29da945b99dea6547c3fbafd57ed8759. Savepoint stored in file:/tmp/savepoint/savepoint-29da94-88299bacafb7.</span><br><span class="line"> </span><br><span class="line">➜  flink-1.7.2 ll /tmp/savepoint/savepoint-29da94-88299bacafb7</span><br><span class="line">total 32K</span><br><span class="line">-rw-r--r-- 1 baoniu 29K Mar 24 10:33 _metadata</span><br></pre></td></tr></table></figure><p>取消和停止（流作业）的区别如下：</p><ul><li>cancel() 调用，立即调用作业算子的 cancel() 方法，以尽快取消它们。如果算子在接到 cancel() 调用后没有停止，Flink 将开始定期中断算子线程的执行，直到所有算子停止为止。</li><li>stop() 调用，是更优雅的停止正在运行流作业的方式。stop() 仅适用于 source 实现了 StoppableFunction 接口的作业。当用户请求停止作业时，作业的所有 source 都将接收 stop() 方法调用。直到所有 source 正常关闭时，作业才会正常结束。这种方式，使作业正常处理完所有作业。</li></ul><h4 id="savepoint"><a href="#savepoint" class="headerlink" title="savepoint"></a>savepoint</h4><p>触发 savepoint。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/flink savepoint -m 127.0.0.1:8081 ec53edcfaeb96b2a5dadbfbe5ff62bbb /tmp/savepoint</span><br><span class="line">Triggering savepoint for job ec53edcfaeb96b2a5dadbfbe5ff62bbb.</span><br><span class="line">Waiting for response...</span><br><span class="line">Savepoint completed. Path: file:/tmp/savepoint/savepoint-ec53ed-84b00ce500ee</span><br><span class="line">You can resume your program from this savepoint with the run command.</span><br></pre></td></tr></table></figure><p>说明：savepoint 和 checkpoint 的区别（<a href="https://www.ververica.com/blog/differences-between-savepoints-and-checkpoints-in-flink" target="_blank" rel="noopener">详见文档</a>）：</p><ul><li>checkpoint 是增量做的，每次的时间较短，数据量较小，只要在程序里面启用后会自动触发，用户无须感知；checkpoint 是作业 failover 的时候自动使用，不需要用户指定。</li><li>savepoint 是全量做的，每次的时间较长，数据量较大，需要用户主动去触发。savepoint 一般用于程序的版本更新（<a href="https://ci.apache.org/projects/flink/flink-docs-stable/ops/upgrading.html#step-1-take-a-savepoint-in-the-old-flink-version" target="_blank" rel="noopener">详见文档</a>），bug修复，A/B Test等场景，需要用户指定。</li></ul><p>通过 -s 参数从指定的 savepoint 启动：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/flink run -d -s /tmp/savepoint/savepoint-f049ff-24ec0d3e0dc7 ./examples/streaming/TopSpeedWindowing.jar</span><br><span class="line">Starting execution of program</span><br><span class="line">Executing TopSpeedWindowing example with default input data set.</span><br><span class="line">Use --input to specify file input.</span><br><span class="line">Printing result to stdout. Use --output to specify output path.</span><br></pre></td></tr></table></figure><p>查看 JobManager 的日志，能够看到类似这样的log：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">2019-03-28 10:30:53,957 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     </span><br><span class="line">- Starting job 790d7b98db6f6af55d04aec1d773852d from savepoint /tmp/savepoint/savepoint-f049ff-24ec0d3e0dc7 ()</span><br><span class="line">2019-03-28 10:30:53,959 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    </span><br><span class="line"> - Reset the checkpoint ID of job 790d7b98db6f6af55d04aec1d773852d to 2.</span><br><span class="line">2019-03-28 10:30:53,959 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     </span><br><span class="line">- Restoring job 790d7b98db6f6af55d04aec1d773852d from latest valid checkpoint: Checkpoint 1 @ 0 for 790d7b98db6f6af55d04aec1d773852d.</span><br></pre></td></tr></table></figure><h4 id="modify"><a href="#modify" class="headerlink" title="modify"></a>modify</h4><p>修改任务并行度。<br>为了方便演示，我们修改 conf/flink-conf.yaml 将 task slot 数从默认的 1 改为 4，并配置 savepoint 目录。（modify 参数后面接 -s 指定savepoint路径当前版本可能有bug，提示无法识别）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">taskmanager.numberOfTaskSlots: 4</span><br><span class="line">state.savepoints.dir: file:///tmp/savepoint</span><br></pre></td></tr></table></figure><p>修改参数后需要重启集群生效，然后再启动任务：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/stop-cluster.sh &amp;&amp; bin/start-cluster.sh</span><br><span class="line">Stopping taskexecutor daemon (pid: 53139) on host zkb-MBP.local.</span><br><span class="line">Stopping standalonesession daemon (pid: 52723) on host zkb-MBP.local.</span><br><span class="line">Starting cluster.</span><br><span class="line">Starting standalonesession daemon on host zkb-MBP.local.</span><br><span class="line">Starting taskexecutor daemon on host zkb-MBP.local.</span><br><span class="line"> </span><br><span class="line">➜  flink-1.7.2 bin/flink run -d examples/streaming/TopSpeedWindowing.jar</span><br><span class="line">Starting execution of program</span><br><span class="line">Executing TopSpeedWindowing example with default input data set.</span><br><span class="line">Use --input to specify file input.</span><br><span class="line">Printing result to stdout. Use --output to specify output path.</span><br><span class="line">Job has been submitted with JobID 7752ea7b0e7303c780de9d86a5ded3fa</span><br></pre></td></tr></table></figure><p>从页面上能看到 Task Slots 变为了 4，这时候任务的默认并发度是 1。</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220147.jpg" alt="standalone-modify-1.jpg"></p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220219.jpg" alt="standalone-modify-2.jpg"></p><p>通过 modify 命令依次将并发度修改为 4 和 3，可以看到每次 modify 命令都会触发一次 savepoint。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/flink modify -p 4 7752ea7b0e7303c780de9d86a5ded3fa</span><br><span class="line">Modify job 7752ea7b0e7303c780de9d86a5ded3fa.</span><br><span class="line">Rescaled job 7752ea7b0e7303c780de9d86a5ded3fa. Its new parallelism is 4.</span><br><span class="line"> </span><br><span class="line">➜  flink-1.7.2 ll /tmp/savepoint</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x 3 baoniu 96 Jun 17 09:05 savepoint-7752ea-00c05b015836/</span><br><span class="line"> </span><br><span class="line">➜  flink-1.7.2 bin/flink modify -p 3 7752ea7b0e7303c780de9d86a5ded3fa</span><br><span class="line">Modify job 7752ea7b0e7303c780de9d86a5ded3fa.</span><br><span class="line">Rescaled job 7752ea7b0e7303c780de9d86a5ded3fa. Its new parallelism is 3.</span><br><span class="line"> </span><br><span class="line">➜  flink-1.7.2 ll /tmp/savepoint</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x 3 baoniu 96 Jun 17 09:08 savepoint-7752ea-449b131b2bd4/</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220242.jpg" alt="standalone-modify-3.jpg"></p><p>查看JobManager的日志，可以看到：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">2019-06-17 09:05:11,179 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Starting job 7752ea7b0e7303c780de9d86a5ded3fa from savepoint file:/tmp/savepoint/savepoint-790d7b-3581698f007e ()</span><br><span class="line">2019-06-17 09:05:11,182 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Reset the checkpoint ID of job 7752ea7b0e7303c780de9d86a5ded3fa to 3.</span><br><span class="line">2019-06-17 09:05:11,182 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Restoring job 790d7b98db6f6af55d04aec1d773852d from latest valid checkpoint: Checkpoint 2 @ 0 for 7752ea7b0e7303c780de9d86a5ded3fa.</span><br><span class="line">2019-06-17 09:05:11,184 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - No master state to restore</span><br><span class="line">2019-06-17 09:05:11,184 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Job CarTopSpeedWindowingExample (7752ea7b0e7303c780de9d86a5ded3fa) switched from state RUNNING to SUSPENDING.</span><br><span class="line">org.apache.flink.util.FlinkException: Job is being rescaled.</span><br></pre></td></tr></table></figure><h4 id="info"><a href="#info" class="headerlink" title="info"></a>info</h4><p>info 命令是用来查看 <a href="https://zhoukaibo.com/tags/flink/">flink</a> 任务的执行计划（StreamGraph）的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/flink info examples/streaming/TopSpeedWindowing.jar</span><br><span class="line">----------------------- Execution Plan -----------------------</span><br><span class="line">&#123;&quot;nodes&quot;:[&#123;&quot;id&quot;:1,&quot;type&quot;:&quot;Source: Custom Source&quot;,&quot;pact&quot;:&quot;Data Source&quot;,&quot;contents&quot;:&quot;Source: Custom Source&quot;,&quot;parallelism&quot;:1&#125;,&#123;&quot;id&quot;:2,&quot;type&quot;:&quot;Timestamps/Watermarks&quot;,&quot;pact&quot;:&quot;Operator&quot;,&quot;contents&quot;:&quot;Timestamps/Watermarks&quot;,&quot;parallelism&quot;:1,&quot;predecessors&quot;:[&#123;&quot;id&quot;:1,&quot;ship_strategy&quot;:&quot;FORWARD&quot;,&quot;side&quot;:&quot;second&quot;&#125;]&#125;,&#123;&quot;id&quot;:4,&quot;type&quot;:&quot;Window(GlobalWindows(), DeltaTrigger, TimeEvictor, ComparableAggregator, PassThroughWindowFunction)&quot;,&quot;pact&quot;:&quot;Operator&quot;,&quot;contents&quot;:&quot;Window(GlobalWindows(), DeltaTrigger, TimeEvictor, ComparableAggregator, PassThroughWindowFunction)&quot;,&quot;parallelism&quot;:1,&quot;predecessors&quot;:[&#123;&quot;id&quot;:2,&quot;ship_strategy&quot;:&quot;HASH&quot;,&quot;side&quot;:&quot;second&quot;&#125;]&#125;,&#123;&quot;id&quot;:5,&quot;type&quot;:&quot;Sink: Print to Std. Out&quot;,&quot;pact&quot;:&quot;Data Sink&quot;,&quot;contents&quot;:&quot;Sink: Print to Std. Out&quot;,&quot;parallelism&quot;:1,&quot;predecessors&quot;:[&#123;&quot;id&quot;:4,&quot;ship_strategy&quot;:&quot;FORWARD&quot;,&quot;side&quot;:&quot;second&quot;&#125;]&#125;]&#125;</span><br><span class="line">--------------------------------------------------------------</span><br></pre></td></tr></table></figure><p>拷贝输出的 json 内容，粘贴到这个网站：<a href="http://flink.apache.org/visualizer/" target="_blank" rel="noopener">http://flink.apache.org/visualizer/</a></p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220400.jpg" alt=""></p><p>可以和实际运行的物理执行计划对比：</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220425.jpg" alt=""></p><h3 id="yarn-per-job"><a href="#yarn-per-job" class="headerlink" title="yarn per-job"></a>yarn per-job</h3><h4 id="单任务-attach-模式"><a href="#单任务-attach-模式" class="headerlink" title="单任务 attach 模式"></a>单任务 attach 模式</h4><p>默认是attach模式，即客户端会一直等待直到程序结束才会退出。</p><ul><li>通过 -m yarn-cluster 指定 yarn 模式</li><li>Yarn上显示名字为 Flink session cluster，这个 batch 的 wordcount 任务运行完会 FINISHED。</li><li>客户端能看到结果输出</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[admin@z17.sqa.zth /home/admin/flink/flink-1.7.2]</span><br><span class="line">$echo $HADOOP_CONF_DIR</span><br><span class="line">/etc/hadoop/conf/</span><br><span class="line"> </span><br><span class="line">[admin@z17.sqa.zth /home/admin/flink/flink-1.7.2]</span><br><span class="line">$./bin/flink run -m yarn-cluster ./examples/batch/WordCount.jar</span><br><span class="line"> </span><br><span class="line">2019-06-17 09:15:24,511 INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at z05c05217.sqa.zth.tbsite.net/11.163.188.29:8050</span><br><span class="line">2019-06-17 09:15:24,690 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar</span><br><span class="line">2019-06-17 09:15:24,690 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar</span><br><span class="line">2019-06-17 09:15:24,907 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Cluster specification: ClusterSpecification&#123;masterMemoryMB=1024, taskManagerMemoryMB=1024, numberTaskManagers=1, slotsPerTaskManager=4&#125;</span><br><span class="line">2019-06-17 09:15:25,430 WARN  org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory       - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.</span><br><span class="line">2019-06-17 09:15:25,438 WARN  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - The configuration directory (&apos;/Users/baoniu/Documents/work/tool/flink/flink-1.7.2/conf&apos;) contains both LOG4J and Logback configuration files. Please delete or rename one of them.</span><br><span class="line">2019-06-17 09:15:36,239 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Submitting application master application_1532332183347_0724</span><br><span class="line">2019-06-17 09:15:36,276 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl         - Submitted application application_1532332183347_0724</span><br><span class="line">2019-06-17 09:15:36,276 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Waiting for the cluster to be allocated</span><br><span class="line">2019-06-17 09:15:36,281 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Deploying cluster, current state ACCEPTED</span><br><span class="line">2019-06-17 09:15:40,426 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - YARN application has been deployed successfully.</span><br><span class="line">Starting execution of program</span><br><span class="line">Executing WordCount example with default input data set.</span><br><span class="line">Use --input to specify file input.</span><br><span class="line">Printing result to stdout. Use --output to specify output path.</span><br><span class="line">(a,5)</span><br><span class="line">(action,1)</span><br><span class="line">(after,1)</span><br><span class="line">(against,1)</span><br><span class="line">(all,2)</span><br><span class="line">... ...</span><br><span class="line">(would,2)</span><br><span class="line">(wrong,1)</span><br><span class="line">(you,1)</span><br><span class="line">Program execution finished</span><br><span class="line">Job with JobID 8bfe7568cb5c3254af30cbbd9cd5971e has finished.</span><br><span class="line">Job Runtime: 9371 ms</span><br><span class="line">Accumulator Results:</span><br><span class="line">- 2bed2c5506e9237fb85625416a1bc508 (java.util.ArrayList) [170 elements]</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220443.jpg" alt=""></p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220510.jpg" alt=""></p><p>如果我们以 attach 模式运行 streaming 的任务，客户端会一直等待不退出，可以运行以下的例子试验下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/flink run -m yarn-cluster ./examples/streaming/TopSpeedWindowing.jar</span><br></pre></td></tr></table></figure><h4 id="单任务-detached-模式"><a href="#单任务-detached-模式" class="headerlink" title="单任务 detached 模式"></a>单任务 detached 模式</h4><ul><li>由于是 detached 模式，客户端提交完任务就退出了</li><li>Yarn 上显示为 Flink per-job cluster</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$./bin/flink run -yd -m yarn-cluster ./examples/streaming/TopSpeedWindowing.jar</span><br><span class="line"> </span><br><span class="line">2019-06-18 09:21:59,247 INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at z05c05217.sqa.zth.tbsite.net/11.163.188.29:8050</span><br><span class="line">2019-06-18 09:21:59,428 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar</span><br><span class="line">2019-06-18 09:21:59,428 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar</span><br><span class="line">2019-06-18 09:21:59,940 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Cluster specification: ClusterSpecification&#123;masterMemoryMB=1024, taskManagerMemoryMB=1024, numberTaskManagers=1, slotsPerTaskManager=4&#125;</span><br><span class="line">2019-06-18 09:22:00,427 WARN  org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory       - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.</span><br><span class="line">2019-06-18 09:22:00,436 WARN  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - The configuration directory (&apos;/Users/baoniu/Documents/work/tool/flink/flink-1.7.2/conf&apos;) contains both LOG4J and Logback configuration files. Please delete or rename one of them.</span><br><span class="line">^@2019-06-18 09:22:12,113 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Submitting application master application_1532332183347_0729</span><br><span class="line">2019-06-18 09:22:12,151 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl         - Submitted application application_1532332183347_0729</span><br><span class="line">2019-06-18 09:22:12,151 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Waiting for the cluster to be allocated</span><br><span class="line">2019-06-18 09:22:12,155 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Deploying cluster, current state ACCEPTED</span><br><span class="line">2019-06-18 09:22:16,275 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - YARN application has been deployed successfully.</span><br><span class="line">2019-06-18 09:22:16,275 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - The Flink YARN client has been started in detached mode. In order to stop Flink on YARN, use the following command or a YARN web interface to stop it:</span><br><span class="line">yarn application -kill application_1532332183347_0729</span><br><span class="line">Please also note that the temporary files of the YARN session in the home directory will not be removed.</span><br><span class="line">Job has been submitted with JobID e61b9945c33c300906ad50a9a11f36df</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220545.jpg" alt=""></p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220559.jpg" alt=""></p><h3 id="yarn-session"><a href="#yarn-session" class="headerlink" title="yarn session"></a>yarn session</h3><h4 id="启动-session"><a href="#启动-session" class="headerlink" title="启动 session"></a>启动 session</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/yarn-session.sh -tm 2048 -s 3</span><br></pre></td></tr></table></figure><p>表示启动一个 yarn session 集群，每个TM的内存是2G，每个TM有3个slot。(注意：-n 参数不生效)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 ./bin/yarn-session.sh -tm 2048 -s 3</span><br><span class="line">2019-06-17 09:21:50,177 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.address, localhost</span><br><span class="line">2019-06-17 09:21:50,179 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.port, 6123</span><br><span class="line">2019-06-17 09:21:50,179 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.heap.size, 1024m</span><br><span class="line">2019-06-17 09:21:50,179 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.heap.size, 1024m</span><br><span class="line">2019-06-17 09:21:50,179 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.numberOfTaskSlots, 4</span><br><span class="line">2019-06-17 09:21:50,179 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: state.savepoints.dir, file:///tmp/savepoint</span><br><span class="line">2019-06-17 09:21:50,180 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: parallelism.default, 1</span><br><span class="line">2019-06-17 09:21:50,180 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: rest.port, 8081</span><br><span class="line">2019-06-17 09:21:50,644 WARN  org.apache.hadoop.util.NativeCodeLoader                       - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">2019-06-17 09:21:50,746 INFO  org.apache.flink.runtime.security.modules.HadoopModule        - Hadoop user set to baoniu (auth:SIMPLE)</span><br><span class="line">2019-06-17 09:21:50,848 INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at z05c05217.sqa.zth.tbsite.net/11.163.188.29:8050</span><br><span class="line">2019-06-17 09:21:51,148 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Cluster specification: ClusterSpecification&#123;masterMemoryMB=1024, taskManagerMemoryMB=2048, numberTaskManagers=1, slotsPerTaskManager=3&#125;</span><br><span class="line">2019-06-17 09:21:51,588 WARN  org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory       - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.</span><br><span class="line">2019-06-17 09:21:51,596 WARN  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - The configuration directory (&apos;/Users/baoniu/Documents/work/tool/flink/flink-1.7.2/conf&apos;) contains both LOG4J and Logback configuration files. Please delete or rename one of them.</span><br><span class="line">^@2019-06-17 09:22:03,304 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Submitting application master application_1532332183347_0726</span><br><span class="line">2019-06-17 09:22:03,336 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl         - Submitted application application_1532332183347_0726</span><br><span class="line">2019-06-17 09:22:03,336 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Waiting for the cluster to be allocated</span><br><span class="line">2019-06-17 09:22:03,340 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Deploying cluster, current state ACCEPTED</span><br><span class="line">2019-06-17 09:22:07,722 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - YARN application has been deployed successfully.</span><br><span class="line">2019-06-17 09:22:08,050 INFO  org.apache.flink.runtime.rest.RestClient                      - Rest client endpoint started.</span><br><span class="line">Flink JobManager is now running on z07.sqa.net:37109 with leader id 00000000-0000-0000-0000-000000000000.</span><br><span class="line">JobManager Web Interface: http://z07.sqa.net:37109</span><br></pre></td></tr></table></figure><p>客户端默认是attach模式，不会退出</p><ul><li>可以 ctrl + c 退出，然后再通过 ./bin/yarn-session.sh -id application_1532332183347_0726 连上来。</li><li>或者启动的时候用 -d 则为detached模式<br>Yarn上显示为 Flink session cluster</li></ul><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220618.jpg" alt=""></p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220648.jpg" alt=""></p><ul><li>在本机的临时目录（有些机器是 /tmp 目录）下会生成一个文件</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 cat /var/folders/2b/r6d49pcs23z43b8fqsyz885c0000gn/T/.yarn-properties-baoniu</span><br><span class="line">#Generated YARN properties file</span><br><span class="line">#Mon Jun 17 09:22:08 CST 2019</span><br><span class="line">parallelism=3</span><br><span class="line">dynamicPropertiesString=</span><br><span class="line">applicationID=application_1532332183347_0726</span><br></pre></td></tr></table></figure><h4 id="提交任务"><a href="#提交任务" class="headerlink" title="提交任务"></a>提交任务</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/flink run ./examples/batch/WordCount.jar</span><br></pre></td></tr></table></figure><p>将会根据 /tmp/.yarn-properties-admin 文件内容提交到了刚启动的 session。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 ./bin/flink run ./examples/batch/WordCount.jar</span><br><span class="line">2019-06-17 09:26:42,767 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - Found Yarn properties file under /var/folders/2b/r6d49pcs23z43b8fqsyz885c0000gn/T/.yarn-properties-baoniu.</span><br><span class="line">2019-06-17 09:26:42,767 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - Found Yarn properties file under /var/folders/2b/r6d49pcs23z43b8fqsyz885c0000gn/T/.yarn-properties-baoniu.</span><br><span class="line">2019-06-17 09:26:43,058 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - YARN properties set default parallelism to 3</span><br><span class="line">2019-06-17 09:26:43,058 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - YARN properties set default parallelism to 3</span><br><span class="line">YARN properties set default parallelism to 3</span><br><span class="line">2019-06-17 09:26:43,097 INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at z05c05217.sqa.zth.tbsite.net/11.163.188.29:8050</span><br><span class="line">2019-06-17 09:26:43,229 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar</span><br><span class="line">2019-06-17 09:26:43,229 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar</span><br><span class="line">2019-06-17 09:26:43,327 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Found application JobManager host name &apos;z05c07216.sqa.zth.tbsite.net&apos; and port &apos;37109&apos; from supplied application id &apos;application_1532332183347_0726&apos;</span><br><span class="line">Starting execution of program</span><br><span class="line">Executing WordCount example with default input data set.</span><br><span class="line">Use --input to specify file input.</span><br><span class="line">Printing result to stdout. Use --output to specify output path.</span><br><span class="line">^@(a,5)</span><br><span class="line">(action,1)</span><br><span class="line">(after,1)</span><br><span class="line">(against,1)</span><br><span class="line">(all,2)</span><br><span class="line">(and,12)</span><br><span class="line">... ...</span><br><span class="line">(wrong,1)</span><br><span class="line">(you,1)</span><br><span class="line">Program execution finished</span><br><span class="line">Job with JobID ad9b0f1feed6d0bf6ba4e0f18b1e65ef has finished.</span><br><span class="line">Job Runtime: 9152 ms</span><br><span class="line">Accumulator Results:</span><br><span class="line">- fd07c75d503d0d9a99e4f27dd153114c (java.util.ArrayList) [170 elements]</span><br></pre></td></tr></table></figure><p>运行结束后TM的资源会释放。</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220705.jpg" alt=""></p><h4 id="提交到指定的session"><a href="#提交到指定的session" class="headerlink" title="提交到指定的session"></a>提交到指定的session</h4><p>通过 -yid 参数来提交到指定的session。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$./bin/flink run -d -p 30 -m yarn-cluster -yid application_1532332183347_0708 ./examples/streaming/TopSpeedWindowing.jar</span><br><span class="line"> </span><br><span class="line">2019-03-24 12:36:33,668 INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at z05c05217.sqa.zth.tbsite.net/11.163.188.29:8050</span><br><span class="line">2019-03-24 12:36:33,773 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar</span><br><span class="line">2019-03-24 12:36:33,773 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar</span><br><span class="line">2019-03-24 12:36:33,837 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Found application JobManager host name &apos;z05c05218.sqa.zth.tbsite.net&apos; and port &apos;60783&apos; from supplied application id &apos;application_1532332183347_0708&apos;</span><br><span class="line">Starting execution of program</span><br><span class="line">Executing TopSpeedWindowing example with default input data set.</span><br><span class="line">Use --input to specify file input.</span><br><span class="line">Printing result to stdout. Use --output to specify output path.</span><br><span class="line">Job has been submitted with JobID 58d5049ebbf28d515159f2f88563f5fd</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220735.jpg" alt=""></p><p>注：<a href="https://github.com/apache/flink/commits/blink" target="_blank" rel="noopener">blink版本</a> 的 session 与 <a href="https://zhoukaibo.com/tags/flink/">flink</a> 的 session 的区别：</p><ul><li>flink 的 session -n 参数不生效，而且不会提前启动TM</li><li>blink 的 session 可以通过 -n 指定启动多少个TM，而且TM会提前起来</li></ul><h2 id="Scala-Shell"><a href="#Scala-Shell" class="headerlink" title="Scala Shell"></a>Scala Shell</h2><p>官方文档：<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/scala_shell.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/scala_shell.html</a></p><h3 id="Deploy"><a href="#Deploy" class="headerlink" title="Deploy"></a>Deploy</h3><h4 id="local"><a href="#local" class="headerlink" title="local"></a>local</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$bin/start-scala-shell.sh local</span><br><span class="line">Starting Flink Shell:</span><br><span class="line">Starting local Flink cluster (host: localhost, port: 8081).</span><br><span class="line">Connecting to Flink cluster (host: localhost, port: 8081).</span><br><span class="line">... ...</span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure><p>任务运行说明：</p><ul><li>Batch 任务内置了 benv 变量，通过 print() 将结果输出到控制台</li><li>Streaming 任务内置了 senv 变量，通过 senv.execute(“job name”) 来提交任务，且Datastream的输出只有在 local 模式下打印到控制台。</li></ul><h4 id="remote"><a href="#remote" class="headerlink" title="remote"></a>remote</h4><p>先启动一个 yarn session cluster</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$./bin/yarn-session.sh  -tm 2048 -s 3</span><br><span class="line"></span><br><span class="line">2019-03-25 09:52:16,341 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.address, localhost</span><br><span class="line">2019-03-25 09:52:16,342 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.port, 6123</span><br><span class="line">2019-03-25 09:52:16,342 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.heap.size, 1024m</span><br><span class="line">2019-03-25 09:52:16,343 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.heap.size, 1024m</span><br><span class="line">2019-03-25 09:52:16,343 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.numberOfTaskSlots, 4</span><br><span class="line">2019-03-25 09:52:16,343 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: parallelism.default, 1</span><br><span class="line">2019-03-25 09:52:16,343 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: state.savepoints.dir, file:///tmp/savepoint</span><br><span class="line">2019-03-25 09:52:16,343 INFO  org.apache.flink.configuration.GlobalConfiguration            </span><br><span class="line">… ...</span><br><span class="line">Flink JobManager is now running on z054.sqa.net:28665 with leader id 00000000-0000-0000-0000-000000000000.</span><br><span class="line">JobManager Web Interface: http://z054.sqa.net:28665</span><br></pre></td></tr></table></figure><p>启动 scala shell，连到 jm</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$bin/start-scala-shell.sh remote z054.sqa.net 28665</span><br><span class="line">Starting Flink Shell:</span><br><span class="line">Connecting to Flink cluster (host: z054.sqa.net, port: 28665).</span><br><span class="line">... ...</span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure><h4 id="yarn"><a href="#yarn" class="headerlink" title="yarn"></a>yarn</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">$./bin/start-scala-shell.sh yarn -n 2 -jm 1024 -s 2 -tm 1024 -nm flink-yarn</span><br><span class="line"></span><br><span class="line">Starting Flink Shell:</span><br><span class="line">2019-03-25 09:47:44,695 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.address, localhost</span><br><span class="line">2019-03-25 09:47:44,697 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.rpc.port, 6123</span><br><span class="line">2019-03-25 09:47:44,697 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: jobmanager.heap.size, 1024m</span><br><span class="line">2019-03-25 09:47:44,697 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.heap.size, 1024m</span><br><span class="line">2019-03-25 09:47:44,697 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: taskmanager.numberOfTaskSlots, 4</span><br><span class="line">2019-03-25 09:47:44,698 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: parallelism.default, 1</span><br><span class="line">2019-03-25 09:47:44,698 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: state.savepoints.dir, file:///tmp/savepoint</span><br><span class="line">2019-03-25 09:47:44,698 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: rest.port, 8081</span><br><span class="line">2019-03-25 09:47:44,717 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - Found Yarn properties file under /tmp/.yarn-properties-admin.</span><br><span class="line">2019-03-25 09:47:45,041 INFO  org.apache.hadoop.yarn.client.RMProxy                         - Connecting to ResourceManager at z05c05217.sqa.zth.tbsite.net/11.163.188.29:8050</span><br><span class="line">2019-03-25 09:47:45,098 WARN  org.apache.hadoop.util.NativeCodeLoader                       - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">2019-03-25 09:47:45,266 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar</span><br><span class="line">2019-03-25 09:47:45,275 INFO  org.apache.flink.yarn.cli.FlinkYarnSessionCli                 - The argument yn is deprecated in will be ignored.</span><br><span class="line">2019-03-25 09:47:45,357 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Cluster specification: ClusterSpecification&#123;masterMemoryMB=1024, taskManagerMemoryMB=1024, numberTaskManagers=2, slotsPerTaskManager=2&#125;</span><br><span class="line">2019-03-25 09:47:45,711 WARN  org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory       - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.</span><br><span class="line">2019-03-25 09:47:45,718 WARN  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - The configuration directory (&apos;/home/admin/flink/flink-1.7.2/conf&apos;) contains both LOG4J and Logback configuration files. Please delete or rename one of them.</span><br><span class="line">2019-03-25 09:47:46,514 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Submitting application master application_1532332183347_0710</span><br><span class="line">2019-03-25 09:47:46,534 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl         - Submitted application application_1532332183347_0710</span><br><span class="line">2019-03-25 09:47:46,534 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Waiting for the cluster to be allocated</span><br><span class="line">2019-03-25 09:47:46,535 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Deploying cluster, current state ACCEPTED</span><br><span class="line">2019-03-25 09:47:51,051 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - YARN application has been deployed successfully.</span><br><span class="line">2019-03-25 09:47:51,222 INFO  org.apache.flink.runtime.rest.RestClient                      - Rest client endpoint started.</span><br><span class="line"></span><br><span class="line">Connecting to Flink cluster (host: 10.10.10.10, port: 56942).</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220753.jpg" alt=""></p><p>按 CTRL + C 退出 shell 后，这个 flink cluster 还会继续运行，不会退出。</p><h3 id="Execute"><a href="#Execute" class="headerlink" title="Execute"></a>Execute</h3><h4 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/stop-cluster.sh</span><br><span class="line">No taskexecutor daemon to stop on host zkb-MBP.local.</span><br><span class="line">No standalonesession daemon to stop on host zkb-MBP.local.</span><br><span class="line">➜  flink-1.7.2 bin/start-scala-shell.sh local</span><br><span class="line">Starting Flink Shell:</span><br><span class="line">Starting local Flink cluster (host: localhost, port: 8081).</span><br><span class="line">Connecting to Flink cluster (host: localhost, port: 8081).</span><br><span class="line"></span><br><span class="line">scala&gt; val text = benv.fromElements(&quot;To be, or not to be,--that is the question:--&quot;)</span><br><span class="line">text: org.apache.flink.api.scala.DataSet[String] = org.apache.flink.api.scala.DataSet@5b407336</span><br><span class="line"></span><br><span class="line">scala&gt; val counts = text.flatMap &#123; _.toLowerCase.split(&quot;\\W+&quot;) &#125;.map &#123; (_, 1) &#125;.groupBy(0).sum(1)</span><br><span class="line">counts: org.apache.flink.api.scala.AggregateDataSet[(String, Int)] = org.apache.flink.api.scala.AggregateDataSet@6ee34fe4</span><br><span class="line"></span><br><span class="line">scala&gt; counts.print()</span><br><span class="line">(be,2)</span><br><span class="line">(is,1)</span><br><span class="line">(not,1)</span><br><span class="line">(or,1)</span><br><span class="line">(question,1)</span><br><span class="line">(that,1)</span><br><span class="line">(the,1)</span><br><span class="line">(to,2)</span><br></pre></td></tr></table></figure><p>对 DataSet 任务来说，print() 会触发任务的执行。</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220816.jpg" alt=""></p><p>也可以将结果输出到文件（先删除 /tmp/ou1，不然会报错同名文件已经存在），继续执行以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; counts.writeAsText(&quot;/tmp/out1&quot;)</span><br><span class="line">res1: org.apache.flink.api.java.operators.DataSink[(String, Int)] = DataSink &apos;&lt;unnamed&gt;&apos; (TextOutputFormat (/tmp/out1) - UTF-8)</span><br><span class="line"></span><br><span class="line">scala&gt; benv.execute(&quot;batch test&quot;)</span><br><span class="line">res2: org.apache.flink.api.common.JobExecutionResult = org.apache.flink.api.common.JobExecutionResult@737652a9</span><br></pre></td></tr></table></figure><p>查看 /tmp/out1 文件就能看到输出结果。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 cat /tmp/out1</span><br><span class="line">(be,2)</span><br><span class="line">(is,1)</span><br><span class="line">(not,1)</span><br><span class="line">(or,1)</span><br><span class="line">(question,1)</span><br><span class="line">(that,1)</span><br><span class="line">(the,1)</span><br><span class="line">(to,2)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220957.jpg" alt=""></p><h4 id="DataSteam"><a href="#DataSteam" class="headerlink" title="DataSteam"></a>DataSteam</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val textStreaming = senv.fromElements(&quot;To be, or not to be,--that is the question:--&quot;)</span><br><span class="line">textStreaming: org.apache.flink.streaming.api.scala.DataStream[String] = org.apache.flink.streaming.api.scala.DataStream@4970b93d</span><br><span class="line"></span><br><span class="line">scala&gt; val countsStreaming = textStreaming.flatMap &#123; _.toLowerCase.split(&quot;\\W+&quot;) &#125;.map &#123; (_, 1) &#125;.keyBy(0).sum(1)</span><br><span class="line">countsStreaming: org.apache.flink.streaming.api.scala.DataStream[(String, Int)] = org.apache.flink.streaming.api.scala.DataStream@6a478680</span><br><span class="line"></span><br><span class="line">scala&gt; countsStreaming.print()</span><br><span class="line">res3: org.apache.flink.streaming.api.datastream.DataStreamSink[(String, Int)] = org.apache.flink.streaming.api.datastream.DataStreamSink@42bfc11f</span><br><span class="line"></span><br><span class="line">scala&gt; senv.execute(&quot;Streaming Wordcount&quot;)</span><br><span class="line">(to,1)</span><br><span class="line">(be,1)</span><br><span class="line">(or,1)</span><br><span class="line">(not,1)</span><br><span class="line">(to,2)</span><br><span class="line">(be,2)</span><br><span class="line">(that,1)</span><br><span class="line">(is,1)</span><br><span class="line">(the,1)</span><br><span class="line">(question,1)</span><br><span class="line">res4: org.apache.flink.api.common.JobExecutionResult = org.apache.flink.api.common.JobExecutionResult@1878815a</span><br></pre></td></tr></table></figure><p>对 DataStream 任务，print() 并不会触发任务的执行，需要显式调用 execute(“job name”) 才会执行任务。</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015220839.jpg" alt=""></p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221018.jpg" alt=""></p><h4 id="TableAPI"><a href="#TableAPI" class="headerlink" title="TableAPI"></a>TableAPI</h4><p>在 Blink 开源版本里面，支持了 TableAPI 方式提交任务（可以用 btenv.sqlQuery 提交sql查询），社区版本 1.8 会支持 TableAPI: <a href="https://issues.apache.org/jira/browse/FLINK-9555" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/FLINK-9555</a></p><h2 id="SQL-Client-Beta"><a href="#SQL-Client-Beta" class="headerlink" title="SQL Client Beta"></a>SQL Client Beta</h2><p>SQL Client 目前还只是测试版，处于开发阶段，只能用于SQL的原型验证，不推荐在生产环境使用。</p><h3 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 bin/start-cluster.sh</span><br><span class="line">Starting cluster.</span><br><span class="line">Starting standalonesession daemon on host zkb-MBP.local.</span><br><span class="line">Starting taskexecutor daemon on host zkb-MBP.local.</span><br><span class="line"></span><br><span class="line">➜  flink-1.7.2 ./bin/sql-client.sh embedded</span><br><span class="line">No default environment specified.</span><br><span class="line">Searching for &apos;/Users/baoniu/Documents/work/tool/flink/flink-1.7.2/conf/sql-client-defaults.yaml&apos;...found.</span><br><span class="line">Reading default environment from: file:/Users/baoniu/Documents/work/tool/flink/flink-1.7.2/conf/sql-client-defaults.yaml</span><br><span class="line">No session environment specified.</span><br><span class="line">Validating current environment...done.</span><br><span class="line">… …</span><br><span class="line"></span><br><span class="line">Flink SQL&gt; help;</span><br><span class="line">The following commands are available:</span><br><span class="line"></span><br><span class="line">QUITQuits the SQL CLI client.</span><br><span class="line">CLEARClears the current terminal.</span><br><span class="line">HELPPrints the available commands.</span><br><span class="line">SHOW TABLESShows all registered tables.</span><br><span class="line">SHOW FUNCTIONSShows all registered user-defined functions.</span><br><span class="line">DESCRIBEDescribes the schema of a table with the given name.</span><br><span class="line">EXPLAINDescribes the execution plan of a query or table with the given name.</span><br><span class="line">SELECTExecutes a SQL SELECT query on the Flink cluster.</span><br><span class="line">INSERT INTOInserts the results of a SQL SELECT query into a declared table sink.</span><br><span class="line">CREATE VIEWCreates a virtual table from a SQL query. Syntax: &apos;CREATE VIEW &lt;name&gt; AS &lt;query&gt;;&apos;</span><br><span class="line">DROP VIEWDeletes a previously created virtual table. Syntax: &apos;DROP VIEW &lt;name&gt;;&apos;</span><br><span class="line">SOURCEReads a SQL SELECT query from a file and executes it on the Flink cluster.</span><br><span class="line">SETSets a session configuration property. Syntax: &apos;SET &lt;key&gt;=&lt;value&gt;;&apos;. Use &apos;SET;&apos; for listing all properties.</span><br><span class="line">RESETResets all session configuration properties.</span><br><span class="line"></span><br><span class="line">Hint: Make sure that a statement ends with &apos;;&apos; for finalizing (multi-line) statements.</span><br></pre></td></tr></table></figure><h4 id="select-查询"><a href="#select-查询" class="headerlink" title="select 查询"></a>select 查询</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Flink SQL&gt; SELECT &apos;Hello World&apos;;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221031.jpg" alt=""></p><p>按 ”Q” 退出这个界面<br>打开 <a href="http://127.0.0.1:8081" target="_blank" rel="noopener">http://127.0.0.1:8081</a> 能看到这条 select 语句产生的查询任务已经结束了。这个查询采用的是读取固定数据集的 Custom Source，输出用的是 Stream Collect Sink，且只输出一条结果。</p><p>注意：如果本机的临时目录存在类似 .yarn-properties-baoniu 的文件，任务会提交到 yarn 上。</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221045.jpg" alt=""></p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221059.jpg" alt=""></p><h4 id="explain"><a href="#explain" class="headerlink" title="explain"></a>explain</h4><p>explain 命令可以查看 SQL 的执行计划。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Flink SQL&gt; explain SELECT name, COUNT(*) AS cnt FROM (VALUES (&apos;Bob&apos;), (&apos;Alice&apos;), (&apos;Greg&apos;), (&apos;Bob&apos;)) AS NameTable(name) GROUP BY name;</span><br><span class="line"></span><br><span class="line">== Abstract Syntax Tree ==        // 抽象语法树</span><br><span class="line">LogicalAggregate(group=[&#123;0&#125;], cnt=[COUNT()])</span><br><span class="line">  LogicalValues(tuples=[[&#123; _UTF-16LE&apos;Bob  &apos; &#125;, &#123; _UTF-16LE&apos;Alice&apos; &#125;, &#123; _UTF-16LE&apos;Greg &apos; &#125;, &#123; _UTF-16LE&apos;Bob  &apos; &#125;]])</span><br><span class="line"></span><br><span class="line">== Optimized Logical Plan ==     // 优化后的逻辑执行计划</span><br><span class="line">DataStreamGroupAggregate(groupBy=[name], select=[name, COUNT(*) AS cnt])</span><br><span class="line">  DataStreamValues(tuples=[[&#123; _UTF-16LE&apos;Bob  &apos; &#125;, &#123; _UTF-16LE&apos;Alice&apos; &#125;, &#123; _UTF-16LE&apos;Greg &apos; &#125;, &#123; _UTF-16LE&apos;Bob  &apos; &#125;]])</span><br><span class="line"></span><br><span class="line">== Physical Execution Plan ==   // 物理执行计划</span><br><span class="line">Stage 3 : Data Source</span><br><span class="line">content : collect elements with CollectionInputFormat</span><br><span class="line"></span><br><span class="line">Stage 5 : Operator</span><br><span class="line">content : groupBy: (name), select: (name, COUNT(*) AS cnt)</span><br><span class="line">ship_strategy : HASH</span><br></pre></td></tr></table></figure><h3 id="结果展示"><a href="#结果展示" class="headerlink" title="结果展示"></a>结果展示</h3><p>SQL Client 支持两种模式来维护并展示查询结果：</p><ul><li><p>table mode: 在内存中物化查询结果，并以分页 table 形式展示。用户可以通过以下命令启用 table mode：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SET execution.result-mode=table</span><br></pre></td></tr></table></figure></li><li><p>changlog mode: 不会物化查询结果，而是直接对 continuous query 产生的添加和撤回（retractions）结果进行展示。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SET execution.result-mode=changelog</span><br></pre></td></tr></table></figure></li></ul><p>接下来通过实际的例子进行演示。</p><h4 id="table-mode"><a href="#table-mode" class="headerlink" title="table mode"></a>table mode</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Flink SQL&gt; SET execution.result-mode=table;</span><br><span class="line">[INFO] Session property has been set.</span><br><span class="line"></span><br><span class="line">Flink SQL&gt; SELECT name, COUNT(*) AS cnt FROM (VALUES (&apos;Bob&apos;), (&apos;Alice&apos;), (&apos;Greg&apos;), (&apos;Bob&apos;)) AS NameTable(name) GROUP BY name;</span><br></pre></td></tr></table></figure><p>运行结果如下图所示：</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221114.jpg" alt=""></p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221131.jpg" alt=""></p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221148.jpg" alt=""></p><h4 id="changlog-mode"><a href="#changlog-mode" class="headerlink" title="changlog mode"></a>changlog mode</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Flink SQL&gt; SET execution.result-mode=changelog;</span><br><span class="line">[INFO] Session property has been set.</span><br><span class="line"></span><br><span class="line">Flink SQL&gt; SELECT name, COUNT(*) AS cnt FROM (VALUES (&apos;Bob&apos;), (&apos;Alice&apos;), (&apos;Greg&apos;), (&apos;Bob&apos;)) AS NameTable(name) GROUP BY name;</span><br></pre></td></tr></table></figure><p>运行结果如下图所示：</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221204.jpg" alt=""></p><p>其中 ‘-’ 代表的就是撤回消息。<br><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221216.jpg" alt=""></p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221231.jpg" alt=""></p><h3 id="Environment-Files"><a href="#Environment-Files" class="headerlink" title="Environment Files"></a>Environment Files</h3><p>目前的 SQL Client 还不支持 DDL 语句，只能通过 yaml 文件的方式来定义 SQL 查询需要的表，udf 和运行参数等信息。</p><p>首先，准备 env.yaml 和 input.csv 两个文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 cat /tmp/env.yaml</span><br><span class="line">tables:</span><br><span class="line">  - name: MyTableSource</span><br><span class="line">    type: source-table</span><br><span class="line">    update-mode: append</span><br><span class="line">    connector:</span><br><span class="line">      type: filesystem</span><br><span class="line">      path: &quot;/tmp/input.csv&quot;</span><br><span class="line">    format:</span><br><span class="line">      type: csv</span><br><span class="line">      fields:</span><br><span class="line">        - name: MyField1</span><br><span class="line">          type: INT</span><br><span class="line">        - name: MyField2</span><br><span class="line">          type: VARCHAR</span><br><span class="line">      line-delimiter: &quot;\n&quot;</span><br><span class="line">      comment-prefix: &quot;#&quot;</span><br><span class="line">    schema:</span><br><span class="line">      - name: MyField1</span><br><span class="line">        type: INT</span><br><span class="line">      - name: MyField2</span><br><span class="line">        type: VARCHAR</span><br><span class="line">  - name: MyCustomView</span><br><span class="line">    type: view</span><br><span class="line">    query: &quot;SELECT MyField2 FROM MyTableSource&quot;</span><br><span class="line">  - name: MyTableSink</span><br><span class="line">    type: sink-table</span><br><span class="line">    update-mode: append</span><br><span class="line">    connector:</span><br><span class="line">      type: filesystem</span><br><span class="line">      path: &quot;/tmp/output.csv&quot;</span><br><span class="line">    format:</span><br><span class="line">      type: csv</span><br><span class="line">      fields:</span><br><span class="line">        - name: MyField1</span><br><span class="line">          type: INT</span><br><span class="line">        - name: MyField2</span><br><span class="line">          type: VARCHAR</span><br><span class="line">    schema:</span><br><span class="line">      - name: MyField1</span><br><span class="line">        type: INT</span><br><span class="line">      - name: MyField2</span><br><span class="line">        type: VARCHAR</span><br><span class="line">        </span><br><span class="line"># Execution properties allow for changing the behavior of a table program.</span><br><span class="line"></span><br><span class="line">execution:</span><br><span class="line">  type: streaming                   # required: execution mode either &apos;batch&apos; or &apos;streaming&apos;</span><br><span class="line">  result-mode: table                # required: either &apos;table&apos; or &apos;changelog&apos;</span><br><span class="line">  max-table-result-rows: 1000000    # optional: maximum number of maintained rows in</span><br><span class="line">                                    #   &apos;table&apos; mode (1000000 by default, smaller 1 means unlimited)</span><br><span class="line">  time-characteristic: event-time   # optional: &apos;processing-time&apos; or &apos;event-time&apos; (default)</span><br><span class="line">  parallelism: 1                    # optional: Flink&apos;s parallelism (1 by default)</span><br><span class="line">  periodic-watermarks-interval: 200 # optional: interval for periodic watermarks (200 ms by default)</span><br><span class="line">  max-parallelism: 16               # optional: Flink&apos;s maximum parallelism (128 by default)</span><br><span class="line">  min-idle-state-retention: 0       # optional: table program&apos;s minimum idle state time</span><br><span class="line">  max-idle-state-retention: 0       # optional: table program&apos;s maximum idle state time</span><br><span class="line">  restart-strategy:                 # optional: restart strategy</span><br><span class="line">    type: fallback                  #   &quot;fallback&quot; to global restart strategy by default</span><br><span class="line"></span><br><span class="line"># Deployment properties allow for describing the cluster to which table programs are submitted to.</span><br><span class="line"></span><br><span class="line">deployment:</span><br><span class="line">  response-timeout: 5000</span><br><span class="line"></span><br><span class="line">➜  flink-1.7.2 cat /tmp/input.csv</span><br><span class="line">1,hello</span><br><span class="line">2,world</span><br><span class="line">3,hello world</span><br><span class="line">1,ok</span><br><span class="line">3,bye bye</span><br><span class="line">4,yes</span><br></pre></td></tr></table></figure><p>启动 SQL Client：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 ./bin/sql-client.sh embedded -e /tmp/env.yaml</span><br><span class="line">No default environment specified.</span><br><span class="line">Searching for &apos;/Users/baoniu/Documents/work/tool/flink/flink-1.7.2/conf/sql-client-defaults.yaml&apos;...found.</span><br><span class="line">Reading default environment from: file:/Users/baoniu/Documents/work/tool/flink/flink-1.7.2/conf/sql-client-defaults.yaml</span><br><span class="line">Reading session environment from: file:/tmp/env.yaml</span><br><span class="line">Validating current environment...done.</span><br><span class="line"></span><br><span class="line">Flink SQL&gt; show tables;</span><br><span class="line">MyCustomView</span><br><span class="line">MyTableSink</span><br><span class="line">MyTableSource</span><br><span class="line"></span><br><span class="line">Flink SQL&gt; describe MyTableSource;</span><br><span class="line">root</span><br><span class="line"> |-- MyField1: Integer</span><br><span class="line"> |-- MyField2: String</span><br><span class="line"></span><br><span class="line">Flink SQL&gt; describe MyCustomView;</span><br><span class="line">root</span><br><span class="line"> |-- MyField2: String</span><br><span class="line"></span><br><span class="line">Flink SQL&gt; create view MyView1 as select MyField1 from MyTableSource;</span><br><span class="line">[INFO] View has been created.</span><br><span class="line"></span><br><span class="line">Flink SQL&gt; show tables;</span><br><span class="line">MyCustomView</span><br><span class="line">MyTableSource</span><br><span class="line">MyView1</span><br><span class="line"></span><br><span class="line">Flink SQL&gt; describe MyView1;</span><br><span class="line">root</span><br><span class="line"> |-- MyField1: Integer</span><br><span class="line"></span><br><span class="line">Flink SQL&gt; select * from MyTableSource;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221246.jpg" alt=""></p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221256.jpg" alt=""></p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221306.jpg" alt=""></p><p>使用 insert into 写入结果表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Flink SQL&gt; insert into MyTableSink select * from MyTableSource;</span><br><span class="line">[INFO] Submitting SQL update statement to the cluster...</span><br><span class="line">[INFO] Table update statement has been successfully submitted to the cluster:</span><br><span class="line">Cluster ID: StandaloneClusterId</span><br><span class="line">Job ID: 3fac2be1fd891e3e07595c684bb7b7a0</span><br><span class="line">Web interface: http://localhost:8081</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221321.jpg" alt=""></p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221339.jpg" alt=""></p><p>查询生成的结果数据文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 cat /tmp/output.csv</span><br><span class="line">1,hello</span><br><span class="line">2,world</span><br><span class="line">3,hello world</span><br><span class="line">1,ok</span><br><span class="line">3,bye bye</span><br><span class="line">4,yes</span><br></pre></td></tr></table></figure><p>也可以在 environment 文件里面定义udf，在 SQL Client 里面通过 『SHOW FUNCTIONS』查询和使用，这里就不再说明了。</p><p>SQL Client 功能社区还在开发中，详见 <a href="https://cwiki.apache.org/confluence/display/FLINK/FLIP-24+-+SQL+Client" target="_blank" rel="noopener">FLIP-24</a>。</p><h2 id="Restful-API"><a href="#Restful-API" class="headerlink" title="Restful API"></a>Restful API</h2><p>接下来我们演示如何通过 Rest API 来提交jar包和执行任务。</p><p>更详细的操作请参考 flink 的 Restful API 文档：<a href="https://ci.apache.org/projects/flink/flink-docs-stable/monitoring/rest_api.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-stable/monitoring/rest_api.html</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">➜  flink-1.7.2 curl http://127.0.0.1:8081/overview</span><br><span class="line">&#123;&quot;taskmanagers&quot;:1,&quot;slots-total&quot;:4,&quot;slots-available&quot;:0,&quot;jobs-running&quot;:3,&quot;jobs-finished&quot;:0,&quot;jobs-cancelled&quot;:0,&quot;jobs-failed&quot;:0,&quot;flink-version&quot;:&quot;1.7.2&quot;,&quot;flink-commit&quot;:&quot;ceba8af&quot;&#125;%</span><br><span class="line"></span><br><span class="line">➜  flink-1.7.2 curl -X POST -H &quot;Expect:&quot; -F &quot;jarfile=@/Users/baoniu/Documents/work/tool/flink/flink-1.7.2/examples/streaming/TopSpeedWindowing.jar&quot; http://127.0.0.1:8081/jars/upload</span><br><span class="line">&#123;&quot;filename&quot;:&quot;/var/folders/2b/r6d49pcs23z43b8fqsyz885c0000gn/T/flink-web-124c4895-cf08-4eec-8e15-8263d347efc2/flink-web-upload/6077eca7-6db0-4570-a4d0-4c3e05a5dc59_TopSpeedWindowing.jar&quot;,&quot;status&quot;:&quot;success&quot;&#125;%       </span><br><span class="line">                                                                                                                                                                                                   ➜  flink-1.7.2 curl http://127.0.0.1:8081/jars</span><br><span class="line">&#123;&quot;address&quot;:&quot;http://localhost:8081&quot;,&quot;files&quot;:[&#123;&quot;id&quot;:&quot;6077eca7-6db0-4570-a4d0-4c3e05a5dc59_TopSpeedWindowing.jar&quot;,&quot;name&quot;:&quot;TopSpeedWindowing.jar&quot;,&quot;uploaded&quot;:1553743438000,&quot;entry&quot;:[&#123;&quot;name&quot;:&quot;org.apache.flink.streaming.examples.windowing.TopSpeedWindowing&quot;,&quot;description&quot;:null&#125;]&#125;]&#125;%</span><br><span class="line">                                                                                                                                         </span><br><span class="line">➜  flink-1.7.2 curl http://127.0.0.1:8081/jars/6077eca7-6db0-4570-a4d0-4c3e05a5dc59_TopSpeedWindowing.jar/plan</span><br><span class="line">&#123;&quot;plan&quot;:&#123;&quot;jid&quot;:&quot;41029eb3feb9132619e454ec9b2a89fb&quot;,&quot;name&quot;:&quot;CarTopSpeedWindowingExample&quot;,&quot;nodes&quot;:[&#123;&quot;id&quot;:&quot;90bea66de1c231edf33913ecd54406c1&quot;,&quot;parallelism&quot;:1,&quot;operator&quot;:&quot;&quot;,&quot;operator_strategy&quot;:&quot;&quot;,&quot;description&quot;:&quot;Window(GlobalWindows(), DeltaTrigger, TimeEvictor, ComparableAggregator, PassThroughWindowFunction) -&amp;gt; Sink: Print to Std. Out&quot;,&quot;inputs&quot;:[&#123;&quot;num&quot;:0,&quot;id&quot;:&quot;cbc357ccb763df2852fee8c4fc7d55f2&quot;,&quot;ship_strategy&quot;:&quot;HASH&quot;,&quot;exchange&quot;:&quot;pipelined_bounded&quot;&#125;],&quot;optimizer_properties&quot;:&#123;&#125;&#125;,&#123;&quot;id&quot;:&quot;cbc357ccb763df2852fee8c4fc7d55f2&quot;,&quot;parallelism&quot;:1,&quot;operator&quot;:&quot;&quot;,&quot;operator_strategy&quot;:&quot;&quot;,&quot;description&quot;:&quot;Source: Custom Source -&amp;gt; Timestamps/Watermarks&quot;,&quot;optimizer_properties&quot;:&#123;&#125;&#125;]&#125;&#125;%                                                                                                                                                    ➜  flink-1.7.2 curl -X POST http://127.0.0.1:8081/jars/6077eca7-6db0-4570-a4d0-4c3e05a5dc59_TopSpeedWindowing.jar/run</span><br><span class="line">&#123;&quot;jobid&quot;:&quot;04d80a24b076523d3dc5fbaa0ad5e1ad&quot;&#125;%</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221355.jpg" alt=""></p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221406.jpg" alt=""></p><p>Restful API 还提供了很多监控和 Metrics 相关的功能，对于任务提交的操作也支持的比较全面。</p><h2 id="Web"><a href="#Web" class="headerlink" title="Web"></a>Web</h2><p>在 <a href="https://zhoukaibo.com/tags/flink/">Flink</a> Dashboard 页面左侧可以看到有个『Submit new Job』的地方，用户可以上传 jar 包和显示执行计划和提交任务。Web 提交功能主要用于新手入门和演示用。</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20191015221421.jpg" alt=""></p><h1 id="结束"><a href="#结束" class="headerlink" title="结束"></a>结束</h1><p>本期的课程到这里就结束了，我们主要讲解了 Flink 的 5 种任务提交的方式。熟练掌握各种任务提交方式，有利于提高我们日常的开发和运维效率。</p>]]></content:encoded>
      
      
      
      <category domain="https://zhoukaibo.com/tags/flink/">flink</category>
      
      
      <comments>https://zhoukaibo.com/2019/10/15/apache-flink-intro-client/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Java SPI 机制分析及其优缺点</title>
      <link>https://zhoukaibo.com/2019/03/16/java-spi/</link>
      <guid>https://zhoukaibo.com/2019/03/16/java-spi/</guid>
      <pubDate>Sat, 16 Mar 2019 12:50:29 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;SPI-概述&quot;&gt;&lt;a href=&quot;#SPI-概述&quot; class=&quot;headerlink&quot; title=&quot;SPI 概述&quot;&gt;&lt;/a&gt;SPI 概述&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://docs.oracle.com/javase/tutorial/ext/b</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="SPI-概述"><a href="#SPI-概述" class="headerlink" title="SPI 概述"></a>SPI 概述</h2><p><a href="https://docs.oracle.com/javase/tutorial/ext/basics/spi.html" target="_blank" rel="noopener">SPI</a> 全称为(Service Provider Interface)，是 JDK 内置的一种服务发现机制。它可以动态的为某个接口寻找服务实现，有点类似 IOC(Inversion of Control)的思想，将装配的控制权移到程序之外，在模块化设计中这个机制尤其重要。</p><p>使用 SPI 机制需要在 classpath 下的 META-INF/services/ 目录里创建一个以服务接口命名的文件，这个文件里的内容就是这个接口的具体的实现类。</p><p>以 JavaMail 程序为例，协议层只需要定义好邮件发送接口，业务层实现对应的协议如 SMTP, IMAP, POP 就可以了。</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122513.jpeg" alt="JavaMail API"></p><h2 id="SPI-实例说明"><a href="#SPI-实例说明" class="headerlink" title="SPI 实例说明"></a>SPI 实例说明</h2><p>接下来通过一个具体的例子来讲解 SPI 的用法，例子的代码在 <a href="https://github.com/kaibozhou/spi-demo" target="_blank" rel="noopener">github</a> 上可以看到。</p><p>例子代码在IDEA中的结构如图：</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122552.jpg" alt="java-spi-example"></p><p>（1）首先我们提供一个接口类 <code>IOperation</code> 以及它的两个实现类 <code>PlusOperationImpl</code> 和 <code>DivisionOperationImpl</code>，都在 <code>com.zhoukaibo.spi</code> 这个包路径下。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">IOperation</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">operation</span><span class="params">(<span class="keyword">int</span> numberA, <span class="keyword">int</span> numberB)</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PlusOperationImpl</span> <span class="keyword">implements</span> <span class="title">IOperation</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">PlusOperationImpl</span><span class="params">()</span> </span>&#123;</span><br><span class="line">System.out.println(<span class="string">"plusOperation construct"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">operation</span><span class="params">(<span class="keyword">int</span> numberA, <span class="keyword">int</span> numberB)</span> </span>&#123;</span><br><span class="line">    System.out.println(<span class="string">"Operation: plus"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> numberA + numberB;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DivisionOperationImpl</span> <span class="keyword">implements</span> <span class="title">IOperation</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">DivisionOperationImpl</span><span class="params">()</span> </span>&#123;</span><br><span class="line">System.out.println(<span class="string">"division construct"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">operation</span><span class="params">(<span class="keyword">int</span> numberA, <span class="keyword">int</span> numberB)</span> </span>&#123;</span><br><span class="line">    System.out.println(<span class="string">"Operation: division"</span>);</span><br><span class="line">        <span class="keyword">return</span> numberA / numberB;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>（2）接着在 classpath 下创建文件夹 META-INF/services，在文件夹中新建一个文件 <code>com.zhoukaibo.spi.IOperation</code>。并在文件中写入具体的实现类：<code>com.zhoukaibo.spi.PlusOperationImpl</code> 和 <code>com.zhoukaibo.spi.DivisionOperationImpl</code>。</p><p>（3）最后，我们就可以利用 ServiceLoader 进行服务发现了：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">ServiceLoader&lt;IOperation&gt; operations = ServiceLoader.load(IOperation.class);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> numberA = <span class="number">6</span>;</span><br><span class="line"><span class="keyword">int</span> numberB = <span class="number">3</span>;</span><br><span class="line">System.out.println(<span class="string">"NumberA: "</span> + numberA + <span class="string">", NumberB: "</span> + numberB);</span><br><span class="line">Iterator&lt;IOperation&gt; iterator = operations.iterator();</span><br><span class="line"><span class="keyword">while</span> (iterator.hasNext()) &#123;</span><br><span class="line">IOperation operation = iterator.next();</span><br><span class="line">System.out.println(operation.operation(numberA, numberB));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>（4）程序输出：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">NumberA: <span class="number">6</span>, NumberB: <span class="number">3</span></span><br><span class="line">division construct</span><br><span class="line">Operation: division</span><br><span class="line"><span class="number">2</span></span><br><span class="line">plusOperation construct</span><br><span class="line">Operation: plus</span><br><span class="line"><span class="number">9</span></span><br></pre></td></tr></table></figure><h2 id="SPI-实现原理"><a href="#SPI-实现原理" class="headerlink" title="SPI 实现原理"></a>SPI 实现原理</h2><ol><li>应用程序调用 ServiceLoader.load 方法</li></ol><p>ServiceLoader.load 方法内先创建一个新的ServiceLoader，并实例化该类中的成员变数，包括：</p><ul><li>ClassLoader loader(类载入器)</li><li>AccessControlContext acc(访问控制器)</li><li>LinkedHashMap&lt;String, S&gt; providers(用于缓存载入成功的类)</li><li>LazyIterator lookupIterator(实现迭代器功能)</li></ul><ol start="2"><li>应用程序通过迭代器获取对象实例</li></ol><p>ServiceLoader 先判断成员变量 providers 对象中否有缓存实例对象，如果有缓存，直接返回。</p><p>如果没有缓存，执行类的装载：读取 META-INF/services/ 下的配置文件，获得所有能被实例化的类的名称，通过反射方法 Class.forName() 载入类对象，并用 instance() 方法将类实例化。把实例化后的类缓存到providers 对象中然后返回实例对象。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>JDK 内置的 SPI 机制本身有它的优点，但由于实现比较简单，也有不少缺点。</p><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><p>使用 <a href="https://zhoukaibo.com/tags/java/">Java</a> SPI 机制的优势是实现解耦，使得接口的定义与具体业务实现分离，而不是耦合在一起。应用程序可以根据实际业务情况启用或替换具体组件。</p><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul><li>不能按需加载。虽然 ServiceLoader 做了延迟载入，但是基本只能通过遍历全部获取，也就是接口的实现类得全部载入并实例化一遍。如果你并不想用某些实现类，或者某些类实例化很耗时，它也被载入并实例化了，这就造成了浪费。</li><li>获取某个实现类的方式不够灵活，只能通过 Iterator 形式获取，不能根据某个参数来获取对应的实现类。</li><li>多个并发多线程使用 ServiceLoader 类的实例是不安全的。</li><li>加载不到实现类时抛出并不是真正原因的异常，错误很难定位。</li></ul><p>鉴于 SPI 的诸多缺点，很多系统都是自己实现了一套类加载机制，例如 dubbo。用户也可以自定义classloader+反射机制来加载，实现并不复杂。此外开源的类加载解决方案有 <a href="https://github.com/pf4j/pf4j" target="_blank" rel="noopener">Plugin Framework for Java (PF4J)</a> 等。</p><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><ul><li>本文 SPI 例子的<a href="https://zhoukaibo.com/tags/java/">Java</a>代码：<a href="https://github.com/kaibozhou/spi-demo" target="_blank" rel="noopener">https://github.com/kaibozhou/spi-demo</a></li><li>使用 PF4J 的示例代码：<a href="https://github.com/kaibozhou/pf4j-demo" target="_blank" rel="noopener">https://github.com/kaibozhou/pf4j-demo</a></li></ul>]]></content:encoded>
      
      
      
      <category domain="https://zhoukaibo.com/tags/java/">java</category>
      
      <category domain="https://zhoukaibo.com/tags/spi/">spi</category>
      
      
      <comments>https://zhoukaibo.com/2019/03/16/java-spi/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>谈谈流计算中的『Exactly Once』特性</title>
      <link>https://zhoukaibo.com/2019/02/14/exactly-once-exactly-same/</link>
      <guid>https://zhoukaibo.com/2019/02/14/exactly-once-exactly-same/</guid>
      <pubDate>Wed, 13 Feb 2019 17:11:29 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;本文翻译自 streaml.io 网站上的一篇博文：&lt;a href=&quot;https://streaml.io/blog/exactly-once&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;“Exactly once is NOT exactly the </description>
        
      
      
      
      <content:encoded><![CDATA[<p>本文翻译自 streaml.io 网站上的一篇博文：<a href="https://streaml.io/blog/exactly-once" target="_blank" rel="noopener">“Exactly once is NOT exactly the same”</a> ，分析了流计算系统中常说的『Exactly Once』特性，主要观点是：『精确一次』并不保证是完全一样。</p><p>目前市面上使用较多的流计算系统有 Apache Storm，<a href="http://zhoukaibo.com/tags/flink/">Apache Flink</a>, Heron, Apache Kafka (Kafka Streams) 和 Apache Spark (Spark Streaming)。关于流计算系统有个被广泛讨论的特性是『exactly-once』语义，很多系统宣称已经支持了这一特性。但是，到底什么是『<a href="http://zhoukaibo.com/tags/exactly-once/">exactly-once</a>』，怎么样才算是实现了『exactly-once』，人们存在很多误解和歧义。接下来我们做下分析。</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>流处理（有时称为事件处理）可以简单地描述为是对无界数据或事件的连续处理。流或事件处理应用程序可以或多或少地被描述为有向图，并且通常被描述为有向无环图（DAG）。在这样的图中，每个边表示数据或事件流，每个顶点表示运算符，会使用程序中定义的逻辑处理来自相邻边的数据或事件。有两种特殊类型的顶点，通常称为 sources 和 sinks。sources 读取外部数据/事件到应用程序中，而 sinks 通常会收集应用程序生成的结果。下图是流式应用程序的示例。</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428121537.png" alt="A typical stream processing topology"></p><p>流处理引擎通常允许用户指定可靠性模式或处理语义，以指示它将为整个应用程序中的数据处理提供哪些保证。这些保证是有意义的，因为你始终会遇到由于网络，机器等可能导致数据丢失的故障。流处理引擎通常为应用程序提供了三种数据处理语义：最多一次、至少一次和精确一次。</p><p>如下是对这些不同处理语义的宽松定义：</p><h3 id="最多一次（At-most-once）"><a href="#最多一次（At-most-once）" class="headerlink" title="最多一次（At-most-once）"></a>最多一次（At-most-once）</h3><p>这本质上是一『尽力而为』的方法。保证数据或事件最多由应用程序中的所有算子处理一次。 这意味着如果数据在被流应用程序完全处理之前发生丢失，则不会进行其他重试或者重新发送。下图中的例子说明了这种情况。</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428121625.png" alt="At-most-once processing semantics"></p><h3 id="至少一次（At-least-once）"><a href="#至少一次（At-least-once）" class="headerlink" title="至少一次（At-least-once）"></a>至少一次（<a href="http://zhoukaibo.com/tags/at-least-once/">At-least-once</a>）</h3><p>应用程序中的所有算子都保证数据或事件至少被处理一次。这通常意味着如果事件在流应用程序完全处理之前丢失，则将从源头重放或重新传输事件。然而，由于事件是可以被重传的，因此一个事件有时会被处理多次，这就是所谓的<a href="http://zhoukaibo.com/tags/at-least-once/">至少一次</a>。</p><p>下图的例子描述了这种情况：第一个算子最初未能成功处理事件，然后在重试时成功，接着在第二次重试时也成功了，其实是没有必要的。</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122058.png" alt="At-least-once processing semantics"></p><h3 id="精确一次（Exactly-once）"><a href="#精确一次（Exactly-once）" class="headerlink" title="精确一次（Exactly-once）"></a>精确一次（Exactly-once）</h3><p>即使是在各种故障的情况下，流应用程序中的所有算子都保证事件只会被『精确一次』的处理。（也有文章将 <a href="http://zhoukaibo.com/tags/exactly-once/">Exactly-once</a> 翻译为：完全一次，恰好一次）</p><p>通常使用两种流行的机制来实现『精确一次』处理语义。</p><ul><li><ol><li>分布式快照/状态检查点</li></ol></li><li><ol start="2"><li>至少一次事件传递和对重复数据去重</li></ol></li></ul><p>实现『精确一次』的分布式快照/状态检查点方法受到 Chandy-Lamport 分布式快照算法的启发[1]。通过这种机制，流应用程序中每个算子的所有状态都会定期做 checkpoint。如果是在系统中的任何地方发生失败，每个算子的所有状态都回滚到最新的全局一致 checkpoint 点。在回滚期间，将暂停所有处理。源也会重置为与最近 checkpoint 相对应的正确偏移量。整个流应用程序基本上是回到最近一次的一致状态，然后程序可以从该状态重新启动。下图描述了这种 checkpoint 机制的基础知识。</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122131.png" alt="Distributed snapshot"></p><p>在上图中，流应用程序在 T1 时间处正常工作，并且做了checkpoint。然而，在时间 T2，算子未能处理输入的数据。此时，S=4 的状态值已保存到持久存储器中，而状态值 S=12 保存在算子的内存中。为了修复这种差异，在时间T3，处理程序将状态回滚到 S=4 并“重放”流中的每个连续状态直到最近，并处理每个数据。最终结果是有些数据已被处理了多次，但这没关系，因为无论执行了多少次回滚，结果状态都是相同的。</p><p>另一种实现『精确一次』的方法是：在每个算子上实现至少一次事件传递和对重复数据去重来。使用此方法的流处理引擎将重放失败事件，以便在事件进入算子中的用户定义逻辑之前，进一步尝试处理并移除每个算子的重复事件。此机制要求为每个算子维护一个事务日志，以跟踪它已处理的事件。利用这种机制的引擎有 Google 的 MillWheel[2] 和 <a href="https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it" target="_blank" rel="noopener">Apache Kafka Streams</a>。下图说明了这种机制的要点。</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122202.png" alt="At-least-once delivery plus deduplication"></p><h2 id="『精确一次』是真正的『精确一次』吗"><a href="#『精确一次』是真正的『精确一次』吗" class="headerlink" title="『精确一次』是真正的『精确一次』吗?"></a>『精确一次』是真正的『精确一次』吗?</h2><p>现在让我们重新审视『<a href="http://zhoukaibo.com/tags/exactly-once/">精确一次</a>』处理语义真正对最终用户的保证。『精确一次』这个术语在描述正好处理一次时会让人产生误导。</p><p>有些人可能认为『精确一次』描述了事件处理的保证，其中流中的每个事件只被处理一次。实际上，没有引擎能够保证正好只处理一次。在面对任意故障时，不可能保证每个算子中的用户定义逻辑在每个事件中只执行一次，因为用户代码被部分执行的可能性是永远存在的。</p><p>考虑具有流处理运算符的场景，该运算符执行打印传入事件的ID的映射操作，然后返回事件不变。下面的伪代码说明了这个操作：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Map</span> (<span class="type">Event</span> event) &#123;</span><br><span class="line">    <span class="type">Print</span> <span class="string">"Event ID: "</span> + event.getId()</span><br><span class="line">    <span class="type">Return</span> event</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>每个事件都有一个GUID(全局惟一ID)。如果用户逻辑的精确执行一次得到保证，那么事件ID将只输出一次。但是，这是无法保证的，因为在用户定义的逻辑的执行过程中，随时都可能发生故障。引擎无法自行确定执行用户定义的处理逻辑的时间点。因此，不能保证任意用户定义的逻辑只执行一次。这也意味着，在用户定义的逻辑中实现的外部操作(如写数据库)也不能保证只执行一次。此类操作仍然需要以幂等的方式执行。</p><p>那么，当引擎声明『<a href="http://zhoukaibo.com/tags/exactly-once/">精确一次</a>』处理语义时，它们能保证什么呢？如果不能保证用户逻辑只执行一次，那么什么逻辑只执行一次？当引擎声明『精确一次』处理语义时，它们实际上是在说，它们可以保证引擎管理的状态更新只提交一次到持久的后端存储。</p><p>上面描述的两种机制都使用持久的后端存储作为真实性的来源，可以保存每个算子的状态并自动向其提交更新。对于机制1(分布式快照/状态检查点)，此持久后端状态用于保存流应用程序的全局一致状态检查点(每个算子的检查点状态)。对于机制2(至少一次事件传递加上重复数据删除)，持久后端状态用于存储每个算子的状态以及每个算子的事务日志，该日志跟踪它已经完全处理的所有事件。</p><p>提交状态或对作为真实来源的持久后端应用更新可以被描述为恰好发生一次。然而，如上所述，计算状态的更新/更改，即处理在事件上执行任意用户定义逻辑的事件，如果发生故障，则可能不止一次地发生。换句话说，事件的处理可以发生多次，但是该处理的效果只在持久后端状态存储中反映一次。因此，我们认为有效地描述这些处理语义最好的术语是『有效一次』（effectively once）。</p><h2 id="分布式快照与至少一次事件传递和重复数据删除的比较"><a href="#分布式快照与至少一次事件传递和重复数据删除的比较" class="headerlink" title="分布式快照与至少一次事件传递和重复数据删除的比较"></a>分布式快照与至少一次事件传递和重复数据删除的比较</h2><p>从语义的角度来看，分布式快照和至少一次事件传递以及重复数据删除机制都提供了相同的保证。然而，由于两种机制之间的实现差异，存在显着的性能差异。</p><p>机制1（分布式快照/状态检查点）的性能开销是最小的，因为引擎实际上是往流应用程序中的所有算子一起发送常规事件和特殊事件，而状态检查点可以在后台异步执行。但是，对于大型流应用程序，故障可能会更频繁地发生，导致引擎需要暂停应用程序并回滚所有算子的状态，这反过来又会影响性能。流式应用程序越大，故障发生的可能性就越大，因此也越频繁，反过来，流式应用程序的性能受到的影响也就越大。然而，这种机制是非侵入性的，运行时需要的额外资源影响很小。</p><p>机制2（至少一次事件传递加重复数据删除）可能需要更多资源，尤其是存储。使用此机制，引擎需要能够跟踪每个算子实例已完全处理的每个元组，以执行重复数据删除，以及为每个事件执行重复数据删除本身。这意味着需要跟踪大量的数据，尤其是在流应用程序很大或者有许多应用程序在运行的情况下。执行重复数据删除的每个算子上的每个事件都会产生性能开销。但是，使用这种机制，流应用程序的性能不太可能受到应用程序大小的影响。对于机制1，如果任何算子发生故障，则需要发生全局暂停和状态回滚；对于机制2，失败的影响更加局部性。当在算子中发生故障时，可能尚未完全处理的事件仅从上游源重放/重传。性能影响与流应用程序中发生故障的位置是隔离的，并且对流应用程序中其他算子的性能几乎没有影响。从性能角度来看，这两种机制的优缺点如下。</p><p>分布式快照/状态检查点的优缺点：<br>优点：</p><ul><li>较小的性能和资源开销</li></ul><p>缺点：</p><ul><li>对性能的影响较大</li><li>拓扑越大，对性能的潜在影响越大</li></ul><p>至少一次事件传递以及重复数据删除机制的优缺点：</p><p>优点：</p><ul><li>故障对性能的影响是局部的</li><li>故障的影响不一定会随着拓扑的大小而增加</li></ul><p>缺点：</p><ul><li>可能需要大量的存储和基础设施来支持</li><li>每个算子的每个事件的性能开销</li></ul><p>虽然从理论上讲，分布式快照和至少一次事件传递加重复数据删除机制之间存在差异，但两者都可以简化为至少一次处理加幂等性。对于这两种机制，当发生故障时(至少实现一次)，事件将被重放/重传，并且通过状态回滚或事件重复数据删除，算子在更新内部管理状态时本质上是幂等的。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在这篇博客文章中，我希望能够让你相信『精确一次』这个词是非常具有误导性的。提供『<a href="http://zhoukaibo.com/tags/exactly-once/">精确一次</a>』的处理语义实际上意味着流处理引擎管理的算子状态的不同更新只反映一次。『精确一次』并不能保证事件的处理，即任意用户定义逻辑的执行，只会发生一次。我们更喜欢用『有效一次』（effectively once）这个术语来表示这种保证，因为处理不一定保证只发生一次，但是对引擎管理的状态的影响只反映一次。两种流行的机制，分布式快照和重复数据删除，被用来实现精确/有效的一次性处理语义。这两种机制为消息处理和状态更新提供了相同的语义保证，但是在性能上存在差异。这篇文章并不是要让你相信任何一种机制都优于另一种，因为它们各有利弊。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><p>Chandy, K. Mani and Leslie Lamport. <a href="http://lamport.azurewebsites.net/pubs/chandy.pdf" target="_blank" rel="noopener">Distributed snapshots: Determining global states of distributed systems</a>. ACM Transactions on Computer Systems (TOCS) 3.1 (1985): 63-75.</p></li><li><p>Akidau, Tyler, et al. <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwj1lOKckO7WAhVHhlQKHWTuCxgQFggxMAE&amp;url=https%3A%2F%2Fwww.cs.cmu.edu%2F~pavlo%2Fcourses%2Ffall2013%2Fstatic%2Fpapers%2Fp734-akidau.pdf&amp;usg=AOvVaw17V8DwQz3BTrfNSXniIO2q" target="_blank" rel="noopener">MillWheel: Fault-tolerant stream processing at internet scale</a>. Proceedings of the VLDB Endowment 6.11 (2013): 1033-1044.</p></li></ol>]]></content:encoded>
      
      
      
      <category domain="https://zhoukaibo.com/tags/exactly-once/">exactly-once</category>
      
      <category domain="https://zhoukaibo.com/tags/at-least-once/">at-least-once</category>
      
      
      <comments>https://zhoukaibo.com/2019/02/14/exactly-once-exactly-same/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Apache Flink结合Kafka构建端到端的Exactly-Once处理</title>
      <link>https://zhoukaibo.com/2019/01/10/flink-kafka-exactly-once/</link>
      <guid>https://zhoukaibo.com/2019/01/10/flink-kafka-exactly-once/</guid>
      <pubDate>Thu, 10 Jan 2019 03:45:43 GMT</pubDate>
      
        
        
      <description>&lt;blockquote&gt;
&lt;p&gt;本文翻译自：&lt;a href=&quot;https://data-artisans.com/blog/end-to-end-exactly-once-processing-apache-flink-apache-kafka&quot; target=&quot;_blank&quot; </description>
        
      
      
      
      <content:encoded><![CDATA[<blockquote><p>本文翻译自：<a href="https://data-artisans.com/blog/end-to-end-exactly-once-processing-apache-flink-apache-kafka" target="_blank" rel="noopener">https://data-artisans.com/blog/end-to-end-exactly-once-processing-apache-flink-apache-kafka</a></p></blockquote><p>Apache Flink自2017年12月发布的1.4.0版本开始，为流计算引入了一个重要的里程碑特性：TwoPhaseCommitSinkFunction（<a href="https://issues.apache.org/jira/browse/FLINK-7210" target="_blank" rel="noopener">相关的Jira</a>）。它提取了两阶段提交协议的通用逻辑，使得通过Flink来构建端到端的Exactly-Once程序成为可能。同时支持一些数据源（source）和输出端（sink），包括Apache Kafka 0.11及更高版本。它提供了一个抽象层，用户只需要实现少数方法就能实现端到端的Exactly-Once语义。</p><p>有关TwoPhaseCommitSinkFunction的使用详见文档: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.4/api/java/org/apache/flink/streaming/api/functions/sink/TwoPhaseCommitSinkFunction.html" target="_blank" rel="noopener">TwoPhaseCommitSinkFunction</a>。或者可以直接阅读Kafka 0.11 sink的文档: <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/connectors/kafka.html#kafka-011" target="_blank" rel="noopener">kafka</a>。</p><p>接下来会详细分析这个新功能以及Flink的实现逻辑，分为如下几点。</p><ul><li>描述Flink checkpoint机制是如何保证Flink程序结果的Exactly-Once的</li><li>显示Flink如何通过两阶段提交协议与数据源和数据输出端交互，以提供端到端的Exactly-Once保证</li><li>通过一个简单的示例，了解如何使用TwoPhaseCommitSinkFunction实现Exactly-Once的文件输出</li></ul><h2 id="Apache-Flink应用程序中的Exactly-Once语义"><a href="#Apache-Flink应用程序中的Exactly-Once语义" class="headerlink" title="Apache Flink应用程序中的Exactly-Once语义"></a>Apache Flink应用程序中的Exactly-Once语义</h2><p>当我们说『Exactly-Once』时，指的是每个输入的事件只影响最终结果一次。即使机器或软件出现故障，既没有重复数据，也不会丢数据。</p><p><a href="https://zhoukaibo.com/tags/flink/">Flink</a>很久之前就提供了Exactly-Once语义。在过去几年中，我们对Flink的<a href="https://data-artisans.com/blog/high-throughput-low-latency-and-exactly-once-stream-processing-with-apache-flink" target="_blank" rel="noopener">checkpoint机制</a>有过深入的描述，这是Flink有能力提供Exactly-Once语义的核心。Flink文档还提供了该功能的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.4/ops/state/checkpoints.html" target="_blank" rel="noopener">全面概述</a>。</p><p>在继续之前，先看下对checkpoint机制的简要介绍，这对理解后面的主题至关重要。</p><p>一次checkpoint是以下内容的一致性快照： </p><ul><li>应用程序的当前状态</li><li>输入流的位置</li></ul><p>Flink可以配置一个固定的时间点，定期产生checkpoint，将checkpoint的数据写入持久存储系统，例如S3或HDFS。将checkpoint数据写入持久存储是异步发生的，这意味着Flink应用程序在checkpoint过程中可以继续处理数据。</p><p>如果发生机器或软件故障，重新启动后，<a href="https://zhoukaibo.com/tags/flink/">Flink</a>应用程序将从最新的checkpoint点恢复处理； Flink会恢复应用程序状态，将输入流回滚到上次checkpoint保存的位置，然后重新开始运行。这意味着Flink可以像从未发生过故障一样计算结果。</p><p>在Flink 1.4.0之前，Exactly-Once语义仅限于Flink应用程序内部，并没有扩展到Flink数据处理完后发送的大多数外部系统。Flink应用程序与各种数据输出端进行交互，开发人员需要有能力自己维护组件的上下文来保证Exactly-Once语义。</p><p>为了提供端到端的Exactly-Once语义 - 也就是说，除了Flink应用程序内部，Flink写入的外部系统也需要能满足Exactly-Once语义 - 这些外部系统必须提供提交或回滚的方法，然后通过Flink的checkpoint机制来协调。</p><p>分布式系统中，协调提交和回滚的常用方法是<a href="https://en.wikipedia.org/wiki/Two-phase_commit_protocol" target="_blank" rel="noopener">两阶段提交协议</a>。在下一节中，我们将讨论Flink的TwoPhaseCommitSinkFunction是如何利用两阶段提交协议来提供端到端的Exactly-Once语义。</p><h2 id="Flink应用程序端到端的Exactly-Once语义"><a href="#Flink应用程序端到端的Exactly-Once语义" class="headerlink" title="Flink应用程序端到端的Exactly-Once语义"></a>Flink应用程序端到端的Exactly-Once语义</h2><p>我们将介绍两阶段提交协议，以及它如何在一个读写Kafka的Flink程序中实现端到端的Exactly-Once语义。Kafka是一个流行的消息中间件，经常与Flink一起使用。Kafka在最近的0.11版本中添加了对事务的支持。这意味着现在通过Flink读写Kafaka，并提供<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/connectors/kafka.html#kafka-011" target="_blank" rel="noopener">端到端的Exactly-Once语义有了必要的支持</a>。</p><p><a href="https://zhoukaibo.com/tags/flink/">Flink</a>对端到端的Exactly-Once语义的支持不仅局限于Kafka，您可以将它与任何一个提供了必要的协调机制的源/输出端一起使用。例如<a href="http://pravega.io/" target="_blank" rel="noopener">Pravega</a>，来自DELL/EMC的开源流媒体存储系统，通过Flink的TwoPhaseCommitSinkFunction也能支持端到端的Exactly-Once语义。</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122306.png" alt="exactly-once-two-phase-commit-1"></p><p>在今天讨论的这个示例程序中，我们有：</p><ul><li>从Kafka读取的数据源（Flink内置的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/connectors/kafka.html#kafka-consumer" target="_blank" rel="noopener">KafkaConsumer</a>）</li><li>窗口聚合</li><li>将数据写回Kafka的数据输出端（Flink内置的<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/connectors/kafka.html#kafka-producer" target="_blank" rel="noopener">KafkaProducer</a>）</li></ul><p>要使数据输出端提供Exactly-Once保证，它必须将所有数据通过一个事务提交给Kafka。提交捆绑了两个checkpoint之间的所有要写入的数据。这可确保在发生故障时能回滚写入的数据。但是在分布式系统中，通常会有多个并发运行的写入任务的，简单的提交或回滚是不够的，因为所有组件必须在提交或回滚时“一致”才能确保一致的结果。Flink使用两阶段提交协议及预提交阶段来解决这个问题。</p><p>在checkpoint开始的时候，即两阶段提交协议的“预提交”阶段。当checkpoint开始时，Flink的JobManager会将checkpoint barrier（将数据流中的记录分为进入当前checkpoint与进入下一个checkpoint）注入数据流。</p><p>brarrier在operator之间传递。对于每一个operator，它触发operator的状态快照写入到state backend。</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122328.png" alt="exactly-once-two-phase-commit-2"></p><p>数据源保存了消费Kafka的偏移量(offset)，之后将checkpoint barrier传递给下一个operator。</p><p>这种方式仅适用于operator具有『内部』状态。所谓内部状态，是指Flink state backend保存和管理的 -例如，第二个operator中window聚合算出来的sum值。当一个进程有它的内部状态的时候，除了在checkpoint之前需要将数据变更写入到state backend，不需要在预提交阶段执行任何其他操作。<a href="https://zhoukaibo.com/tags/flink/">Flink</a>负责在checkpoint成功的情况下正确提交这些写入，或者在出现故障时中止这些写入。</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122346.png" alt="exactly-once-two-phase-commit-3"></p><h2 id="示例Flink应用程序启动预提交阶段"><a href="#示例Flink应用程序启动预提交阶段" class="headerlink" title="示例Flink应用程序启动预提交阶段"></a>示例Flink应用程序启动预提交阶段</h2><p>但是，当进程具有『外部』状态时，需要作些额外的处理。外部状态通常以写入外部系统（如Kafka）的形式出现。在这种情况下，为了提供Exactly-Once保证，外部系统必须支持事务，这样才能和两阶段提交协议集成。</p><p>在本文示例中的数据需要写入Kafka，因此数据输出端（Data Sink）有外部状态。在这种情况下，在预提交阶段，除了将其状态写入state backend之外，数据输出端还必须预先提交其外部事务。</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122406.png" alt="exactly-once-two-phase-commit-4"></p><p>当checkpoint barrier在所有operator都传递了一遍，并且触发的checkpoint回调成功完成时，预提交阶段就结束了。所有触发的状态快照都被视为该checkpoint的一部分。checkpoint是整个应用程序状态的快照，包括预先提交的外部状态。如果发生故障，我们可以回滚到上次成功完成快照的时间点。</p><p>下一步是通知所有operator，checkpoint已经成功了。这是两阶段提交协议的提交阶段，JobManager为应用程序中的每个operator发出checkpoint已完成的回调。</p><p>数据源和widnow operator没有外部状态，因此在提交阶段，这些operator不必执行任何操作。但是，数据输出端（Data Sink）拥有外部状态，此时应该提交外部事务。</p><p><img src="https://raw.githubusercontent.com/kaibozhou/kaibozhou.github.io/img/20190428122426.png" alt="exactly-once-two-phase-commit-5"></p><p>我们对上述知识点总结下：</p><ul><li>一旦所有operator完成预提交，就提交一个commit。</li><li>如果至少有一个预提交失败，则所有其他提交都将中止，我们将回滚到上一个成功完成的checkpoint。</li><li>在预提交成功之后，提交的commit需要保证最终成功 - operator和外部系统都需要保障这点。如果commit失败（例如，由于间歇性网络问题），整个<a href="https://zhoukaibo.com/tags/flink/">Flink</a>应用程序将失败，应用程序将根据用户的重启策略重新启动，还会尝试再提交。这个过程至关重要，因为如果commit最终没有成功，将会导致数据丢失。</li></ul><p>因此，我们可以确定所有operator都同意checkpoint的最终结果：所有operator都同意数据已提交，或提交被中止并回滚。</p><h2 id="在Flink中实现两阶段提交Operator"><a href="#在Flink中实现两阶段提交Operator" class="headerlink" title="在Flink中实现两阶段提交Operator"></a>在Flink中实现两阶段提交Operator</h2><p>完整的实现两阶段提交协议可能有点复杂，这就是为什么<a href="https://zhoukaibo.com/tags/flink/">Flink</a>将它的通用逻辑提取到抽象类TwoPhaseCommitSinkFunction中的原因。</p><p>接下来基于输出到文件的简单示例，说明如何使用TwoPhaseCommitSinkFunction。用户只需要实现四个函数，就能为数据输出端实现Exactly-Once语义：</p><ul><li>beginTransaction - 在事务开始前，我们在目标文件系统的临时目录中创建一个临时文件。随后，我们可以在处理数据时将数据写入此文件。</li><li>preCommit - 在预提交阶段，我们刷新文件到存储，关闭文件，不再重新写入。我们还将为属于下一个checkpoint的任何后续文件写入启动一个新的事务。</li><li>commit - 在提交阶段，我们将预提交阶段的文件原子地移动到真正的目标目录。需要注意的是，这会增加输出数据可见性的延迟。</li><li>abort - 在中止阶段，我们删除临时文件。</li></ul><p>我们知道，如果发生任何故障，Flink会将应用程序的状态恢复到最新的一次checkpoint点。一种极端的情况是，预提交成功了，但在这次commit的通知到达operator之前发生了故障。在这种情况下，Flink会将operator的状态恢复到已经预提交，但尚未真正提交的状态。</p><p>我们需要在预提交阶段保存足够多的信息到checkpoint状态中，以便在重启后能正确的中止或提交事务。在这个例子中，这些信息是临时文件和目标目录的路径。</p><p>TwoPhaseCommitSinkFunction已经把这种情况考虑在内了，并且在从checkpoint点恢复状态时，会优先发出一个commit。我们需要以幂等方式实现提交，一般来说，这并不难。在这个示例中，我们可以识别出这样的情况：临时文件不在临时目录中，但已经移动到目标目录了。</p><p>在TwoPhaseCommitSinkFunction中，还有一些其他边界情况也会考虑在内，请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.4/api/java/org/apache/flink/streaming/api/functions/sink/TwoPhaseCommitSinkFunction.html" target="_blank" rel="noopener">Flink文档</a>了解更多信息。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总结下本文涉及的一些要点：</p><ul><li><a href="https://zhoukaibo.com/tags/flink/">Flink</a>的checkpoint机制是支持两阶段提交协议并提供端到端的Exactly-Once语义的基础。</li><li>这个方案的优点是: Flink不像其他一些系统那样，通过网络传输存储数据 - 不需要像大多数批处理程序那样将计算的每个阶段写入磁盘。</li><li>Flink的TwoPhaseCommitSinkFunction提取了两阶段提交协议的通用逻辑，基于此将Flink和支持事务的外部系统结合，构建端到端的Exactly-Once成为可能。</li><li>从<a href="https://data-artisans.com/blog/announcing-the-apache-flink-1-4-0-release" target="_blank" rel="noopener">Flink 1.4.0</a>开始，Pravega和Kafka 0.11 producer都提供了Exactly-Once语义；Kafka在0.11版本首次引入了事务，为在Flink程序中使用Kafka producer提供Exactly-Once语义提供了可能性。</li><li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/connectors/kafka.html#kafka-011" target="_blank" rel="noopener">Kafaka 0.11 producer</a>的事务是在TwoPhaseCommitSinkFunction基础上实现的，和at-least-once producer相比只增加了非常低的开销。</li></ul><p>这是个令人兴奋的功能，期待Flink TwoPhaseCommitSinkFunction在未来支持更多的数据接收端。</p>]]></content:encoded>
      
      
      
      <category domain="https://zhoukaibo.com/tags/flink/">flink</category>
      
      <category domain="https://zhoukaibo.com/tags/exactly-once/">exactly-once</category>
      
      <category domain="https://zhoukaibo.com/tags/at-least-once/">at-least-once</category>
      
      <category domain="https://zhoukaibo.com/tags/kafka/">kafka</category>
      
      
      <comments>https://zhoukaibo.com/2019/01/10/flink-kafka-exactly-once/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Hello World</title>
      <link>https://zhoukaibo.com/2019/01/10/hello-world/</link>
      <guid>https://zhoukaibo.com/2019/01/10/hello-world/</guid>
      <pubDate>Thu, 10 Jan 2019 01:47:55 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;Hello World！&lt;/p&gt;
&lt;p&gt;2019年1月。&lt;/p&gt;
</description>
        
      
      
      
      <content:encoded><![CDATA[<p>Hello World！</p><p>2019年1月。</p>]]></content:encoded>
      
      
      
      
      <comments>https://zhoukaibo.com/2019/01/10/hello-world/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
